
@article{lundberg_unified_nodate,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction’s accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a uniﬁed framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identiﬁcation of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class uniﬁes six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this uniﬁcation, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	language = {en},
	author = {Lundberg, Scott M and Lee, Su-In},
	note = {02257},
	pages = {10},
}

@article{lin_focal_2018,
	title = {Focal {Loss} for {Dense} {Object} {Detection}},
	url = {http://arxiv.org/abs/1708.02002},
	abstract = {The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.},
	urldate = {2020-12-07},
	journal = {arXiv:1708.02002 [cs]},
	author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollár, Piotr},
	month = feb,
	year = {2018},
	note = {05277 
arXiv: 1708.02002},
}

@misc{noauthor_pdf_nodate,
	title = {({PDF}) {Analysis} of mixed finite element methods for the stokes problem: {A} unified approach},
	url = {https://www.researchgate.net/publication/242921951_Analysis_of_mixed_finite_element_methods_for_the_stokes_problem_A_unified_approach},
	urldate = {2020-10-07},
	note = {00251},
}

@article{chen_introduction_nodate,
	title = {{INTRODUCTION} {TO} {MULTIGRID} {METHODS}},
	language = {en},
	author = {Chen, Long},
	pages = {8},
}

@article{welleck_consistency_2020,
	title = {Consistency of a {Recurrent} {Language} {Model} {With} {Respect} to {Incomplete} {Decoding}},
	url = {http://arxiv.org/abs/2002.02492},
	abstract = {Despite strong performance on a variety of tasks, neural sequence models trained with maximum likelihood have been shown to exhibit issues such as length bias and degenerate repetition. We study the related issue of receiving infinite-length sequences from a recurrent language model when using common decoding algorithms. To analyze this issue, we first define inconsistency of a decoding algorithm, meaning that the algorithm can yield an infinite-length sequence that has zero probability under the model. We prove that commonly used incomplete decoding algorithms - greedy search, beam search, top-k sampling, and nucleus sampling - are inconsistent, despite the fact that recurrent language models are trained to produce sequences of finite length. Based on these insights, we propose two remedies which address inconsistency: consistent variants of top-k and nucleus sampling, and a self-terminating recurrent language model. Empirical results show that inconsistency occurs in practice, and that the proposed methods prevent inconsistency.},
	urldate = {2021-04-07},
	journal = {arXiv:2002.02492 [cs, stat]},
	author = {Welleck, Sean and Kulikov, Ilia and Kim, Jaedeok and Pang, Richard Yuanzhe and Cho, Kyunghyun},
	month = oct,
	year = {2020},
	note = {00000 
arXiv: 2002.02492},
	keywords = {Abs, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{fu_theoretical_2021,
	title = {A {Theoretical} {Analysis} of the {Repetition} {Problem} in {Text} {Generation}},
	url = {http://arxiv.org/abs/2012.14660},
	abstract = {Text generation tasks, including translation, summarization, language models, and etc. see rapid growth during recent years. Despite the remarkable achievements, the repetition problem has been observed in nearly all text generation models undermining the generation performance extensively. To solve the repetition problem, many methods have been proposed, but there is no existing theoretical analysis to show why this problem happens and how it is resolved. In this paper, we propose a new framework for theoretical analysis for the repetition problem. We ﬁrst deﬁne the Average Repetition Probability (ARP) to characterize the repetition problem quantitatively. Then, we conduct an extensive analysis of the Markov generation model and derive several upper bounds of the average repetition probability with intuitive understanding. We show that most of the existing methods are essentially minimizing the upper bounds explicitly or implicitly. Grounded on our theory, we show that the repetition problem is, unfortunately, caused by the traits of our language itself. One major reason is attributed to the fact that there exist too many words predicting the same word as the subsequent word with high probability. Consequently, it is easy to go back to that word and form repetitions and we dub it as the high inﬂow problem. Furthermore, we extend our analysis to broader generation models by deriving a concentration bound of the average repetition probability for a general generation model. Finally, based on the theoretical upper bounds, we propose a novel rebalanced encoding approach to alleviate the high inﬂow problem and thus reducing the upper bound. The experimental results show that our theoretical framework is applicable in general generation models and our proposed rebalanced encoding approach alleviates the repetition problem signiﬁcantly in both the translation task and the language modeling task. The source code of this paper can be obtained from https://github.com/fuzihaofzh/repetition-problem-nlg.},
	language = {en},
	urldate = {2021-04-14},
	journal = {arXiv:2012.14660 [cs]},
	author = {Fu, Zihao and Lam, Wai and So, Anthony Man-Cho and Shi, Bei},
	month = mar,
	year = {2021},
	note = {00000 
arXiv: 2012.14660},
	keywords = {Abs, Code, Computer Science - Computation and Language},
}

@article{allen_finite_1998,
	title = {Finite element and difference approximation of some linear stochastic partial differential equations},
	volume = {64},
	issn = {1045-1129},
	url = {https://doi.org/10.1080/17442509808834159},
	doi = {10.1080/17442509808834159},
	abstract = {Difference and finite element methods are described, analyzed, and tested for numerical solution of linear parabolic and elliptic SPDEs driven by white noise. Weak and integral formulations of the stochastic partial differential equations are approximated, respectively, by finite element and difference methods. The white noise processes are approximated by piecewise constant random processes to facilitate convergence proofs for the finite element method. Error analyses of the two numerical methods yield estimates of convergence rates. Computational experiments indicate that the two numerical methods have similar accuracy but the finite element method is computationally more efficient than the difference method},
	number = {1-2},
	urldate = {2021-05-10},
	journal = {Stochastics and Stochastic Reports},
	author = {Allen, E. J. and Novosel, S. J. and Zhang, Z.},
	month = may,
	year = {1998},
	note = {00000 
Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/17442509808834159},
	keywords = {AMS(MOS) Subject Classifications: 60H15, 65U05, Stochastic partial differential equations, finite differences, finite elements},
	pages = {117--142},
}

@misc{jungel_how_nodate,
	title = {How to write and publish a math paper?},
	language = {en},
	author = {Jüngel, Ansgar},
	note = {00000},
}

@article{tu_empirical_2020,
	title = {An {Empirical} {Study} on {Robustness} to {Spurious} {Correlations} using {Pre}-trained {Language} {Models}},
	url = {http://arxiv.org/abs/2007.06778},
	abstract = {Recent work has shown that pre-trained language models such as BERT improve robustness to spurious correlations in the dataset. Intrigued by these results, we find that the key to their success is generalization from a small amount of counterexamples where the spurious correlations do not hold. When such minority examples are scarce, pre-trained models perform as poorly as models trained from scratch. In the case of extreme minority, we propose to use multi-task learning (MTL) to improve generalization. Our experiments on natural language inference and paraphrase identification show that MTL with the right auxiliary tasks significantly improves performance on challenging examples without hurting the in-distribution performance. Further, we show that the gain from MTL mainly comes from improved generalization from the minority examples. Our results highlight the importance of data diversity for overcoming spurious correlations.},
	urldate = {2021-04-02},
	journal = {arXiv:2007.06778 [cs]},
	author = {Tu, Lifu and Lalwani, Garima and Gella, Spandana and He, He},
	month = aug,
	year = {2020},
	note = {00000 
arXiv: 2007.06778},
}

@article{huang_morleywangxu_2021,
	title = {A {Morley}–{Wang}–{Xu} {Element} {Method} for a {Fourth} {Order} {Elliptic} {Singular} {Perturbation} {Problem}},
	volume = {87},
	copyright = {All rights reserved},
	issn = {1573-7691},
	url = {https://doi.org/10.1007/s10915-021-01483-2},
	doi = {10.1007/s10915-021-01483-2},
	abstract = {A Morley–Wang–Xu (MWX) element method with a simply modified right hand side is proposed for a fourth order elliptic singular perturbation problem, in which the discrete bilinear form is standard as usual nonconforming finite element methods. The sharp error analysis is given for this MWX element method. And the Nitsche’s technique is applied to the MXW element method to achieve the optimal convergence rate in the case of the boundary layers. An important feature of the MWX element method is solver-friendly. Based on a discrete Stokes complex in two dimensions, the MWX element method is decoupled into one Lagrange element method of Poisson equation, two Morley element methods of Poisson equation and one nonconforming \$\$P\_1\$\$–\$\$P\_0\$\$element method of Brinkman problem, which implies efficient and robust solvers for the MWX element method. Some numerical examples are provided to verify the theoretical results.},
	language = {en},
	number = {3},
	urldate = {2021-05-06},
	journal = {Journal of Scientific Computing},
	author = {Huang, Xuehai and Shi, Yuling and Wang, Wenqing},
	month = may,
	year = {2021},
	note = {00000},
	pages = {84},
}

@article{xie_skillearn_nodate,
	title = {Skillearn: {Machine} {Learning} {Inspired} by {Humans}’ {Learning} {Skillearn}},
	language = {en},
	author = {Xie, Pengtao},
	note = {00000},
	pages = {114},
}

@article{gessel_mikis_2005,
	title = {On {Miki}'s identity for {Bernoulli} numbers},
	volume = {110},
	issn = {0022314X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0022314X04001416},
	doi = {10.1016/j.jnt.2003.08.010},
	language = {en},
	number = {1},
	urldate = {2021-04-13},
	journal = {Journal of Number Theory},
	author = {Gessel, Ira M.},
	month = jan,
	year = {2005},
	note = {00000},
	pages = {75--82},
}

@article{__2019,
	title = {基于神经网络的知识推理研究综述},
	volume = {55},
	issn = {1002-8331},
	url = {https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CJFQ&dbname=CJFDLAST2019&filename=JSGG201912003&v=MjA1ODZHNEg5ak5yWTlGWjRSOGVYMUx1eFlTN0RoMVQzcVRyV00xRnJDVVI3cWZaT2RvRnk3blVyck9MejdNYWI=},
	abstract = {知识推理是知识图谱补全的重要手段,一直以来都是知识图谱领域的研究热点之一。随着神经网络不断取得新的发展,其在知识推理中的应用在近几年逐渐得到广泛重视。基于神经网络的知识推理方法具备更强的推理能力和泛化能力,对知识库中实体、属性、关系和文本信息的利用率更高,推理效果更好。简要介绍知识图谱及知识图谱补全的相关概念,阐述知识推理的概念及基本原理,从语义、结构和辅助存储三个维度展开,综述当下基于神经网络的知识推理最新研究进展,总结了基于神经网络的知识推理在理论、算法和应用方面存在的问题和发展方向。},
	language = {中文;},
	number = {12},
	urldate = {2020-03-17},
	journal = {计算机工程与应用},
	author = {{张仲伟} and {曹雷} and {陈希亮} and {寇大磊} and {宋天挺}},
	year = {2019},
	keywords = {knowledge graph},
	pages = {8--19+36},
}

@article{__2013,
	title = {国内知识图谱应用研究综述},
	volume = {57},
	issn = {0252-3116},
	url = {https://kns.cnki.net/kns/detail/detail.aspx?QueryID=2&CurRec=6&recid=&FileName=TSQB201303025&DbName=CJFD2013&DbCode=CJFQ&yx=&pr=&URLID=&bsm=QS0101;},
	abstract = {以知识图谱在情报学领域中的应用为例,首先简要介绍情报学知识图谱研究的必要性、现状、发展趋势以及知识图谱绘制工具及方法流程,然后对在国内248篇知识图谱研究文献进行内容分析,全面梳理知识图谱在情报学领域及其子领域的应用概况,并简要介绍知识图谱在其他学科或领域中的应用。},
	language = {中文;},
	number = {03},
	urldate = {2020-03-16},
	journal = {图书情报工作},
	author = {{胡泽文} and {孙建军} and {武夷山}},
	year = {2013},
	pages = {131--137+84},
}

@article{__2017,
	title = {基于中文知识图谱的电商领域问答系统},
	volume = {34},
	issn = {1000-386X},
	url = {https://kns.cnki.net/kns/detail/detail.aspx?QueryID=10&CurRec=4&recid=&FileName=JYRJ201705027&DbName=CJFDLAST2017&DbCode=CJFQ&yx=&pr=&URLID=&bsm=QK0201;},
	abstract = {随着知识图谱的迅速发展,面向知识图谱的中文领域问答系统已成为目前最新最热的研究方向之一,对于提高专业领域服务智能化程度具有较高的意义和价值。针对中文口语语义表达多样化、不符合语法规范以及电商领域特殊性问题,提出一套流式的中文知识图谱自动问答系统CEQA,能够较好地完成电商领域商品咨询以及统计推理等复杂问题,特别是有效地提升了中英文混合商品名称识别、语义链接以及复杂问句的依存分析等方面的性能。实验结果表明,该系统在电商领域问答应用中具有较高的准确率和实用价值。},
	language = {中文;},
	number = {05},
	urldate = {2020-03-12},
	journal = {计算机应用与软件},
	author = {{杜泽宇} and {杨燕} and {贺樑}},
	year = {2017},
	pages = {153--159},
}

@phdthesis{_monte_2020,
	type = {硕士},
	title = {{抛物随机偏微分方程的高效Monte} {Carlo法}},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202101&filename=1020769022.nh&v=Wwdm1HLcXzyj2EkwkXBk7fAvmBRjC%25mmd2BeSWkb1h%25mmd2Bp8LHzYtJrNQ8w6MCivOMKr80fR},
	abstract = {抛物方程是一类非常重要的偏微分方程,它的扩散系数及源项由于材料及环境的不确定性而具有随机性.对于随机项的处理,Monte Carlo方法是一种基本的方法.本文以有限差分法,有限元法为基础结合Monte Carlo方法来研究解随机抛物方程的高效算法.针对一维的随机抛物方程,我们研究了多水平Monte Carlo方法结合中心有限差分法离散,分析了其计算复杂度和误差估计.针对二维随机抛物方程,我们采用ensemble quasi-Monte Carlo方法求解方程,对时间离散采用Crank-Nicolson格式,对空间离散采用有限元法.分析了该离散格式的稳定性和收敛性,得到了较理想的结果.为检验方法...},
	language = {中文;},
	urldate = {2021-05-10},
	school = {贵州大学},
	author = {{向亚红}},
	year = {2020},
	note = {00000},
	keywords = {Multilevel Monte Carlo, finite difference method, finite element method, parabolic equation with random coefficients, quasi-Monte Carlo, 多水平Monte Carlo, 带随机系数的抛物方程, 拟Monte Carlo, 有限元法, 有限差分法},
}

@article{__2016,
	title = {带有随机输入的椭圆偏微分方程的混合有限元方法},
	volume = {50},
	issn = {1006-2467},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2016&filename=SHJT201604024&v=7qthBa4W4DAcRYU7p4TBjeZRhvT3taxhHkvSgQ4OXj4EqhCMo7eSsXwI0dU7iVsV},
	abstract = {对具有齐次Dirichlet边界条件的线性随机椭圆型偏微分方程考虑了一类混合有限元方法,以同时高精度逼近未知函数与其扩散通量的统计矩.理论分析表明该方法对真解及其扩散通量的均值具有一阶最优逼近精度,数值实验也验证了理论结果的正确性.},
	language = {中文;},
	number = {04},
	urldate = {2021-05-10},
	journal = {上海交通大学学报},
	author = {{高蕾} and {谢富纪}},
	year = {2016},
	note = {00000},
	keywords = {mixed finite element methods, numerical experients, optimal order error estimates, stochastic elliptic partial differential equations, 数值实验, 最优误差估计, 混合有限元方法, 随机椭圆偏微分方程},
	pages = {625--630+635},
}

@phdthesis{__2011,
	type = {硕士},
	title = {随机偏微分方程的几类数值格式},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD2011&filename=1011098906.nh&v=tOBxkH8F%25mmd2BSandZiZRuGT6z0jufSLmmnfORdjpyoVUnvih9MIjs0KETMg3TE2E%25mmd2BzV},
	abstract = {本文是一篇关于随机偏微分方程数值求解方法的综述报告.文中介绍了近年来用于求解随机椭圆型偏微分方程的几类有效的数值方法.主要介绍了求解方程系数和右端源项带有随机项的随机问题的MonetCarolGalreikn有限元方法,随机Glarekin有限元方法和随机配置方法,并对上述几类方法进行了比较和分析.},
	language = {中文;},
	urldate = {2021-05-10},
	school = {吉林大学},
	author = {{贾连广}},
	year = {2011},
	note = {00000},
	keywords = {Galerkin finite element method, Monte Carlo Galerkin有限元法, collocation method, stochastic Galerkin finite element method, stochastic partial differential equations, 随机Galerkin有限元方法, 随机偏微分方程, 随机配置方法},
}

@phdthesis{_galerkin_2018,
	type = {博士},
	title = {{一类线性随机偏微分方程的弱Galerkin有限元方法}},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CDFD&dbname=CDFDLAST2019&filename=1019005928.nh&v=MyQtYfYtWHPA1xNeoAQ5%25mmd2BGqSdxL%25mmd2FIaE75IvQTmxSm9vFFNF0YtTmQ7l4rANtntmW},
	abstract = {弱Galerkin有限元方法是一类近似求解偏微分方程的迅速发展的新方法.不同于经典Galerkin方法,它的试探和检验函数空间是由整个区域内完全不连续的分片多项式构成的.该方法已经被广泛应用于很多领域[29,35,38,54,61-63].本文中我们应用该方法求解一类具加性噪音的线性随机偏微分方程模型,我们建立了半离散和全离散近似格式并且在强收敛意义下得到了最优误差估计.本文分为六章:第一章总体概括了本文所研究问题的应用背景和意义;第二章介绍随机方面的一些基础知识,并给出了无穷维空间中Ito型随机积分的定义和具加性噪音的线性随机偏微分方程及其温和解的定义;第三章介绍弱Galerkin有限元方法...},
	language = {中文;},
	urldate = {2021-05-10},
	school = {吉林大学},
	author = {{朱弘泽}},
	year = {2018},
	note = {00000},
	keywords = {RT element, RT元, additive noise, linear stochastic partial differential equations, stability term, weak Galerkin finite element method, 加性噪音, 弱Galerkin有限元方法, 稳定子, 线性随机偏微分方程},
}

@phdthesis{__2019-1,
	type = {硕士},
	title = {一类时间分数阶随机偏微分方程有限元方法},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202001&filename=1019180672.nh&v=1L925qdVZ9i7V0BrUMJ7SP7mBKI9I6%25mmd2FEQunACB5BRznevXiY57MGLO5FY5Mp2L6l},
	abstract = {分数阶随机偏微分方程是近几年来数学界的热门研究方向之一.由于分数阶微积分算子具有遗传性和记忆性,可以描述很多带有噪声扰动的反常扩散现象.因此分数阶随机偏微分方程模型被应用于粘弹性力学,多孔介质的弥散,分型理论,神经科学等多个领域.本篇论文中我们主要介绍了关于Caputo分数阶随机Allen-Cahn方程的温和解的理论分析和数值计算.本文在结构上分为五章来阐述.第一章介绍了随机偏微分方程的研究背景和历史发展过程,并且叙述了随机分数阶微分方程目前国内外的研究现状;第二章,首先介绍了一些预备知识和若干重要的定理引理,其次,构造出了分数阶随机Allen-Cahn方程的温和解;第三章我们利用Picard...},
	language = {中文;},
	urldate = {2021-05-10},
	school = {河南大学},
	author = {{蔡晓莉}},
	year = {2019},
	note = {00000},
	keywords = {Allen-Cahn equation, Allen-Cahn方程, Error analysis, Finite element method, Fractional order, Mild solution, 分数阶, 有限元, 温和解, 误差分析},
}

@article{__2015,
	title = {异构众核系统及其编程模型与性能优化技术研究综述},
	volume = {43},
	issn = {0372-2112},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2015&filename=DZXU201501018&v=8e2FkS%25mmd2FYTwGIYAmoSj1B%25mmd2Fma1ysGhsvDp4hkNEWTLGwMaWD%25mmd2Fxp8%25mmd2FaPzwxK2%25mmd2BMfmKC},
	abstract = {异构众核系统已成为当前高性能计算领域重要的发展趋势.针对异构众核系统,从架构、编程、所支持的应用三方面分析对比当前不同异构系统的特点,揭示了异构系统的发展趋势及异构系统相对于传统多核并行系统的优势;然后从编程模型和性能优化方面分析了异构系统存在的问题和面临的挑战,以及国内外研究现状,结合当前研究存在的问题和难点,探讨了该领域进一步深入的研究方向;同时对两种典型的异构众核系统CPU+GPU和CPU+MIC进行不同应用类型的Benchmark测试,验证了两种异构系统不同的应用特点,为用户选择具体异构系统提供参考,在此基础上提出将两种众核处理器(GPU和MIC)结合在一个计算节点内构成新型混合异构系...},
	language = {中文;},
	number = {01},
	urldate = {2021-03-04},
	journal = {电子学报},
	author = {{巨涛} and {朱正东} and {董小社}},
	year = {2015},
	note = {00000},
	pages = {111--119},
}

@article{__nodate,
	title = {基于深度强化学习的组合优化研究进展},
	issn = {0254-4156},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CAPJ&dbname=CAPJLAST&filename=MOTO20201207000&v=JCreBvAGjWjh1cvanENm98eEoyCVJ7t15dD0jZ6mJ%25mmd2Bm3f%25mmd2B67Gup4zpCm2%25mmd2FQU8VWA},
	abstract = {组合优化问题广泛存在于国防、交通、工业、生活等各个领域， 几十年来， 传统运筹优化方法是解决组合优化问题的主要手段， 但随着实际应用中问题规模的不断扩大、求解实时性的要求越来越高， 传统运筹优化算法面临着很大的计算压力， 很难实现组合优化问题的在线求解. 近年来随着深度学习技术的迅猛发展， 深度强化学习在围棋、机器人等领域的瞩目成果显示了其强大的学习能力与序贯决策能力. 鉴于此， 近年来涌现出了多个利用深度强化学习方法解决组合优化问题的新方法， 具有求解速度快、模型泛化能力强的优势， 为组合优化问题的求解提供了一种全新的思路. 因此本文总结回顾近些年利用深度强化学习方法解决组合优化问题的相关理...},
	language = {中文},
	urldate = {2021-03-03},
	journal = {自动化学报},
	author = {{李凯文} and {张涛} and {王锐} and {覃伟健} and {贺惠晖} and {黄鸿}},
	note = {00000},
	pages = {1--22},
}

@phdthesis{__2013-1,
	type = {硕士},
	title = {一类四阶发展型随机微分方程的数值计算},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201302&filename=1013193582.nh&v=XZkR5u5V%25mmd2BIz2%25mmd2BhBshDYCL%25mmd2BHQRAZKjkQqFBN1ovxfBlBRUU4oMX%25mmd2BapIliIZAIcweL},
	abstract = {随着偏微分方程的不断发展,人们越来越发现确定的方程模型已经不能完全解决实际中的问题,因此随机偏微分方程的理论研究和计算就应运而生,而且成为了当今数学领域中比较重要的领域. 	本文主要考虑如下四阶线性随机偏微分方程： 	的近似数值解法. 	在第一章里我们主要介绍有关随机偏微分方程的半群理论.第二章里我们着重介绍随机偏微分方程的一些理论知识和性质.第三章中,我们首先给出了数值求解确定性问题的有限元框架,然后主要讨论了Argyris元方法.最后一章,我们分析了随机偏微分方程的逼近问题,并给出了数值实验.},
	language = {中文;},
	urldate = {2021-05-10},
	school = {吉林大学},
	author = {{张稳}},
	year = {2013},
	note = {00000},
	keywords = {Argyris element, Argyris元, Finite, Semigroup, Stochastic partial differential equation, Wiener process, Wiener过程, element Method, 半群, 有限元方法, 随机偏微分方程},
}

@article{__2007,
	title = {二或三维空间上椭圆型随机偏微分方程的间断有限元方法},
	issn = {1006-9232},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFD2007&filename=JAXK200707008&v=RlNL6sNW252dJ53HiMmaDxuFIM5SiZuPV4A%25mmd2BfwYLiMrIAdauTM2nBHSq55%25mmd2FhTbeD},
	abstract = {本文研究由空间白噪声驱动带有齐次Dirichlet边界条件的一类二或三维椭圆型随机偏微分方程的间断有限元数值方法．建立了这种方法的L{\textasciitilde}2范数误差估计．特别地,给出了一个维数d=2的数值例子．},
	language = {中文;},
	number = {07},
	urldate = {2021-05-10},
	journal = {中国科学(A辑:数学)},
	author = {{姚瑞明} and {薄立军}},
	year = {2007},
	note = {00000},
	keywords = {椭圆型随机偏微分方程, 白噪声, 间断有限元法},
	pages = {838--850},
}

@phdthesis{__2018,
	type = {博士},
	title = {几类随机微分方程和随机偏微分方程数值解法研究},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CDFD&dbname=CDFDLAST2018&filename=1018093992.nh&v=4AfheiI%25mmd2FCszJfxocRW4SY1mgh8O%25mmd2BX5GfwpIFXMNoniIPjwzIdvU7XOoc6pD1nEGu},
	abstract = {随着人类社会的发展和科学技术的不断进步,随机微分方程(SDEs)和随机偏微分方程(SPDEs)在生物、金融、物理、化学、医学以及工程等领域发挥着越来越重要的作用.通常情况下,大多数SDEs和SPDEs的显式的解析解很难获得,因此,构造适当的数值算法求解SDEs和SPDEs具有十分重要的理论意义和实际应用价值.本文的研究内容有两部分组成.第一部分研究两类跳扩散随机微分方程(JSDEs)的数值算法的强收敛性分析和误差估计.首先对漂移系数只满足单边Lipschitz条件和局部Lipschitz条件的JSDEs,研究分析一类随机分步θ-格式的强收敛性和误差估计,理论结果通过数值试验得到进一步验证;其次...},
	language = {中文;},
	urldate = {2021-05-10},
	school = {山东大学},
	author = {{杨旭}},
	year = {2018},
	note = {00000},
	keywords = {Error Estimates, Finite Element Method, Jump-Adapted Method, Jump-Diffusion Stochastic Differential Equations, Numerical Simulation, Stochastic Differential Equations, Stochastic Partial Differential Equations, Stochastic Split-Step Scheme, Strong Convergence, 倒向随机偏微分方程, 强收敛, 数值模拟, 有限元方法, 误差估计, 跳扩散随机微分方程, 跳适应方法, 随机偏微分方程, 随机分步格式, 随机微分方程},
}

@article{__2019-2,
	title = {并行代数多重网格算法:大规模计算应用现状与挑战},
	volume = {40},
	issn = {1000-3266},
	shorttitle = {并行代数多重网格算法},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2020&filename=SZJS201904001&v=4mzkmw6k9PXvKJy08XPjFREgfTuRqm6OM8kikk66nT2WtqYntpPvRVwdFA4IA3cv},
	abstract = {代数多重网格(AMG)是求解偏微分方程离散线性代数方程组最有效的算法之一,广泛应用于科学与工程计算领域实际问题的大规模数值模拟.随着超级计算机性能不断提升,实际数值模拟的计算规模和并行规模越来越大,同时,实际问题应用特征和计算机体系结构特征越来越复杂,AMG面临并行可扩展、算法可扩展和浮点性能优化的严峻挑战.本文结合大规模计算的发展趋势,特别是面向即将到来的百亿亿次(E级)计算,分析AMG算法在这三个方面的挑战,总结研究现状与进展,展望未来研究重点.},
	language = {中文;},
	number = {04},
	urldate = {2021-03-04},
	journal = {数值计算与计算机应用},
	author = {{徐小文}},
	year = {2019},
	note = {00000},
	pages = {243--260},
}

@phdthesis{__2020,
	type = {博士},
	title = {间断有限元方法和随机偏微分方程的缩减基方法的研究},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CDFD&dbname=CDFDLAST2021&filename=1020098320.nh&v=FlW6DGxUoWZGP%25mmd2Bm2z9ZD39YJpV2dN%25mmd2FLSzXR%25mmd2B02of3nwjgsj%25mmd2B9MfWSOGaO7xjXbxv},
	abstract = {本文研究间断有限元(discontinuous Galerkin,简称DG)方法求解偏微分方程的数值分析,及其在可压缩磁流体动力学(Magnetohydrodynamic,简称MHD)中的数值模拟,以及利用缩减基方法(reduced basis method,简称RBM)加快对随机偏微分方程的数值求解。论文主要分成两个部分。第一部分包括DG方法的数值分析和MHD数值模拟的研究。数值分析上,我们主要利用一种平移技术构造了一种全新的特殊投影算子,并分析了投影算子的有界性,证明了对于线性的双曲守恒律方程,交错网格上的中心间断有限元(central DG,简称CDG)方法的半离散格式的最优误差估计。在...},
	language = {中文;},
	urldate = {2021-05-10},
	school = {中国科学技术大学},
	author = {{刘勇}},
	year = {2020},
	note = {00000},
	keywords = {Discontinuous Galerkin method, compressible MHD, entropy stability, hyperbolic conservation laws, locally divergence-free, optimal error estimates, reduced basis method, stochastic PDE, superconvergence, 双曲守恒律, 可压磁流体方程, 局部散度为零, 最优误差估计, 熵稳定, 缩减基方法, 超收敛, 间断Galerkin方法, 随机偏微分方程},
}

@article{__2012,
	title = {大型稀疏法方程组的代数多重网格解法},
	volume = {29},
	issn = {1673-6338},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFD2012&filename=JFJC201201004&v=lzXQwcUflHOQY2nyxhxl3FT3ReaEa%25mmd2ByG3c8ilJkoK9Gwvxvx%25mmd2Br7RKXs8H4DMB2gg},
	abstract = {测量平差中经常会遇到大型稀疏法方程组的求解。传统的线性方程组迭代解法能够很快平滑误差分量中的高频分量;但对于低频分量衰减很慢。代数多重网格算法通过建立多重网格,并在不同的网格层上分别处理高低频误差分量,将所有层相互协调起来求解同一问题。这对于大规模稀疏线性方程组的求解,具有高效性。这里介绍了代数多重网格算法,并进行了改进,得到了AMG-CG算法。数值算例表明,代数多重网格算法(AMG)以及改进的AMG-CG算法对求解大型稀疏法方程组具有高效性和数值稳定性,改进后的AMG-CG算法在计算效率上进一步提高,对于大型稀疏法方程组的求解是可行有效的算法。},
	language = {中文;},
	number = {01},
	urldate = {2020-10-13},
	journal = {测绘科学技术学报},
	author = {{郭飞霄} and {杨力} and {刘荣} and {汪菲菲}},
	year = {2012},
	note = {00000},
	pages = {5--8},
}

@phdthesis{_brinkman_2019,
	type = {硕士},
	title = {{弱有限元方法在二阶椭圆问题和Brinkman问题的应用}},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD202001&filename=1019160984.nh&v=32C95ph0bHaM%25mmd2BlO7izevxSlulwAx3Ny5mNa%25mmd2Ff9e9ciZT06Szx%25mmd2BCTJKrdgNw%25mmd2FceAv},
	abstract = {弱 Galerkin(WG)有限元方法(weak Galerkin finite element method)作为计算数学领域的后起之秀,常被用于求解各类与偏微分方程相关的问题.它通过定义弱微分算子来代替传统的微分算子,从而克服了传统有限元方法在选取近似函数方面的困难.本文首先取不带稳定子的混合形式的WG有限元方法来处理二阶椭圆问题,并进行求解,对标量值函数u及向量值函数g进行了相应的数值离散,并给出了相应的误差方程及两个函数分别在H1及L2范数下的最优阶收敛性分析.其次,本文用带稳定子的WG有限元方法研究了时间相关Brinkman方程.得到了两种数值格式,一种是只对空间变量进行离散的半离散...},
	language = {中文;},
	urldate = {2020-10-07},
	school = {吉林大学},
	author = {{孙立娜}},
	year = {2019},
	note = {00000},
}

@phdthesis{__2009,
	type = {博士},
	title = {四阶椭圆方程的非协调有限元方法},
	url = {https://chn.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CDFD&dbname=CDFD0911&filename=2009243179.nh&v=O2Mp22MSVJ2JP0vxhohNhuw8lOiEm543ogiE7nYP9fPNazzafFeRrZqWk1FBaYRq},
	abstract = {在论文中,我们主要讨论了四阶椭圆问题的一些非协调有限元逼近。由于技术上的困难,我们通常采用非协调有限元来逼近四阶问题。但是,并不是所有的板元对四阶奇异摄动问题都关于摄动参数一致收敛,而且大多数的讨论都是在网格满足正则性条件或拟一致假设的前提下进行的。本文主要是在不同的网格剖分下讨论了一些非协调板元应用到四阶板弯曲问题和四阶奇异摄动问题时的收敛性。
第一章,我们给出了一些基本空间的定义,记号和有限元方法的一些基础知识以及其相关的性质。
第二章,我们给出了两个Morley型的非C{\textasciitilde}0非协调板元在各向异性网格下对四阶板弯曲问题的逼近,利用Poincare不等式得到了插...},
	language = {中文;},
	urldate = {2020-10-01},
	school = {复旦大学},
	author = {{谢萍丽}},
	year = {2009},
	note = {00000},
	keywords = {Morley},
}

@phdthesis{__2010,
	type = {博士},
	title = {四阶椭圆型方程若干有限元新方法和高效求解算法},
	url = {https://chn.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CDFD&dbname=CDFD0911&filename=2010113624.nh&v=37j4xnI9Mu0KOA9zasoXaZkcOjRHgZpjMJcPLYLYCWhNU6YpSVVFrOi5piYt4ZPI},
	abstract = {四阶椭圆型微分方程广泛应用于固体力学、材料科学和图像处理诸领域,因此对它的数值解研究不但具有重要的理论意义也具有直接应用价值.本文的主要工作是构造了重调和方程的一种基于Poisson求解子的快速求解器;对四阶椭圆偏微分方程的MWX元方法设计了两网格局部和并行求解算法并进行了误差分析;将C0间断有限元方法应用于四阶椭圆偏微分方程并给出求解由该方法离散得到的线性代数方程组的区域分解法;也提出了自适应有限元方法的一个抽象框架.
首先,构造了重调和方程的一种基于Poisson求解子的快速求解器.通过位能极小原理,建立了重调和方程和Stokes方程之间的等价性,即重调和方程等价于一...},
	language = {中文;},
	urldate = {2020-10-01},
	school = {上海交通大学},
	author = {{黄学海}},
	year = {2010},
	note = {00000},
	keywords = {Morley},
}

@article{_morley_1994,
	title = {Morley单元在复合材料层合壳体分析中的推广},
	issn = {10064869},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFD9495&filename=NCSB401.004&v=ODN6OyDZ%25mmd2BC283CyrYS3jFcJDFGQQ2CM00fWxMDNcQUVi%25mmd2BAqsdUPsVJhL3KkG8BC7},
	abstract = {本文将Ａｏｒｌｅｙ平板弯曲单元推广到复合材料层合壳体的分析中，算例表明，由Ｍｏｒｌｅｙ元构造的复合材料平板壳元具有很好的实用价值。},
	language = {中文},
	number = {01},
	urldate = {2020-10-01},
	journal = {南昌水专学报},
	author = {{孙辉}},
	year = {1994},
	note = {00000},
	keywords = {Morley},
	pages = {22--27},
}

@article{_morley_2017,
	title = {{二阶椭圆问题基于Morley元离散的内点惩罚间断有限元方法}},
	volume = {38},
	issn = {1674-3563},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2017&filename=WZSF201704002&v=l6I3vtlebEe3kN%25mmd2FGRQUToHu84FyrTuz%25mmd2BIqS4PJ98pwMAjNXLFb3nNr7h2zdeCimI},
	abstract = {本文在离散的二阶椭圆边界值问题的基础上,通过引进内点惩罚项,构造了基于Morley元离散的内点惩罚间断有限元方法,并对此离散方法进行了先验误差分析.},
	language = {中文;},
	number = {04},
	urldate = {2020-10-01},
	journal = {温州大学学报(自然科学版)},
	author = {{汤凯} and {黄学海} and {王文庆}},
	year = {2017},
	note = {00000},
	keywords = {Morley},
	pages = {7--12},
}

@article{_matlab_2010,
	title = {{利用有限差分和MATLAB矩阵运算直接求解二维泊松方程}},
	volume = {32},
	issn = {1001-8891},
	url = {https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CJFQ&dbname=CJFD2010&filename=HWJS201004008&uid=WEEvREcwSlJHSldRa1Fhb09pSnNwVys1cldDWTNVUnlUQkVIU0cwN3cyRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!&v=MTU3MzJxNDlGYklSOGVYMUx1eFlTN0RoMVQzcVRyV00xRnJDVVI3cWZZT2R0RmlEZ1U3N0pMVHJCZmJHNEg5SE0=},
	abstract = {根据有限差分法原理,将求解范围用等间距网格划分为一系列离散节点后,二维泊松方程可以转化为用一个矩阵方程表示的关于各未知节点的多元线性方程组。利用MATLAB提供的矩阵左除命令,即可得到各未知节点的函数近似值。该方法概念简单,使用方便,不需要花费较多精力编程即可以求解大型线性方程组。},
	language = {中文;},
	number = {04},
	urldate = {2020-07-12},
	journal = {红外技术},
	author = {{王忆锋} and {唐利斌}},
	year = {2010},
	note = {00000},
	pages = {213--216+230},
}

@article{_poissonmatlab_2018,
	title = {{有限元法求解二维Poisson方程的MATLAB实现}},
	volume = {37},
	issn = {1009-4970},
	url = {https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CJFQ&dbname=CJFDLAST2018&filename=LSZB201805005&uid=WEEvREcwSlJHSldRa1Fhb09pSnNwVys1cldDWTNVUnlUQkVIU0cwN3cyRT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!&v=MDMwNDhZT2R0RmlEZ1U3ekFLVDdSYkxHNEg5bk1xbzlGWVlSOGVYMUx1eFlTN0RoMVQzcVRyV00xRnJDVVI3cWY=},
	abstract = {文章讨论了圆形区域上的三角形单元剖分、有限元空间,通过变分形式离散得到有限元方程.用MATLAB编程求得数值解,并进行了误差分析.},
	language = {中文;},
	number = {05},
	urldate = {2020-07-12},
	journal = {洛阳师范学院学报},
	author = {{陈莲} and {郭元辉} and {邹叶童}},
	year = {2018},
	note = {00000},
	pages = {15--18},
}

@article{_navier-stokes_2020,
	title = {{不可压缩黏性流体的二维Navier}-{Stokes方程的间断有限元模拟}},
	volume = {41},
	issn = {1000-0887},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2020&filename=YYSX202008003&v=wo3TmsgHqjPVrNRZs2gT6c77lKOGgSvH25VAwLYZWE10ZNyTz%25mmd2Bolia25OpqgqKAQ},
	abstract = {由于不可压缩Navier-Stokes方程由守恒律、扩散及约束发展方程混合构成,为测试数值方法,该文基于非结构网格,对该方程建立了DG(discontinuous Galerkin)格式,讨论了不同黏性系数ν在方腔涡流问题的数值结果,验证了该方法的有效性且不依赖于问题的维数.圆柱绕流问题的模拟结果进一步表明此方法精度高、可有效求解具有运动界面的不可压缩黏性流体问题,使得模拟边界层、剪切层及复杂涡流解十分有效,并可以成功地推广到解决复杂现象数值模拟中的激波结构.},
	language = {中文;},
	number = {08},
	urldate = {2020-10-07},
	journal = {应用数学和力学},
	author = {{陈亚飞} and {郑云英}},
	year = {2020},
	note = {00000},
	keywords = {Stokes},
	pages = {844--852},
}

@article{_navier-stokes_2020-1,
	title = {{定常不可压Navier}-{Stokes方程的并行有限元算法}},
	volume = {38},
	issn = {1004-5570},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2020&filename=NATR202003001&v=8sJApo9nqfMLrarY9jW5j4RsZBlMGoufbuSxoIRj%25mmd2BchZL42JzK3GOXCt74Ey5nhJ},
	abstract = {Navier-Stokes(N-S)方程组是描述流体运动的基本方程组,其数值模拟对我国的国防建设与工业设计非常重要。在高性能并行机和并行计算技术飞速发展的今天,其并行数值计算方法的研究是当前计算流体力学领域最前沿的热门课题之一。基于局部与并行有限元离散技巧和区域分解方法,给出了数值求解定常不可压N-S方程的若干高效并行算法,这些算法实现简单,稍加修改现有的串行程序即可实现并行计算,通信需求少,能快速有效地模拟复杂的流体流动行为。我们给出了一些理论结果和数值算例,验证这些算法的有效性。},
	language = {中文;},
	number = {03},
	urldate = {2020-10-07},
	journal = {贵州师范大学学报(自然科学版)},
	author = {{尚月强} and {郑波} and {周康瑞} and {丁琪}},
	year = {2020},
	note = {00000},
	keywords = {Stokes},
	pages = {1--13},
}

@phdthesis{_morley-wang-xu_2018,
	type = {硕士},
	title = {{四阶椭圆奇异扰动问题基于Morley}-{Wang}-{Xu元离散的超惩罚法}},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201901&filename=1018285263.nh&v=ngRQ3D2xFXRTqvKKVHGolB1HM9tnbWAysiRuf7U0xBNw%25mmd2BIg%25mmd2BpzoW1nxl8pi7CxCe},
	abstract = {本文将考虑在间断有限元方法的基础上,用王鸣和许进超定义的Morley-Wang-Xu元对四阶椭圆奇异扰动问题进行数值求解,而对于此四阶椭圆问题的变分形式,本文将使用超惩罚方法,并进行误差分析以及数值模拟.首先考虑当ε=0,四阶椭圆奇异扰动问题退化成二阶问题,即Poisson方程时的情况,在变分形式上加上Laplace算子的超惩罚项,在精确解具有适当正则性假设下,利用三角不等式,Green公式,Cauchy-Schwarz不等式以及逆迹不等式,对此二阶椭圆问题进行先验误差分析.对于二阶问题的后验误差估计,在原来的条件下先定义两个算子\_hL和Π\_h,建立后验误差估计的一些局部下界估计式,利用已知的...},
	language = {中文;},
	urldate = {2020-10-01},
	school = {温州大学},
	author = {{周瑞月}},
	year = {2018},
	note = {00000},
	keywords = {Morley},
}

@article{_morley_1990,
	title = {{关于Morley元的误差估计}},
	issn = {0254-7791},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFD9093&filename=JSSX199002000&v=4UU0Irc6mVPrFmfIwxp%25mmd2BVRX31wvaggF%25mmd2BbQ1RS%25mmd2FI0ptn93Kc97qGWfY2z7i3DBEAG},
	abstract = {\&lt;正\&gt; §1.引言 解薄板弯曲问题的三角形Morley元是六十年代出现的一种非协调元,它的形函数是完整的二次多项式,节点参数是单元顶点上的三个函数值及三边中点上的法向导数值.由于板弯曲问题的常应变是二次多项式,所以这是一个参数最少的非协调板元.由},
	language = {中文;},
	number = {02},
	urldate = {2020-10-01},
	journal = {计算数学},
	author = {{石钟慈}},
	year = {1990},
	note = {00000},
	keywords = {Morley},
	pages = {113--118},
}

@article{_stokes_1987,
	title = {Stokes问题的混合有限元分析},
	issn = {0254-7791},
	url = {https://kns.cnki.net/KCMS/detail/detail.aspx?dbcode=CJFD&dbname=CJFD8589&filename=JSSX198701006&uid=WEEvREcwSlJHSldSdmVqMVc3ejRzUlZ1QXZMVW5MdWh5aEdkWEIxVHN5UT0=$9A4hF_YAuvQ5obgVAqNKPCYcEjKensW4ggI8Fm4gTkoUKaID8j8gFw!!&v=MDA2ODdTN0RoMVQzcVRyV00xRnJDVVI3cWZiK2RuRnl6aFY3M0JMejdZZHJLeEZ0Yk1ybzlGWW9SOGVYMUx1eFk=},
	abstract = {\&lt;正\&gt; 在问题(ST)中,u=(u\_1,u\_2){\textasciitilde}T是流体速度,p是压力. 设X\_h及M\_h分别为(H\_0{\textasciitilde}1(Ω)){\textasciitilde}2及L\_0{\textasciitilde}2(Ω)的有限元离散空间。且X\_h(H\_0{\textasciitilde}1(Ω)){\textasciitilde}2,M\_h?},
	language = {中文;},
	number = {01},
	urldate = {2020-08-18},
	journal = {计算数学},
	author = {{王烈衡}},
	year = {1987},
	note = {00000},
	keywords = {Stokes},
	pages = {70--81},
}

@phdthesis{_morley_2014,
	type = {硕士},
	title = {{求解四阶椭圆问题矩形Morley元方法的多重网格算法}},
	url = {https://chn.oversea.cnki.net/KCMS/detail/detail.aspx?dbcode=CMFD&dbname=CMFD201501&filename=1014340541.nh&v=mNDX1H0Mg2qfrbKwoVmjcdpQ0gCbzhIvP1cE4dAFHCCn9T8N4kibDLXXmlVZ69T2},
	abstract = {在本文中,我们研究了求解四阶椭圆问题矩形Morley元方法的多重网格算法.
首先,我们给出了四阶椭圆问题的变分形式,介绍了矩形Morley元以及四阶椭圆问题的矩形Morley元的离散逼近.接着,我们设计了一个网格转移算子,证明了网格转移算子的稳定性和逼近性.然后,我们提出了W循环多重网格算法,给出了光滑性和逼近性的证明,并证明了该算法的最优收敛性.最后,我们通过数值算例验证了理论结果.},
	language = {中文;},
	urldate = {2020-10-01},
	school = {南京师范大学},
	author = {{乔培娟}},
	year = {2014},
	note = {00000},
	keywords = {Morley},
}

@article{_morley-wang-xu_2017,
	title = {{二阶椭圆问题基于Morley}-{Wang}-{Xu元离散的超惩罚法}},
	volume = {38},
	issn = {1674-3563},
	url = {https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CJFD&dbname=CJFDLAST2017&filename=WZSF201703004&v=l6I3vtlebEeKEQk5%25mmd2BczwEWG6ut3vHaKxMd0LRlSj2MYACiZA5tsRb1IUuYQPFfq4},
	abstract = {讨论了在间断有限元方法基础上,对二阶椭圆问题加上超惩罚项,用修改后的Morley-Wang-Xu元解二阶椭圆边界值问题,并进行了误差分析.},
	language = {中文;},
	number = {03},
	urldate = {2020-10-01},
	journal = {温州大学学报(自然科学版)},
	author = {{周瑞月} and {黄学海} and {王文庆}},
	year = {2017},
	note = {00000},
	keywords = {Morley},
	pages = {23--29},
}

@article{han_equation_2012,
	title = {An equation decomposition method for the numerical solution of a fourth-order elliptic singular perturbation problem},
	volume = {28},
	number = {3},
	journal = {Numerical Methods for Partial Differential Equations},
	author = {Han, Houde and Huang, Zhongyi},
	year = {2012},
	keywords = {foesp},
	pages = {942--953},
}

@book{karlin_game_2017,
	address = {Providence, Rhode                     Island},
	title = {Game {Theory}, {Alive}},
	isbn = {978-1-4704-1982-0 978-1-4704-3667-4},
	url = {http://www.ams.org/mbk/101},
	language = {en},
	urldate = {2021-05-13},
	publisher = {American Mathematical                     Society},
	author = {Karlin, Anna and Peres, Yuval},
	month = apr,
	year = {2017},
	doi = {10.1090/mbk/101},
}

@misc{noauthor___nodate,
	title = {中国知网\_百度百科},
	url = {https://baike.baidu.com/item/%E4%B8%AD%E5%9B%BD%E7%9F%A5%E7%BD%91},
	urldate = {2021-05-12},
	note = {00000},
}

@misc{noauthor_google-researchtorchsde_2021,
	title = {google-research/torchsde},
	copyright = {Apache-2.0 License         ,                      Apache-2.0 License},
	url = {https://github.com/google-research/torchsde},
	abstract = {Differentiable SDE solvers with GPU support and efficient sensitivity analysis.},
	urldate = {2021-05-12},
	publisher = {Google Research},
	month = may,
	year = {2021},
	note = {original-date: 2020-07-06T23:13:11Z},
	keywords = {deep-learning, deep-neural-networks, differential-equations, dynamical-systems, neural-differential-equations, pytorch, stochastic-differential-equations, stochastic-processes, stochastic-volatility-models},
}

@book{manning_foundations_1999,
	edition = {1},
	title = {Foundations of {Statistical} {Natural} {Language} {Processing}},
	isbn = {978-0-262-13360-9},
	url = {http://gen.lib.rus.ec/book/index.php?md5=1eba924b51130fe7a97a25c076636f8d},
	urldate = {2021-05-10},
	publisher = {The MIT Press},
	author = {Manning, Christopher D. and Schuetze, Hinrich},
	year = {1999},
	note = {00000},
}

@article{merity_pointer_nodate,
	title = {Pointer {Sentinel} {Mixture} {Models}},
	language = {en},
	author = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
	keywords = {Abs, Code},
	pages = {13},
}

@book{stroock_introduction_2005,
	address = {Berlin Heidelberg},
	series = {Graduate {Texts} in {Mathematics}},
	title = {An {Introduction} to {Markov} {Processes}},
	isbn = {978-3-540-23451-7},
	url = {https://www.springer.com/gp/book/9783540234517},
	abstract = {To some extent, it would be accurate to summarize the contents of this book as an intolerably protracted description of what happens when either one raises a transition probability matrix P (i. e. , all entries (P)»j are n- negative and each row of P sums to 1) to higher and higher powers or one exponentiates R(P — I), where R is a diagonal matrix with non-negative entries. Indeed, when it comes right down to it, that is all that is done in this book. However, I, and others of my ilk, would take offense at such a dismissive characterization of the theory of Markov chains and processes with values in a countable state space, and a primary goal of mine in writing this book was to convince its readers that our offense would be warranted. The reason why I, and others of my persuasion, refuse to consider the theory here as no more than a subset of matrix theory is that to do so is to ignore the pervasive role that probability plays throughout. Namely, probability theory provides a model which both motivates and provides a context for what we are doing with these matrices. To wit, even the term "transition probability matrix" lends meaning to an otherwise rather peculiar set of hypotheses to make about a matrix.},
	language = {en},
	urldate = {2021-05-10},
	publisher = {Springer-Verlag},
	author = {Stroock, Daniel W.},
	year = {2005},
	doi = {10.1007/b138428},
	note = {00000 },
}

@inproceedings{welleck_neural_2020,
	title = {Neural {Text} {Generation} {With} {Unlikelihood} {Training}},
	url = {https://iclr.cc/virtual_2020/poster_SJeYe0NtvH.html},
	abstract = {Neural text generation is a key tool in natural language applications, but it is well known there are major problems at its core. In particular, standard likelihood training and decoding leads to dull and repetitive outputs. While some post-hoc fixes have been proposed, in particular top-k and nucleus sampling, they do not address the fact that the token-level probabilities predicted by the model are poor. In this paper we show that the likelihood objective itself is at fault, resulting in a model that assigns too much probability to sequences containing repeats and frequent words, unlike those from the human training distribution. We propose a new objective, unlikelihood training, which forces unlikely generations to be assigned lower probability by the model. We show that both token and sequence level unlikelihood training give less repetitive, less dull text while maintaining perplexity, giving superior generations using standard greedy or beam search. According to human evaluations, our approach with standard beam search also outperforms the currently popular decoding methods of nucleus sampling or beam blocking, thus providing a strong alternative to existing techniques.},
	language = {en},
	urldate = {2021-05-07},
	author = {Welleck, Sean and Kulikov, Ilia and Roller, Stephen and Dinan, Emily and Cho, Kyunghyun and Weston, Jason},
	month = apr,
	year = {2020},
	note = {00000},
	keywords = {Abs, Code},
}

@book{papadopoulos_stochastic_2018,
	address = {Cham},
	series = {Mathematical {Engineering}},
	title = {Stochastic {Finite} {Element} {Methods}},
	isbn = {978-3-319-64527-8 978-3-319-64528-5},
	url = {http://link.springer.com/10.1007/978-3-319-64528-5},
	language = {en},
	urldate = {2021-05-10},
	publisher = {Springer International Publishing},
	author = {Papadopoulos, Vissarion and Giovanis, Dimitris G.},
	year = {2018},
	doi = {10.1007/978-3-319-64528-5},
	note = {00000 },
}

@article{stefanou_stochastic_2009,
	title = {The stochastic finite element method: {Past}, present and future},
	volume = {198},
	issn = {0045-7825},
	shorttitle = {The stochastic finite element method},
	url = {https://www.sciencedirect.com/science/article/pii/S0045782508004118},
	doi = {10.1016/j.cma.2008.11.007},
	abstract = {A powerful tool in computational stochastic mechanics is the stochastic finite element method (SFEM). SFEM is an extension of the classical deterministic FE approach to the stochastic framework i.e. to the solution of static and dynamic problems with stochastic mechanical, geometric and/or loading properties. The considerable attention that SFEM received over the last decade can be mainly attributed to the spectacular growth of computing power rendering possible the efficient treatment of large-scale problems. This article aims at providing a state-of-the-art review of past and recent developments in the SFEM area and indicating future directions as well as some open issues to be examined by the computational mechanics community in the future.},
	language = {en},
	number = {9},
	urldate = {2021-05-10},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Stefanou, George},
	month = feb,
	year = {2009},
	note = {00000},
	keywords = {Monte Carlo simulation, Parallel processing, Solution techniques, Stochastic finite elements, Stochastic partial differential equations, Stochastic processes and fields},
	pages = {1031--1051},
}

@article{contreras_stochastic_1980,
	title = {The stochastic finite-element method},
	volume = {12},
	issn = {0045-7949},
	url = {https://www.sciencedirect.com/science/article/pii/0045794980900310},
	doi = {10.1016/0045-7949(80)90031-0},
	abstract = {A generic stochastic finite-element method for modeling structures is proposed as a means to analyze and design structures in a probabilistic framework. Stochastic differential and difference equation theory is applied in structures discretized with the finite-element methodology.Transient structural loads, idealized as stochastic processes, are incorporated into finite-element dynamic models with uncertain parameters. An estimate of the probability of failure based on known and established procedures in second-moment reliability analysis can be made with the aid of a transformation to gaussian space of the random variables that define structural reliability. The stochastic finite-element method will facilitate the use of probabilistic mathematical structural models for structural code development or design of important structures. It will also permit better estimation of structural reliability, which, when combined with risk analysis, could lead to improved decision-making processes.},
	language = {en},
	number = {3},
	urldate = {2021-05-10},
	journal = {Computers \& Structures},
	author = {Contreras, Humberto},
	month = sep,
	year = {1980},
	note = {00000},
	pages = {341--348},
}

@article{tamborrino_neural_nodate,
	title = {Neural network connectivity and response latency modeled by stochastic processes},
	language = {en},
	author = {Tamborrino, Massimiliano},
	pages = {156},
}

@article{zapranis_weather_2009,
	title = {Weather derivatives pricing: {Modeling} the seasonal residual variance of an {Ornstein}–{Uhlenbeck} temperature process with neural networks},
	volume = {73},
	issn = {09252312},
	shorttitle = {Weather derivatives pricing},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231209002422},
	doi = {10.1016/j.neucom.2009.01.018},
	abstract = {In this paper, we use neural networks in order to model the seasonal component of the residual variance of a mean-reverting Ornstein-Uhlenbeck temperature process, with seasonality in the level and volatility. We also use wavelet analysis to identify the seasonality component in the temperature process as well as in the volatility of the temperature anomalies. Our model is validated on more than 100 years of data collected from Paris, one of the European cities traded at Chicago Mercantile Exchange. Our results show a significant improvement over more traditional alternatives, regarding the statistical properties of the temperature process, which can be used in the context of MonteCarlo simulations for pricing weather derivatives.},
	language = {en},
	number = {1-3},
	urldate = {2021-05-10},
	journal = {Neurocomputing},
	author = {Zapranis, Achilleas and Alexandridis, Antonis},
	month = dec,
	year = {2009},
	pages = {37--48},
}

@book{klebaner_introduction_2005,
	address = {London ; Hackensack, N.J},
	edition = {2nd ed},
	title = {Introduction to stochastic calculus with applications},
	isbn = {978-1-86094-555-7 978-1-86094-566-3},
	language = {en},
	publisher = {Imperial College Press},
	author = {Klebaner, Fima C.},
	year = {2005},
	note = {00000 
OCLC: ocm61263806},
	keywords = {Calculus, Stochastic analysis},
}

@misc{stanfordonline_stanford_2019,
	title = {Stanford {CS230}: {Deep} {Learning} {\textbar} {Autumn} 2018 {\textbar} {Lecture} 8 - {Career} {Advice} / {Reading} {Research} {Papers}},
	shorttitle = {Stanford {CS230}},
	url = {https://www.youtube.com/watch?v=733m6qBH-jI},
	abstract = {Andrew Ng, Adjunct Professor \&amp; Kian Katanforoosh, Lecturer - Stanford University
http://onlinehub.stanford.edu/​

Andrew Ng
Adjunct Professor, Computer Science

Kian Katanforoosh 
Lecturer, Computer Science
 
To follow along with the course schedule and syllabus, visit: 
http://cs230.stanford.edu/​
 
To get the latest news on Stanford’s upcoming professional programs in Artificial Intelligence, visit: http://learn.stanford.edu/AI.html​
 
To view all online courses and programs offered by Stanford, visit: http://online.stanford.edu​},
	urldate = {2021-04-21},
	author = {{stanfordonline}},
	month = apr,
	year = {2019},
	note = {00000},
}

@phdthesis{weng_evaluating_nodate,
	title = {Evaluating {Robustness} of {Neural} {Networks}},
	author = {Weng, Tsui-Wei},
}

@inproceedings{chen_lifting_1999,
	address = {Atlanta, Georgia, United States},
	title = {Lifting {Markov} chains to speed up mixing},
	isbn = {978-1-58113-067-6},
	url = {http://portal.acm.org/citation.cfm?doid=301250.301315},
	doi = {10.1145/301250.301315},
	abstract = {There are several examples where the mixing time of a Markov chain can be reduced substantially, often to about its square root, by “lifting”, i.e., by splitting each state into several states. In several examples of random walks on groups, the lifted chain not only mixes better, but is easier to analyze.},
	language = {en},
	urldate = {2021-04-21},
	booktitle = {Proceedings of the thirty-first annual {ACM} symposium on {Theory} of computing  - {STOC} '99},
	publisher = {ACM Press},
	author = {Chen, Fang and Lovász, László and Pak, Igor},
	year = {1999},
	pages = {275--281},
}

@article{chen_recent_nodate,
	title = {Recent {Evaluation} {Metrics} for {Text} {Generation}},
	language = {en},
	author = {Chen, Wang and Li, Piji},
	pages = {63},
}

@article{boob_feasible_2020,
	title = {A {Feasible} {Level} {Proximal} {Point} {Method} for {Nonconvex} {Sparse} {Constrained} {Optimization}},
	url = {http://arxiv.org/abs/2010.12169},
	abstract = {Nonconvex sparse models have received signiﬁcant attention in high-dimensional machine learning. In this paper, we study a new model consisting of a general convex or nonconvex objectives and a variety of continuous nonconvex sparsityinducing constraints. For this constrained model, we propose a novel proximal point algorithm that solves a sequence of convex subproblems with gradually relaxed constraint levels. Each subproblem, having a proximal point objective and a convex surrogate constraint, can be efﬁciently solved based on a fast routine for projection onto the surrogate constraint. We establish the asymptotic convergence of the proposed algorithm to the Karush-Kuhn-Tucker (KKT) solutions. We also establish new convergence complexities to achieve an approximate KKT solution when the objective can be smooth/nonsmooth, deterministic/stochastic and convex/nonconvex with complexity that is on a par with gradient descent for unconstrained optimization problems in respective cases. To the best of our knowledge, this is the ﬁrst study of the ﬁrst-order methods with complexity guarantee for nonconvex sparse-constrained problems. We perform numerical experiments to demonstrate the effectiveness of our new model and efﬁciency of the proposed algorithm for large scale problems.},
	language = {en},
	urldate = {2021-04-21},
	journal = {arXiv:2010.12169 [cs, math]},
	author = {Boob, Digvijay and Deng, Qi and Lan, Guanghui and Wang, Yilin},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.12169},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@article{gessel_note_2020,
	title = {A note on {Stirling} permutations},
	url = {http://arxiv.org/abs/2005.04133},
	abstract = {In this note we generalize an identity of John Riordan and Robert Donaghey relating the enumerator for “Stirling permutations” to the Eulerian polynomials.},
	language = {en},
	urldate = {2021-04-21},
	journal = {arXiv:2005.04133 [math]},
	author = {Gessel, Ira M.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.04133},
	keywords = {05A15, Mathematics - Combinatorics},
}

@article{dhand_combinatorial_2014,
	title = {A combinatorial proof of strict unimodality for \$q\$-binomial coefficients},
	url = {http://arxiv.org/abs/1402.1199},
	abstract = {Pak and Panova recently proved that the \$q\$-binomial coefficient \$\{m+n {\textbackslash}choose m\}\_q\$ is a strictly unimodal polynomial in \$q\$ for \$m,n {\textbackslash}geq 8\$, via the representation theory of the symmetric group. We give a direct combinatorial proof of their result by characterizing when a product of chains is strictly unimodal and then applying O'Hara's structure theorem for the partition lattice \$L(m,n)\$. In fact, we prove a stronger result: if \$m, n {\textbackslash}geq 8d\$, and \$2d {\textbackslash}leq r {\textbackslash}leq mn/2\$, then the \$r\$-th rank of \$L(m,n)\$ has at least \$d\$ more elements that the next lower rank.},
	language = {en},
	urldate = {2021-04-21},
	journal = {arXiv:1402.1199 [math]},
	author = {Dhand, Vivek},
	month = mar,
	year = {2014},
	note = {arXiv: 1402.1199},
	keywords = {Mathematics - Combinatorics},
}

@article{li_maximum_nodate,
	title = {Maximum {Principle} {Based} {Algorithms} for {Deep} {Learning}},
	abstract = {The continuous dynamical system approach to deep learning is explored in order to devise alternative frameworks for training algorithms. Training is recast as a control problem and this allows us to formulate necessary optimality conditions in continuous time using the Pontryagin’s maximum principle (PMP). A modiﬁcation of the method of successive approximations is then used to solve the PMP, giving rise to an alternative training algorithm for deep learning. This approach has the advantage that rigorous error estimates and convergence results can be established. We also show that it may avoid some pitfalls of gradient-based methods, such as slow convergence on ﬂat landscapes near saddle points. Furthermore, we demonstrate that it obtains favorable initial convergence rate periteration, provided Hamiltonian maximization can be eﬃciently carried out - a step which is still in need of improvement. Overall, the approach opens up new avenues to attack problems associated with deep learning, such as trapping in slow manifolds and inapplicability of gradient-based methods for discrete trainable variables.},
	language = {en},
	author = {Li, Qianxiao},
	pages = {29},
}

@article{rovatsos_logical_nodate,
	title = {Logical {Agents}: {Knowledge} {Bases} and the {Wumpus} {World}},
	language = {en},
	author = {Rovatsos, Michael},
	pages = {34},
}

@article{gessel_binomial_1985,
	title = {Binomial determinants, paths, and hook length formulae},
	volume = {58},
	issn = {00018708},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0001870885901215},
	doi = {10.1016/0001-8708(85)90121-5},
	language = {en},
	number = {3},
	urldate = {2021-04-21},
	journal = {Advances in Mathematics},
	author = {Gessel, Ira and Viennot, Gérard},
	month = dec,
	year = {1985},
	pages = {300--321},
}

@article{wang_neural_nodate,
	title = {Neural {Network} {Control} {Policy} {Verification} with {Persistent} {Adversarial} {Perturbations}},
	abstract = {Deep neural networks are known to be fragile to small adversarial perturbations, which raises serious concerns when a neural network policy is interconnected with a physical system in a closed loop. In this paper, we show how to combine recent works on static neural network certiﬁcation tools with robust control theory to certify a neural network policy in a control loop. We give a sufﬁcient condition and an algorithm to ensure that the closed loop state and control constraints are satisﬁed when the persistent adversarial perturbation is ∞ norm bounded. Our method is based on ﬁnding a positively invariant set of the closed loop dynamical system, and thus we do not require the continuity of the neural network policy. Along with the veriﬁcation result, we also develop an effective attack strategy for neural network control systems that outperforms exhaustive Monte-Carlo search signiﬁcantly. We show that our certiﬁcation algorithm works well on learned models and could achieve 5 times better result than the traditional Lipschitz-based method to certify the robustness of a neural network policy on the cart-pole balance control problem.},
	language = {en},
	author = {Wang, Yuh-Shyang and Weng, Tsui-Wei and Daniel, Luca},
	pages = {10},
}

@article{weng_toward_2020,
	title = {{TOWARD} {EVALUATING} {ROBUSTNESS} {OF} {DEEP} {REIN}- {FORCEMENT} {LEARNING} {WITH} {CONTINUOUS} {CONTROL}},
	abstract = {Deep reinforcement learning has achieved great success in many previously difﬁcult reinforcement learning tasks, yet recent studies show that deep RL agents are also unavoidably susceptible to adversarial perturbations, similar to deep neural networks in classiﬁcation tasks. Prior works mostly focus on model-free adversarial attacks and agents with discrete actions. In this work, we study the problem of continuous control agents in deep RL with adversarial attacks and propose the ﬁrst two-step algorithm based on learned model dynamics. Extensive experiments on various MuJoCo domains (Cartpole, Fish, Walker, Humanoid) demonstrate that our proposed framework is much more effective and efﬁcient than model-free attacks baselines in degrading agent performance as well as driving agents to unsafe states.},
	language = {en},
	author = {Weng, Tsui-Wei and Uesato, Jonathan and Xiao, Kai and Gowal, Sven and Stanforth, Robert and Kohli, Pushmeet},
	year = {2020},
	pages = {13},
}

@article{chen_robust_2021,
	title = {{ROBUST} {OVERFITTING} {MAY} {BE} {MITIGATED} {BY} {PROP}- {ERLY} {LEARNED} {SMOOTHENING}},
	abstract = {A recent study (Rice et al., 2020) revealed overﬁtting to be a dominant phenomenon in adversarially robust training of deep networks, and that appropriate early-stopping of adversarial training (AT) could match the performance gains of most recent algorithmic improvements. This intriguing problem of robust overﬁtting motivates us to seek more remedies. As a pilot study, this paper investigates two empirical means to inject more learned smoothening during AT: one leveraging knowledge distillation and self-training to smooth the logits, the other performing stochastic weight averaging (Izmailov et al., 2018) to smooth the weights. Despite the embarrassing simplicity, the two approaches are surprisingly effective and hassle-free in mitigating robust overﬁtting. Experiments demonstrate that by plugging in them to AT, we can simultaneously boost the standard accuracy by 3.72\% ∼ 6.68\% and robust accuracy by 0.22\% ∼ 2.03\%, across multiple datasets (STL-10, SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet), perturbation types ( ∞ and 2), and robustiﬁed methods (PGD, TRADES, and FSGM), establishing the new state-of-the-art bar in AT. We present systematic visualizations and analyses to dive into their possible working mechanisms. We also carefully exclude the possibility of gradient masking by evaluating our models’ robustness against transfer attacks. Codes are available at https: //github.com/VITA-Group/Alleviate-Robust-Overfitting.},
	language = {en},
	author = {Chen, Tianlong and Zhang, Zhenyu and Liu, Sijia and Chang, Shiyu and Wang, Zhangyang},
	year = {2021},
	pages = {19},
}

@article{reinisch_modeling_2007,
	title = {Modeling the {F2} topside and plasmasphere for {IRI} using {IMAGE}/{RPI} and {ISIS} data},
	volume = {39},
	issn = {02731177},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0273117706006028},
	doi = {10.1016/j.asr.2006.05.032},
	abstract = {Empirical models are an important tool for the study of the diﬀerent geospace regions from Earth to Sun, providing the user with easy access to a synthesis of reliable measurements from ground and space for speciﬁc parameters and regions. This paper describes a new eﬀort to develop a coherent model of the topside F2 layer and the plasmasphere with the goal to improve the representation of the topside electron density in the IRI model and to extend the IRI description into the plasmasphere. An a-Chapman function with a continuously varying scale height, dubbed a vary-Chap function, is used to describe the topside F2 vertical electron density proﬁle N(h) that seamlessly connects the ionosphere with the plasmasphere. The Chapman scale height H(h) varies only slowly with height near hmF2 and increases rapidly at the O+ to light–ion transition height. A hyperbolic tangent function suitably represents this variation. New plasmasphere density proﬁle data from the IMAGE/RPI measurements and topside proﬁles from the ISIS topside sounders are used to construct a continuous proﬁle from hmF2 to several RE altitude.},
	language = {en},
	number = {5},
	urldate = {2021-04-21},
	journal = {Advances in Space Research},
	author = {Reinisch, B.W. and Nsumei, P. and Huang, X. and Bilitza, D.K.},
	year = {2007},
	pages = {731--738},
}

@article{fong_how_nodate,
	title = {How to {Read} a {CS} {Research} {Paper}?},
	language = {en},
	author = {Fong, Philip W L},
	pages = {4},
}

@article{noauthor_nvidia_nodate,
	title = {{NVIDIA} {A40} datasheet},
	language = {en},
	pages = {2},
}

@article{wang_fast_2021,
	title = {{ON} {FAST} {ADVERSARIAL} {ROBUSTNESS} {ADAPTATION} {IN} {MODEL}-{AGNOSTIC} {META}-{LEARNING}},
	abstract = {Model-agnostic meta-learning (MAML) has emerged as one of the most successful meta-learning techniques in few-shot learning. It enables us to learn a meta-initialization of model parameters (that we call meta-model) to rapidly adapt to new tasks using a small amount of labeled training data. Despite the generalization power of the meta-model, it remains elusive that how adversarial robustness can be maintained by MAML in few-shot learning. In addition to generalization, robustness is also desired for a meta-model to defend adversarial examples (attacks). Toward promoting adversarial robustness in MAML, we ﬁrst study when a robustness-promoting regularization should be incorporated, given the fact that MAML adopts a bi-level (ﬁne-tuning vs. meta-update) learning procedure. We show that robustifying the meta-update stage is sufﬁcient to make robustness adapted to the task-speciﬁc ﬁne-tuning stage even if the latter uses a standard training protocol. We also make additional justiﬁcation on the acquired robustness adaptation by peering into the interpretability of neurons’ activation maps. Furthermore, we investigate how robust regularization can efﬁciently be designed in MAML. We propose a general but easily-optimized robustness-regularized meta-learning framework, which allows the use of unlabeled data augmentation, fast adversarial attack generation, and computationally-light ﬁne-tuning. In particular, we for the ﬁrst time show that the auxiliary contrastive learning task can enhance the adversarial robustness of MAML. Finally, extensive experiments are conducted to demonstrate the effectiveness of our proposed methods in robust few-shot learning. Codes are available at https://github.com/wangren09/MetaAdv.},
	language = {en},
	author = {Wang, Ren and Xu, Kaidi and Liu, Sijia and Chen, Pin-Yu and Wang, Meng and Weng, Tsui-Wei and Gan, Chuang},
	year = {2021},
	pages = {16},
}

@book{nowlan_masters_2017,
	title = {Masters of mathematics: the problems they solved, why these are important, and what you should know about them},
	isbn = {978-94-6300-891-4 978-94-6300-892-1},
	shorttitle = {Masters of mathematics},
	language = {en},
	author = {Nowlan, Robert A},
	year = {2017},
	note = {OCLC: 1041358492},
}

@article{mohapatra_towards_nodate,
	title = {Towards {Verifying} {Robustness} of {Neural} {Networks} {Against} {A} {Family} of {Semantic} {Perturbations}},
	abstract = {Verifying robustness of neural networks given a speciﬁed threat model is a fundamental yet challenging task. While current veriﬁcation methods mainly focus on the ℓp-norm threat model of the input instances, robustness veriﬁcation against semantic adversarial attacks inducing large ℓp-norm perturbations, such as color shifting and lighting adjustment, are beyond their capacity. To bridge this gap, we propose Semantify-NN, a model-agnostic and generic robustness veriﬁcation approach against semantic perturbations for neural networks. By simply inserting our proposed semantic perturbation layers (SP-layers) to the input layer of any given model, Semantify-NN is model-agnostic, and any ℓp-norm based veriﬁcation tools can be used to verify the model robustness against semantic perturbations. We illustrate the principles of designing the SP-layers and provide examples including semantic perturbations to image classiﬁcation in the space of hue, saturation, lightness, brightness, contrast and rotation, respectively. In addition, an efﬁcient reﬁnement technique is proposed to further signiﬁcantly improve the semantic certiﬁcate. Experiments on various network architectures and different datasets demonstrate the superior veriﬁcation performance of Semantify-NN over ℓp-norm-based veriﬁcation frameworks that naively convert semantic perturbation to ℓp-norm. The results show that Semantify-NN can support robustness veriﬁcation against a wide range of semantic perturbations.},
	language = {en},
	author = {Mohapatra, Jeet and Weng, Tsui-Wei and Chen, Pin-Yu and Liu, Sijia and Daniel, Luca},
	pages = {9},
}

@article{liu_multicenter_nodate,
	title = {{MULTICENTER} {NETWORK} {AND} {SYNCHRONIZATION}},
	language = {en},
	author = {Liu, Zengrong and Dong, Chengdong and Fan, Qingduan},
	pages = {7},
}

@article{koo_dual_nodate,
	title = {Dual {Decomposition} for {Parsing} with {Non}-{Projective} {Head} {Automata}},
	abstract = {This paper introduces algorithms for nonprojective parsing based on dual decomposition. We focus on parsing algorithms for nonprojective head automata, a generalization of head-automata models to non-projective structures. The dual decomposition algorithms are simple and efﬁcient, relying on standard dynamic programming and minimum spanning tree algorithms. They provably solve an LP relaxation of the non-projective parsing problem. Empirically the LP relaxation is very often tight: for many languages, exact solutions are achieved on over 98\% of test sentences. The accuracy of our models is higher than previous work on a broad range of datasets.},
	language = {en},
	author = {Koo, Terry and Rush, Alexander M and Collins, Michael and Jaakkola, Tommi and Sontag, David},
	pages = {11},
}

@article{raghunathan_demystifying_2010,
	title = {Demystifying the {American} {Graduate} {Admissions} {Process}},
	abstract = {This paper attempts to discuss at length the various factors (along with their respective weightage) that the admissions committee at a top graduate school in the US takes into account while reviewing a Master of Science (MS) application.},
	language = {en},
	author = {Raghunathan, Karthik},
	year = {2010},
	pages = {9},
}

@article{clark_what_nodate,
	title = {What {Does} {BERT} {Look} {At}? {An} {Analysis} of {BERT}’s {Attention}},
	abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classiﬁers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT’s attention heads exhibit patterns such as attending to delimiter tokens, speciﬁc positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we ﬁnd heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classiﬁer and use it to further demonstrate that substantial syntactic information is captured in BERT’s attention.},
	language = {en},
	author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
	pages = {11},
}

@article{noauthor_intestinal_1986,
	title = {Intestinal helminths of lesser scaup ducks: an interactive community},
	volume = {64},
	language = {en},
	year = {1986},
	pages = {11},
}

@article{mourad_bubble-stabilized_2006,
	title = {A bubble-stabilized ﬁnite element method for {Dirichlet} constraints on embedded interfaces},
	abstract = {We examine a bubble-stabilized ﬁnite element method for enforcing Dirichlet constraints on embedded interfaces. By embedded we refer to problems of general interest wherein the geometry of the interface is assumed independent of some underlying bulk mesh. As such, the robust imposition of Dirichlet constraints with a Lagrange multiplier ﬁeld is not trivial. To focus issues, we consider a simple one-sided problem that is representative of a wide class of evolving interface problems. The bulk ﬁeld is decomposed into coarse and ﬁne scales, giving rise to coarse-scale and ﬁne-scale onesided sub-problems. The ﬁne-scale solution is approximated with bubble functions, permitting static condensation and giving rise to a stabilized form bearing strong analogy with a classical method. Importantly, the method is simple to implement, readily extends to multiple dimensions, obviates the need to specify any free stabilization parameters, and gives rise to a symmetric, positive-deﬁnite system of equations. The performance of the method is then examined through several numerical examples. The accuracy of the Lagrange multiplier is compared to results obtained using a local version of the domain integral method. The variational multiscale approach is shown to both stabilize the Lagrange multiplier and improve the accuracy of the post-processed ﬂuxes. Copyright c 2006 John Wiley \& Sons, Ltd.},
	language = {en},
	author = {Mourad, Hashem M and Dolbow, John and Harari, Isaac},
	year = {2006},
	pages = {21},
}

@inproceedings{zhennan_reasoning_2017,
	title = {A {Reasoning} {Algorithm} {Based} on {Novel} {Extension} {Rule} in {Wumpus} {World}},
	url = {https://www.intelcomp-design.com/images/ICIE/ICIE011.pdf},
	doi = {10.26480/icie.01.2017.36.40},
	language = {en},
	urldate = {2021-04-21},
	author = {Zhennan, Cai and Ge, Gao and Zijian,, Han and Kangning, Zhu and Guangli,, Li and Shuai, Lü},
	month = sep,
	year = {2017},
	pages = {36--40},
}

@misc{he_why_2021,
	title = {Why do (neural) {LMs} repeat themselves?},
	author = {He, He},
	year = {2021},
	note = {00000},
	keywords = {Abs, Brief, Details},
}

@article{holtzman_curious_2020,
	title = {The {Curious} {Case} of {Neural} {Text} {Degeneration}},
	url = {http://arxiv.org/abs/1904.09751},
	abstract = {Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.},
	urldate = {2021-04-07},
	journal = {arXiv:1904.09751 [cs]},
	author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	month = feb,
	year = {2020},
	note = {00000 
arXiv: 1904.09751},
	keywords = {Abs, Code},
}

@misc{dayal_how_2020,
	title = {How to evaluate {Text} {Generation} {Models} ? {Metrics} for {Automatic} {Evaluation} of {NLP} {Models}},
	shorttitle = {How to evaluate {Text} {Generation} {Models} ?},
	url = {https://towardsdatascience.com/how-to-evaluate-text-generation-models-metrics-for-automatic-evaluation-of-nlp-models-e1c251b04ec1},
	abstract = {Metrics that are used in NLP for the comparison of generative or extractive tasks, between the generated and target texts.},
	language = {en},
	urldate = {2021-04-18},
	journal = {Medium},
	author = {Dayal, Divish},
	month = nov,
	year = {2020},
	note = {00000},
	keywords = {Abs, Brief, Code, Details},
}

@misc{mann_how_2019,
	title = {How to sample from language models},
	url = {https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277},
	abstract = {Causal language models like GPT-2 are trained to predict the probability of the next word given some context. For example, given “I ate a…},
	language = {en},
	urldate = {2021-04-17},
	journal = {Medium},
	author = {Mann, Ben},
	month = may,
	year = {2019},
	note = {00000},
	keywords = {Abs, Brief, Code, Details},
}

@article{khandelwal_sharp_2018,
	title = {Sharp {Nearby}, {Fuzzy} {Far} {Away}: {How} {Neural} {Language} {Models} {Use} {Context}},
	shorttitle = {Sharp {Nearby}, {Fuzzy} {Far} {Away}},
	url = {http://arxiv.org/abs/1805.04623},
	abstract = {We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.},
	urldate = {2021-04-17},
	journal = {arXiv:1805.04623 [cs]},
	author = {Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
	month = may,
	year = {2018},
	note = {00000 
arXiv: 1805.04623},
	keywords = {Abs, Code},
}

@article{garlitz_numbers_nodate,
	title = {{SOME} {NUMBERS} {RELATED} {TO} {THE} {STIRLING} {NUMBERS} {OF} {THE} {FIRST} {AND} {SECOND} {KIND}},
	language = {en},
	author = {Garlitz, Leonard},
	pages = {8},
}

@article{carlitz_coefficients_nodate,
	title = {{THE} {COEFFICIENTS} {IN} {AN} {ASYMPTOTIC} {EXPANSION}},
	language = {en},
	author = {Carlitz, L},
	pages = {5},
}

@article{berndt_how_nodate,
	title = {{HOW} {TO} {WRITE} {MATHEMATICAL} {PAPERS}},
	language = {en},
	author = {Berndt, Bruce C},
	note = {00000},
	pages = {5},
}

@incollection{rota_ten_1997,
	address = {Boston, MA},
	title = {Ten {Lessons} {I} {Wish} {I} {Had} {Been} {Taught}},
	isbn = {978-0-8176-4780-3 978-0-8176-4781-0},
	url = {http://link.springer.com/10.1007/978-0-8176-4781-0_18},
	language = {en},
	urldate = {2021-04-03},
	booktitle = {Indiscrete {Thoughts}},
	publisher = {Birkhäuser Boston},
	author = {Rota, Gian-Carlo},
	editor = {Palombi, Fabrizio},
	collaborator = {Rota, Gian-Carlo},
	year = {1997},
	note = {00037},
	pages = {195--203},
}

@misc{oded_goldreich_how_nodate,
	title = {How to write a paper},
	author = {Oded Goldreich},
	note = {00000},
	keywords = {used},
}

@article{wang_rethinking_2021,
	title = {{RETHINKING} {ARCHITECTURE} {SELECTION} {IN} {DIFFER}- {ENTIABLE} {NAS}},
	abstract = {Differentiable Neural Architecture Search is one of the most popular Neural Architecture Search (NAS) methods for its search efﬁciency and simplicity, accomplished by jointly optimizing the model weight and architecture parameters in a weight-sharing supernet via gradient-based algorithms. At the end of the search phase, the operations with the largest architecture parameters will be selected to form the ﬁnal architecture, with the implicit assumption that the values of architecture parameters reﬂect the operation strength. While much has been discussed about the supernet’s optimization, the architecture selection process has received little attention. We provide empirical and theoretical analysis to show that the magnitude of architecture parameters does not necessarily indicate how much the operation contributes to the supernet’s performance. We propose an alternative perturbation-based architecture selection that directly measures each operation’s inﬂuence on the supernet. We re-evaluate several differentiable NAS methods with the proposed architecture selection and ﬁnd that it is able to extract signiﬁcantly improved architectures from the underlying supernets consistently. Furthermore, we ﬁnd that several failure modes of DARTS can be greatly alleviated with the proposed selection method, indicating that much of the poor generalization observed in DARTS can be attributed to the failure of magnitude-based architecture selection rather than entirely the optimization of its supernet.},
	language = {en},
	author = {Wang, Ruochen and Cheng, Minhao and Chen, Xiangning and Tang, Xiaocheng and Hsieh, Cho-Jui},
	year = {2021},
	pages = {18},
}

@article{liu_darts_2019,
	title = {{DARTS}: {Differentiable} {Architecture} {Search}},
	shorttitle = {{DARTS}},
	url = {http://arxiv.org/abs/1806.09055},
	abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
	urldate = {2021-04-08},
	journal = {arXiv:1806.09055 [cs, stat]},
	author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
	month = apr,
	year = {2019},
	note = {00000 
arXiv: 1806.09055},
}

@article{boopathy_fast_2021,
	title = {Fast {Training} of {Provably} {Robust} {Neural} {Networks} by {SingleProp}},
	url = {http://arxiv.org/abs/2102.01208},
	abstract = {Recent works have developed several methods of defending neural networks against adversarial attacks with certiﬁed guarantees. However, these techniques can be computationally costly due to the use of certiﬁcation during training. We develop a new regularizer that is both more efﬁcient than existing certiﬁed defenses, requiring only one additional forward propagation through a network, and can be used to train networks with similar certiﬁed accuracy. Through experiments on MNIST and CIFAR-10 we demonstrate improvements in training speed and comparable certiﬁed accuracy compared to state-of-the-art certiﬁed defenses.},
	language = {en},
	urldate = {2021-04-06},
	journal = {arXiv:2102.01208 [cs, stat]},
	author = {Boopathy, Akhilan and Weng, Tsui-Wei and Liu, Sijia and Chen, Pin-Yu and Zhang, Gaoyuan and Daniel, Luca},
	month = feb,
	year = {2021},
	note = {00000 
arXiv: 2102.01208},
}

@article{xie_skillearn_2021,
	title = {Skillearn: {Machine} {Learning} {Inspired} by {Humans}' {Learning} {Skills}},
	shorttitle = {Skillearn},
	url = {http://arxiv.org/abs/2012.04863},
	abstract = {Humans, as the most powerful learners on the planet, have accumulated a lot of learning skills, such as learning through tests, interleaving learning, self-explanation, active recalling, to name a few. These learning skills and methodologies enable humans to learn new topics more eﬀectively and eﬃciently. We are interested in investigating whether humans’ learning skills can be borrowed to help machines to learn better. Speciﬁcally, we aim to formalize these skills and leverage them to train better machine learning (ML) models. To achieve this goal, we develop a general framework – Skillearn, which provides a principled way to represent humans’ learning skills mathematically and use the formally-represented skills to improve the training of ML models. In two case studies, we apply Skillearn to formalize two learning skills of humans: learning by passing tests and interleaving learning, and use the formalized skills to improve neural architecture search. Experiments on various datasets show that trained using the skills formalized by Skillearn, ML models achieve signiﬁcantly better performance.},
	language = {en},
	urldate = {2021-04-06},
	journal = {arXiv:2012.04863 [cs]},
	author = {Xie, Pengtao and Du, Xuefeng and Ban, Hao},
	month = mar,
	year = {2021},
	note = {00000 
arXiv: 2012.04863},
}

@article{mohapatra_higher-order_2020,
	title = {Higher-{Order} {Certification} for {Randomized} {Smoothing}},
	url = {http://arxiv.org/abs/2010.06651},
	abstract = {Randomized smoothing is a recently proposed defense against adversarial attacks that has achieved state-of-the-art provable robustness against 2 perturbations. A number of publications have extended the guarantees to other metrics, such as 1 or ∞, by using different smoothing measures. Although the current framework has been shown to yield near-optimal p radii, the total safety region certiﬁed by the current framework can be arbitrarily small compared to the optimal.},
	language = {en},
	urldate = {2021-04-06},
	journal = {arXiv:2010.06651 [cs, stat]},
	author = {Mohapatra, Jeet and Ko, Ching-Yun and Weng, Tsui-Wei and Chen, Pin-Yu and Liu, Sijia and Daniel, Luca},
	month = oct,
	year = {2020},
	note = {00000 
arXiv: 2010.06651},
}

@article{hu_learning_2019,
	title = {Learning {Data} {Manipulation} for {Augmentation} and {Weighting}},
	url = {http://arxiv.org/abs/1910.12795},
	abstract = {Manipulating data, such as weighting data examples or augmenting with new instances, has been increasingly used to improve model training. Previous work has studied various rule- or learning-based approaches designed for speciﬁc types of data manipulation. In this work, we propose a new method that supports learning different manipulation schemes with the same gradient-based algorithm. Our approach builds upon a recent connection of supervised learning and reinforcement learning (RL), and adapts an off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training. Different parameterization of the “data reward” function instantiates different manipulation schemes. We showcase data augmentation that learns a text transformation network, and data weighting that dynamically adapts the data sample importance. Experiments show the resulting algorithms signiﬁcantly improve the image and text classiﬁcation performance in low data regime and class-imbalance problems.},
	language = {en},
	urldate = {2021-04-06},
	journal = {arXiv:1910.12795 [cs, stat]},
	author = {Hu, Zhiting and Tan, Bowen and Salakhutdinov, Ruslan and Mitchell, Tom and Xing, Eric P.},
	month = oct,
	year = {2019},
	note = {00000 
arXiv: 1910.12795},
}

@article{rotskoff_trainability_2019,
	title = {Trainability and {Accuracy} of {Neural} {Networks}: {An} {Interacting} {Particle} {System} {Approach}},
	shorttitle = {Trainability and {Accuracy} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1805.00915},
	abstract = {Neural networks, a central tool in machine learning, have demonstrated remarkable, high ﬁdelity performance on image recognition and classiﬁcation tasks. These successes evince an ability to accurately represent high dimensional functions, but rigorous results about the approximation error of neural networks after training are few. Here we establish conditions for global convergence of the standard optimization algorithm used in machine learning applications, stochastic gradient descent (SGD), and quantify the scaling of its error with the size of the network. This is done by reinterpreting SGD as the evolution of a particle system with interactions governed by a potential related to the objective or “loss” function used to train the network. We show that, when the number n of units is large, the empirical distribution of the particles descends on a convex landscape towards the global minimum at a rate independent of n, with a resulting approximation error that universally scales as O(n−1). These properties are established in the form of a Law of Large Numbers and a Central Limit Theorem for the empirical distribution. Our analysis also quantiﬁes the scale and nature of the noise introduced by SGD and provides guidelines for the step size and batch size to use when training a neural network. We illustrate our ﬁndings on examples in which we train neural networks to learn the energy function of the continuous 3-spin model on the sphere. The approximation error scales as our analysis predicts in as high a dimension as d = 25.},
	language = {en},
	urldate = {2021-04-06},
	journal = {arXiv:1805.00915 [cond-mat, stat]},
	author = {Rotskoff, Grant M. and Vanden-Eijnden, Eric},
	month = jul,
	year = {2019},
	note = {arXiv: 1805.00915},
}

@book{zienkiewicz_finite_2010,
	address = {Amsterdam},
	edition = {6. ed., repr},
	title = {The finite element method {Its} basis and fundamentals},
	isbn = {978-0-7506-6320-5},
	shorttitle = {The finite element method},
	language = {en},
	publisher = {Elsevier Butterworth-Heinemann},
	author = {Zienkiewicz, Olgierd C. and Taylor, Robert L. and Zhu, Jianzhong},
	year = {2010},
	note = {00000 
OCLC: 838170287},
}

@article{zhang_set_nodate,
	title = {A {SET} {OF} {SYMMETRIC} {QUADRATURE} {RULES} {ON} {TRIANGLES} {AND} {TETRAHEDRA}},
	abstract = {We present a program for computing symmetric quadrature rules on triangles and tetrahedra. A set of rules are obtained by using this program. Quadrature rules up to order 21 on triangles and up to order 14 on tetrahedra have been obtained which are useful for use in ﬁnite element computations. All rules presented here have positive weights with points lying within the integration domain.},
	language = {en},
	author = {Zhang, Linbo and Cui, Tao and Liu, Hui},
	note = {00000},
	pages = {8},
}

@article{ying_hybrid_1983,
	title = {A hybrid finite element method for stokes flow: {Part} {II}—{Stability} and convergence studies},
	volume = {36},
	issn = {0045-7825},
	shorttitle = {A hybrid finite element method for stokes flow},
	url = {http://www.sciencedirect.com/science/article/pii/0045782583901536},
	doi = {10.1016/0045-7825(83)90153-6},
	abstract = {In Part I we have presented a hybrid finite element method based on an assumed stress field which has the features: (i) the unknowns in the final system of finite element equations are (a) the nodal velocities, and (b) the ‘constant term’ in the arbitrary pressure field over each element; (ii) ‘exact’ integrations were performed for each element. In the following we present studies of stability and convergence of the above hybrid finite element method.},
	language = {en},
	number = {1},
	urldate = {2020-10-07},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Ying, L. -A. and Atluri, S. N.},
	month = jan,
	year = {1983},
	note = {00000},
	keywords = {Stokes},
	pages = {39--60},
}

@article{xu_auxiliary_1996,
	title = {The auxiliary space method and optimal multigrid preconditioning techniques for unstructured grids},
	volume = {56},
	issn = {0010-485X, 1436-5057},
	url = {http://link.springer.com/10.1007/BF02238513},
	doi = {10.1007/BF02238513},
	abstract = {Zusammenfassung The Auxiliary Space Method and Optimal Multigrid Preconditioning Techniques for Unstructured Grids. An abstract framework of awciliaryspace method is proposed and, as an application, an optimal multigrid technique is developed for general unstructured grids. The auxiliary space method is a (nonnested) two level preconditioning technique based on a simple relaxation scheme (smoother) and an auxiliary space (that may be roughly understood as a nonnested coarser space). An optimal multigrid preconditioner is then obtained for a discretized partial differential operator defined on an unstructured grid by using an attxiliary space defined on a more structured grid in which a further nested multigrid method can be naturally applied. This new technique makes it possible to apply multigrid methods to general unstructured grids without too much more programming effort than traditional solution methods. Some simple examples are also given to illustrate the abstract theory and for instance the Morley finite element space is used as an auxiliary space to construct a preconditioner for Argyris element for biharmoulc equations. Some numerical results are also given to demonstrate the efficiency of using structured grid for auxiliary space to precondition unstructured grids.},
	language = {en},
	number = {3},
	urldate = {2020-10-27},
	journal = {Computing},
	author = {Xu, J.},
	month = sep,
	year = {1996},
	note = {00000},
	keywords = {foesp},
	pages = {215--235},
}

@article{wilson_multigrid_2010,
	title = {Multigrid preconditioned conjugate-gradient solver for mixed finite-element method},
	volume = {14},
	issn = {1420-0597, 1573-1499},
	url = {http://link.springer.com/10.1007/s10596-009-9152-z},
	doi = {10.1007/s10596-009-9152-z},
	abstract = {The mixed ﬁnite-element approximation to a second-order elliptic PDE results in a saddle-point problem and leads to an indeﬁnite linear system of equations. The mixed system of equations can be transformed into coupled symmetric positive-deﬁnite matrix equations, or a Schur complement problem, using block Gauss elimination. A preconditioned conjugategradient algorithm is used for solving the Schur complement problem. The mixed ﬁnite-element method is closely related to the cell-centered ﬁnite difference scheme for solving second-order elliptic problems with variable coefﬁcients. For the cell-centered ﬁnite difference scheme, a simple multigrid algorithm can be deﬁned and used as a preconditioner. For distorted grids, an additional iteration is needed. Nested iteration with a multigrid preconditioned conjugate gradient inner iteration results in an effective numerical solution technique for the mixed system of linear equations arising from a discretization on distorted grids. Numerical results show that the preconditioned conjugate-gradient inner iteration is robust with respect to grid size and variability in the hydraulic conductivity tensor.},
	language = {en},
	number = {2},
	urldate = {2020-10-04},
	journal = {Computational Geosciences},
	author = {Wilson, John David and Naff, Richard L.},
	month = mar,
	year = {2010},
	note = {00000},
	pages = {289--299},
}

@article{wang_uniformly_2013,
	title = {Uniformly stable rectangular elements for fourth order elliptic singular perturbation problems},
	volume = {29},
	number = {3},
	journal = {Numerical Methods for Partial Differential Equations},
	author = {Wang, Li and Wu, Yongke and Xie, Xiaoping},
	year = {2013},
	note = {00000 
Publisher: Wiley Online Library},
	keywords = {foesp},
	pages = {721--737},
}

@article{wang_weak_2015,
	title = {Weak {Galerkin} finite element methods for elliptic {PDEs}},
	volume = {45},
	issn = {1674-7216},
	url = {http://engine.scichina.com/doi/10.1360/N012014-00233},
	doi = {10.1360/N012014-00233},
	language = {en},
	number = {7},
	urldate = {2020-08-18},
	journal = {SCIENTIA SINICA Mathematica},
	author = {Wang, ChunMei and Wang, JunPing},
	month = jul,
	year = {2015},
	note = {00000},
	pages = {1061--1092},
}

@article{stenberg_analysis_1984,
	title = {Analysis of mixed finite elements methods for the {Stokes} problem: a unified approach},
	volume = {42},
	shorttitle = {Analysis of mixed finite elements methods for the {Stokes} problem},
	number = {165},
	journal = {Mathematics of computation},
	author = {Stenberg, Rolf},
	year = {1984},
	note = {00000},
	pages = {9--23},
}

@book{morley_triangular_1967,
	title = {The triangular equilibrium element in the solution of plate bending problems},
	publisher = {RAE},
	author = {Morley, Leslie Sydney Dennis},
	year = {1967},
	note = {00000},
	keywords = {Morley},
}

@article{mu_simple_2017,
	title = {A simple finite element method for the {Stokes} equations},
	volume = {43},
	issn = {1019-7168, 1572-9044},
	url = {http://link.springer.com/10.1007/s10444-017-9526-z},
	doi = {10.1007/s10444-017-9526-z},
	abstract = {The goal of this paper is to introduce a simple finite element method to solve the Stokes equations. This method is in primal velocity-pressure formulation and is so simple such that both velocity and pressure are approximated by piecewise constant functions. Implementation issues as well as error analysis are investigated. A basis for a divergence free subspace of the velocity field is constructed so that the original saddle point problem can be reduced to a symmetric and positive definite system with much fewer unknowns. The numerical experiments indicate that the method is accurate.},
	language = {en},
	number = {6},
	urldate = {2020-08-13},
	journal = {Advances in Computational Mathematics},
	author = {Mu, Lin and Ye, Xiu},
	month = dec,
	year = {2017},
	note = {00000},
	keywords = {Stokes},
	pages = {1305--1324},
}

@article{shang_parallel_2013,
	title = {A parallel subgrid stabilized finite element method based on fully overlapping domain decomposition for the {Navier}–{Stokes} equations},
	volume = {403},
	issn = {0022-247X},
	url = {http://www.sciencedirect.com/science/article/pii/S0022247X13001753},
	doi = {10.1016/j.jmaa.2013.02.060},
	abstract = {Based on a fully overlapping domain decomposition technique and finite element discretization, a parallel subgrid stabilized method for the incompressible Navier–Stokes equations is proposed and analyzed. In this method, each processor computes a local stabilized solution in its own subdomain by solving a global problem on a mesh that is fine around its own subdomain and coarse elsewhere, where the stabilization term is based on an elliptic operator defined on the same mesh. This method has low communication complexity. It only requires the application of an existing sequential solver on the global meshes associated with each subdomain, and hence can reuse the existing sequential software. Convergence theory of the method is developed. Algorithmic parameter scalings are derived. Numerical results are also given to verify the theoretical predictions and demonstrate the effectiveness of the method.},
	language = {en},
	number = {2},
	urldate = {2020-10-09},
	journal = {Journal of Mathematical Analysis and Applications},
	author = {Shang, Yueqiang},
	month = jul,
	year = {2013},
	note = {00000},
	keywords = {Stokes},
	pages = {667--679},
}

@article{ming_morley_2006,
	title = {The {Morley} element for fourth order elliptic equations in any dimensions},
	volume = {103},
	issn = {0945-3245},
	url = {https://doi.org/10.1007/s00211-005-0662-x},
	doi = {10.1007/s00211-005-0662-x},
	abstract = {In this paper, the well-known nonconforming Morley element for biharmonic equations in two spatial dimensions is extended to any higher dimensions in a canonical fashion. The general n-dimensional Morley element consists of all quadratic polynomials defined on each n-simplex with degrees of freedom given by the integral average of the normal derivative on each (n-1)-subsimplex and the integral average of the function value on each (n-2)-subsimplex. Explicit expressions of nodal basis functions are also obtained for this element on general n-simplicial grids. Convergence analysis is given for this element when it is applied as a nonconforming finite element discretization for the biharmonic equation.},
	language = {en},
	number = {1},
	urldate = {2020-09-21},
	journal = {Numerische Mathematik},
	author = {Ming, Wang and Xu, Jinchao},
	month = mar,
	year = {2006},
	note = {00000},
	keywords = {Morley},
	pages = {155--169},
}

@article{li_stabilized_2008,
	title = {A stabilized finite element method based on two local {Gauss} integrations for the {Stokes} equations},
	volume = {214},
	issn = {0377-0427},
	url = {http://www.sciencedirect.com/science/article/pii/S0377042707000829},
	doi = {10.1016/j.cam.2007.02.015},
	abstract = {This paper considers a stabilized method based on the difference between a consistent and an under-integrated mass matrix of the pressure for the Stokes equations approximated by the lowest equal-order finite element pairs (i.e., the P1–P1 and Q1–Q1 pairs). This method only offsets the discrete pressure space by the residual of the simple and symmetry term at element level in order to circumvent the inf–sup condition. Optimal error estimates are obtained by applying the standard Galerkin technique. Finally, the numerical illustrations agree completely with the theoretical expectations.},
	language = {en},
	number = {1},
	urldate = {2020-10-09},
	journal = {Journal of Computational and Applied Mathematics},
	author = {Li, Jian and He, Yinnian},
	month = apr,
	year = {2008},
	note = {00000},
	keywords = {Stokes},
	pages = {58--65},
}

@article{hofreither_robust_2017,
	series = {Special {Issue} on {Isogeometric} {Analysis}: {Progress} and {Challenges}},
	title = {A robust multigrid method for {Isogeometric} {Analysis} in two dimensions using boundary correction},
	volume = {316},
	issn = {0045-7825},
	url = {http://www.sciencedirect.com/science/article/pii/S0045782516301414},
	doi = {10.1016/j.cma.2016.04.003},
	abstract = {We consider geometric multigrid methods for the solution of linear systems arising from isogeometric discretizations of elliptic partial differential equations. For classical finite elements, such methods are well known to be fast solvers showing optimal convergence behavior. However, the naive application of multigrid to the isogeometric case results in significant deterioration of the convergence rates if the spline degree is increased. Recently, a robust approximation error estimate and a corresponding inverse inequality for B-splines of maximum smoothness have been shown, both with constants independent of the spline degree. We use these results to construct multigrid solvers for discretizations of two-dimensional problems based on tensor product B-splines with maximum smoothness which exhibit robust convergence rates.},
	language = {en},
	urldate = {2020-10-25},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Hofreither, Clemens and Takacs, Stefan and Zulehner, Walter},
	month = apr,
	year = {2017},
	note = {00000},
	pages = {22--42},
}

@article{georgoulis_discontinuous_2007,
	title = {Discontinuous {Galerkin} {Methods} for {Advection}-{Diffusion}-{Reaction} {Problems} on {Anisotropically} {Refined} {Meshes}},
	volume = {30},
	issn = {1064-8275},
	url = {https://epubs.siam.org/doi/10.1137/060672352},
	doi = {10.1137/060672352},
	abstract = {In this paper we consider the a posteriori and a priori error analysis of discontinuous Galerkin interior penalty methods for second-order partial differential equations with nonnegative characteristic form on anisotropically refined computational meshes. In particular, we discuss the question of error estimation for linear target functionals, such as the outflow flux and the local average of the solution. Based on our a posteriori error bound, we design and implement the corresponding adaptive algorithm to ensure reliable and efficient control of the error in the prescribed functional to within a given tolerance. This involves exploiting both local isotropic and anisotropic mesh refinement. The theoretical results are illustrated by a series of numerical experiments.},
	number = {1},
	urldate = {2021-01-12},
	journal = {SIAM Journal on Scientific Computing},
	author = {Georgoulis, Emmanuil H. and Hall, Edward and Houston, Paul},
	month = dec,
	year = {2007},
	note = {00000 
Publisher: Society for Industrial and Applied Mathematics},
	pages = {246--271},
}

@article{georgoulis_discontinuous_2009,
	title = {Discontinuous {Galerkin} methods for the biharmonic problem},
	volume = {29},
	issn = {0272-4979},
	url = {https://doi.org/10.1093/imanum/drn015},
	doi = {10.1093/imanum/drn015},
	abstract = {This work is concerned with the design and analysis of hp-version discontinuous Galerkin (DG) finite element methods for boundary-value problems involving the biharmonic operator. The first part extends the unified approach of Arnold et al. (2001/2002, SIAM J. Numer. Anal.,39, 1749-–1779) developed for the Poisson problem, to the design of DG methods via an appropriate choice of numerical flux functions for fourth-order problems; as an example, we retrieve the interior penalty DG method developed by Süli \&amp; Mozolevski (2007, Comput. Methods Appl. Mech. Eng., 196, 1851-–1863). The second part of this work is concerned with a new a priori error analysis of the hp-version interior penalty DG method, when the error is measured in terms of both the energy norm and the L2-norm, as well as certain linear functionals of the solution, for elemental polynomial degrees p ≥ 2. Also, provided that the solution is piecewise analytic in an open neighbourhood of each element, exponential convergence is also proved for the p-version of the DG method. The sharpness of the theoretical developments is illustrated by numerical experiments.},
	number = {3},
	urldate = {2021-01-12},
	journal = {IMA Journal of Numerical Analysis},
	author = {Georgoulis, Emmanuil H. and Houston, Paul},
	month = jul,
	year = {2009},
	note = {00000},
	pages = {573--594},
}

@article{gahalaut_multigrid_2013,
	title = {Multigrid methods for isogeometric discretization},
	volume = {253},
	issn = {0045-7825},
	url = {http://www.sciencedirect.com/science/article/pii/S0045782512002678},
	doi = {10.1016/j.cma.2012.08.015},
	abstract = {We present (geometric) multigrid methods for isogeometric discretization of scalar second order elliptic problems. The smoothing property of the relaxation method, and the approximation property of the intergrid transfer operators are analyzed. These properties, when used in the framework of classical multigrid theory, imply uniform convergence of two-grid and multigrid methods. Supporting numerical results are provided for the smoothing property, the approximation property, convergence factor and iterations count for V-, W- and F-cycles, and the linear dependence of V-cycle convergence on the smoothing steps. For two dimensions, numerical results include the problems with variable coefficients, simple multi-patch geometry, a quarter annulus, and the dependence of convergence behavior on refinement levels ℓ, whereas for three dimensions, only the constant coefficient problem in a unit cube is considered. The numerical results are complete up to polynomial order p=4, and for C0 and Cp-1 smoothness.},
	language = {en},
	urldate = {2020-10-25},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Gahalaut, K. P. S. and Kraus, J. K. and Tomar, S. K.},
	month = jan,
	year = {2013},
	note = {00000},
	pages = {413--425},
}

@article{crouzeix_conforming_1973,
	title = {Conforming and nonconforming finite element methods for solving the stationary {Stokes} equations {I}},
	volume = {7},
	number = {R3},
	journal = {ESAIM: Mathematical Modelling and Numerical Analysis-Modélisation Mathématique et Analyse Numérique},
	author = {Crouzeix, Michel and Raviart, P.-A.},
	year = {1973},
	note = {00000},
	keywords = {Stokes},
	pages = {33--75},
}

@article{chen_anisotropic_2010,
	title = {{AN} {ANISOTROPIC} {NONCONFORMING} {ELEMENT} {FOR} {FOURTH} {ORDER} {ELLIPTIC} {SINGULAR} {PERTURBATION} {PROBLEM}.},
	volume = {7},
	number = {4},
	journal = {International Journal of Numerical Analysis \& Modeling},
	author = {Chen, Shaochun and Liu, Mingfang and Qiao, Zhonghua},
	year = {2010},
	note = {00000},
	keywords = {foesp},
}

@article{chen_programming_2018,
	title = {Programming of {Finite} {Element} {Methods} in {MATLAB}},
	url = {http://arxiv.org/abs/1804.05156},
	abstract = {We discuss how to implement the linear finite element method for solving the Poisson equation. We begin with the data structure to represent the triangulation and boundary conditions, introduce the sparse matrix, and then discuss the assembling process. We pay special attention to an efficient programming style using sparse matrices in MATLAB.},
	urldate = {2020-08-12},
	journal = {arXiv:1804.05156 [math]},
	author = {Chen, Long},
	month = apr,
	year = {2018},
	note = {00000 
arXiv: 1804.05156
version: 1},
}

@article{brezzi_stability_1991,
	title = {Stability of {Higher}-{Order} {Hood}–{Taylor} {Methods}},
	volume = {28},
	issn = {0036-1429},
	url = {https://epubs.siam.org/doi/abs/10.1137/0728032},
	doi = {10.1137/0728032},
	abstract = {The stability of a higher-order Hood–Taylor method for the approximation of the stationary Stokes equations using continuous piecewise polynomials of degree 3 to approximate velocities and continuous piecewise polynomials of degree 2 to approximate the pressure is proved. This result implies that the standard finite element method using these spaces satisfies a quasi-optimal error estimate. The technique used may also be applied to prove the stability of Hood–Taylor rectangular elements of arbitrary degree k for velocities and \$k - 1\$ for pressure in each variable.},
	number = {3},
	urldate = {2020-10-07},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Brezzi, Franco and Falk, Richard S.},
	month = jun,
	year = {1991},
	note = {00000 
Publisher: Society for Industrial and Applied Mathematics},
	keywords = {Stokes},
	pages = {581--590},
}

@book{brenner_mathematical_2008,
	address = {New York, NY},
	series = {Texts in {Applied} {Mathematics}},
	title = {The {Mathematical} {Theory} of {Finite} {Element} {Methods}},
	volume = {15},
	isbn = {978-0-387-75933-3 978-0-387-75934-0},
	url = {http://link.springer.com/10.1007/978-0-387-75934-0},
	language = {en},
	urldate = {2020-08-13},
	publisher = {Springer New York},
	author = {Brenner, Susanne C. and Scott, L. Ridgway},
	editor = {Marsden, J. E. and Sirovich, L. and Antman, S. S.},
	year = {2008},
	doi = {10.1007/978-0-387-75934-0},
	note = {00000 },
}

@article{pak_how_nodate,
	title = {{How} to {Write} a {Clear} {Math} {Paper}: {Some} {21ST} {Century} {Tips}},
	abstract = {In this note we explain the importance of clarity and give other tips for mathematical writing. Some of it is mildly opinionated, but most is just common sense and experience.},
	language = {en},
	author = {Pak, Igor},
	note = {00000},
	keywords = {used},
	pages = {18},
}

@article{poonen_practical_nodate,
	title = {Practical suggestions for mathematical writing},
	language = {en},
	author = {Poonen, Bjorn},
	note = {00000},
	pages = {5},
}

@article{lee_guide_nodate,
	title = {A {Guide} to {Writing} {Mathematics}},
	language = {en},
	author = {Lee, Dr Kevin P},
	note = {00023},
	keywords = {used},
	pages = {17},
}

@book{higham_handbook_1998,
	address = {Philadelphia},
	edition = {2nd ed},
	title = {Handbook of writing for the mathematical sciences},
	isbn = {978-0-89871-420-3},
	language = {en},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Higham, Nicholas J.},
	year = {1998},
	note = {00322},
}

@misc{tao_write_2007,
	title = {Write a rapid prototype first},
	url = {https://terrytao.wordpress.com/advice-on-writing-papers/write-a-rapid-prototype-first/},
	abstract = {The best performance improvement is the transition from the nonworking state to the working state. (John Ousterhout) Discovering a solution to a mathematical problem is only half of the battle.  Ac…},
	language = {en},
	urldate = {2021-04-03},
	journal = {What's new},
	author = {Tao},
	month = may,
	year = {2007},
	note = {00000},
	keywords = {done},
}

@misc{tao_how_nodate,
	title = {How to write},
	author = {Tao},
	note = {00000},
}

@article{reiter_writing_nodate,
	title = {Writing a {Research} {Paper} in {Mathematics}},
	language = {en},
	author = {Reiter, Ashley},
	note = {00006},
	pages = {6},
}

@misc{donald_e_knuth_tracy_larrabee_and_paul_m_roberts_mathematical_nodate,
	title = {Mathematical {Writing}},
	author = {Donald E. Knuth, Tracy Larrabee, {and} Paul M. Roberts},
	note = {00000},
}

@article{tan_checklist_nodate,
	title = {Checklist and {Tips} for giving a (decent) technical talk},
	language = {en},
	author = {Tan, Vincent Y F and Draper, Stark C},
	note = {00000},
	pages = {2},
}

@article{pak_hamiltonian_2009,
	title = {Hamiltonian paths in {Cayley} graphs},
	volume = {309},
	issn = {0012-365X},
	url = {https://www.sciencedirect.com/science/article/pii/S0012365X09000776},
	doi = {https://doi.org/10.1016/j.disc.2009.02.018},
	abstract = {The classical question raised by Lovász asks whether every Cayley graph is Hamiltonian. We present a short survey of various results in that direction and make some additional observations. In particular, we prove that every finite group G has a generating set of size at most log2{\textbar}G{\textbar}, such that the corresponding Cayley graph contains a Hamiltonian cycle. We also present an explicit construction of 3-regular Hamiltonian expanders.},
	number = {17},
	journal = {Discrete Mathematics},
	author = {Pak, Igor and Radoičić, Radoš},
	year = {2009},
	note = {00000},
	pages = {5501--5508},
}

@inproceedings{deudon_learning_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Learning {Heuristics} for the {TSP} by {Policy} {Gradient}},
	isbn = {978-3-319-93031-2},
	doi = {10.1007/978-3-319-93031-2_12},
	abstract = {The aim of the study is to provide interesting insights on how efficient machine learning algorithms could be adapted to solve combinatorial optimization problems in conjunction with existing heuristic procedures. More specifically, we extend the neural combinatorial optimization framework to solve the traveling salesman problem (TSP). In this framework, the city coordinates are used as inputs and the neural network is trained using reinforcement learning to predict a distribution over city permutations. Our proposed framework differs from the one in [1] since we do not make use of the Long Short-Term Memory (LSTM) architecture and we opted to design our own critic to compute a baseline for the tour length which results in more efficient learning. More importantly, we further enhance the solution approach with the well-known 2-opt heuristic. The results show that the performance of the proposed framework alone is generally as good as high performance heuristics (OR-Tools). When the framework is equipped with a simple 2-opt procedure, it could outperform such heuristics and achieve close to optimal results on 2D Euclidean graphs. This demonstrates that our approach based on machine learning techniques could learn good heuristics which, once being enhanced with a simple local search, yield promising results.},
	language = {en},
	booktitle = {Integration of {Constraint} {Programming}, {Artificial} {Intelligence}, and {Operations} {Research}},
	publisher = {Springer International Publishing},
	author = {Deudon, Michel and Cournut, Pierre and Lacoste, Alexandre and Adulyasak, Yossiri and Rousseau, Louis-Martin},
	editor = {van Hoeve, Willem-Jan},
	year = {2018},
	note = {00079},
	pages = {170--181},
}

@article{bello_neural_2017,
	title = {Neural {Combinatorial} {Optimization} with {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1611.09940},
	abstract = {This paper presents a framework to tackle combinatorial optimization problems using neural networks and reinforcement learning. We focus on the traveling salesman problem (TSP) and train a recurrent network that, given a set of city coordinates, predicts a distribution over different city permutations. Using negative tour length as the reward signal, we optimize the parameters of the recurrent network using a policy gradient method. We compare learning the network parameters on a set of training graphs against learning them on individual test graphs. Despite the computational expense, without much engineering and heuristic designing, Neural Combinatorial Optimization achieves close to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied to the KnapSack, another NP-hard problem, the same method obtains optimal solutions for instances with up to 200 items.},
	urldate = {2021-03-10},
	journal = {arXiv:1611.09940 [cs, stat]},
	author = {Bello, Irwan and Pham, Hieu and Le, Quoc V. and Norouzi, Mohammad and Bengio, Samy},
	month = jan,
	year = {2017},
	note = {00478 
arXiv: 1611.09940},
}

@article{ganesan_sparsh-amg_2020,
	title = {{SParSH}-{AMG}: {A} library for hybrid {CPU}-{GPU} algebraic multigrid and preconditioned iterative methods},
	shorttitle = {{SParSH}-{AMG}},
	url = {http://arxiv.org/abs/2007.00056},
	abstract = {Hybrid CPU-GPU algorithms for Algebraic Multigrid methods (AMG) to efficiently utilize both CPU and GPU resources are presented. In particular, hybrid AMG framework focusing on minimal utilization of GPU memory with performance on par with GPU-only implementations is developed. The hybrid AMG framework can be tuned to operate at a significantly lower GPU-memory, consequently, enables to solve large algebraic systems. Combining the hybrid AMG framework as a preconditioner with Krylov Subspace solvers like Conjugate Gradient, BiCG methods provides a solver stack to solve a large class of problems. The performance of the proposed hybrid AMG framework is analysed for an array of matrices with different properties and size. Further, the performance of CPU-GPU algorithms are compared with the GPU-only implementations to illustrate the significantly lower memory requirements.},
	urldate = {2021-03-07},
	journal = {arXiv:2007.00056 [cs]},
	author = {Ganesan, Sashikumaar and Shah, Manan},
	month = jun,
	year = {2020},
	note = {00000 
arXiv: 2007.00056},
}

@article{yang_hybrid_2017,
	title = {A hybrid computing method of {SpMV} on {CPU}–{GPU} heterogeneous computing systems},
	volume = {104},
	issn = {0743-7315},
	url = {https://www.sciencedirect.com/science/article/pii/S0743731517300011},
	doi = {10.1016/j.jpdc.2016.12.023},
	abstract = {Sparse matrix–vector multiplication (SpMV) is an important issue in scientific computing and engineering applications. The performance of SpMV can be improved using parallel computing. The implementation and optimization of SpMV on GPU are research hotspots. Due to some irregularities of sparse matrices, the use of a single compression format is not satisfactory. The hybrid storage format can expand the range of adaptation of the compression algorithms. However, because of the imbalance of non-zero elements, the parallel computing capability of a GPU cannot be fully utilized. The parallel computing capability of a CPU is also rising due to increased number of cores in CPU. However, when a GPU is computing, the CPU controls the process instead of contributing to the computational work. It leads to under-utilization of the computing power of CPU. Due to the characteristics of the sparse matrices, the data can be split into two parts using the hybrid storage format to be allocated to CPU and GPU for simultaneous computing. In order to take full advantage of computing resources of CPU and GPU, the CPU–GPU heterogeneous computing model is adopted in this paper to improve the performance of SpMV. With analysis of the characteristics of CPU and GPU, an optimization strategy of sparse matrix partitioning using a distribution function is proposed to improve the computing performance of SpMV on the heterogeneous computing platform. The experimental results on two test machines demonstrate noticeable performance improvement.},
	language = {en},
	urldate = {2021-03-07},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Yang, Wangdong and Li, Kenli and Li, Keqin},
	month = jun,
	year = {2017},
	note = {00031},
	pages = {49--60},
}

@article{naumov_amgx_2015,
	title = {{AmgX}: {A} {Library} for {GPU} {Accelerated} {Algebraic} {Multigrid} and {Preconditioned} {Iterative} {Methods}},
	volume = {37},
	issn = {1064-8275},
	shorttitle = {{AmgX}},
	url = {https://epubs.siam.org/doi/abs/10.1137/140980260},
	doi = {10.1137/140980260},
	abstract = {The solution of large sparse linear systems arises in many applications, such as computational fluid dynamics and oil reservoir simulation. In realistic cases the matrices are often so large that they require large scale distributed parallel computing to obtain the solution of interest in a reasonable time. In this paper we discuss the design and implementation of the AmgX library, which provides drop-in GPU acceleration of distributed algebraic multigrid (AMG) and preconditioned iterative methods. The AmgX library implements both classical and aggregation-based AMG methods with different selector and interpolation strategies, along with a variety of smoothers and preconditioners, including block-Jacobi, Gauss--Seidel, and incomplete-LU factorization. The library  contains many of the standard and flexible preconditioned Krylov subspace iterative methods, which can be combined with any of the available multigrid methods or simpler preconditioners. The parallelism in the aggregation scheme exploits parallel graph matching techniques, while the smoothers and preconditioners often rely on parallel graph coloring algorithms. The AMG algorithm implemented in the AmgX library achieves \$2\$--\$5{\textbackslash}times\$ speedup on a single GPU against a competitive implementation on the CPU. As will be shown in the numerical experiments section, both setup and solve phases scale well across multiple nodes, sustaining this performance advantage.},
	number = {5},
	urldate = {2021-03-01},
	journal = {SIAM Journal on Scientific Computing},
	author = {Naumov, M. and Arsaev, M. and Castonguay, P. and Cohen, J. and Demouth, J. and Eaton, J. and Layton, S. and Markovskiy, N. and Reguly, I. and Sakharnykh, N. and Sellappan, V. and Strzodka, R.},
	month = jan,
	year = {2015},
	note = {00060 
Publisher: Society for Industrial and Applied Mathematics},
	pages = {S602--S626},
}

@article{li_toward_2016,
	title = {Toward {Cost}-{Effective} {Reservoir} {Simulation} {Solvers} on {GPUs}},
	volume = {8},
	issn = {2070-0733, 2075-1354},
	url = {https://www.cambridge.org/core/journals/advances-in-applied-mathematics-and-mechanics/article/toward-costeffective-reservoir-simulation-solvers-on-gpus/F0B61893D7C747FFE4576D46A2002B4D},
	doi = {10.4208/aamm.2015.m1138},
	abstract = {In this paper, we focus on graphical processing unit (GPU) and discuss how its architecture affects the choice of algorithm and implementation of fully-implicit petroleum reservoir simulation. In order to obtain satisfactory performance on new many-core architectures such as GPUs, the simulator developers must know a great deal on the specific hardware and spend a lot of time on fine tuning the code. Porting a large petroleum reservoir simulator to emerging hardware architectures is expensive and risky. We analyze major components of an in-house reservoir simulator and investigate how to port them to GPUs in a cost-effective way. Preliminary numerical experiments show that our GPU-based simulator is robust and effective. More importantly, these numerical results clearly identify the main bottlenecks to obtain ideal speedup on GPUs and possibly other many-core architectures.},
	language = {en},
	number = {6},
	urldate = {2021-03-01},
	journal = {Advances in Applied Mathematics and Mechanics},
	author = {Li, Zheng and Wu, Shuhong and Xu, Jinchao and Zhang, Chensong},
	month = dec,
	year = {2016},
	note = {00003 
Publisher: Cambridge University Press},
	pages = {971--991},
}

@article{feng_numerical_2013,
	title = {Numerical {Study} of {Geometric} {Multigrid} {Methods} on {CPU}--{GPU} {Heterogeneous} {Computers}},
	url = {http://arxiv.org/abs/1208.4247},
	abstract = {The geometric multigrid method (GMG) is one of the most efficient solving techniques for discrete algebraic systems arising from elliptic partial differential equations. GMG utilizes a hierarchy of grids or discretizations and reduces the error at a number of frequencies simultaneously. Graphics processing units (GPUs) have recently burst onto the scientific computing scene as a technology that has yielded substantial performance and energy-efficiency improvements. A central challenge in implementing GMG on GPUs, though, is that computational work on coarse levels cannot fully utilize the capacity of a GPU. In this work, we perform numerical studies of GMG on CPU--GPU heterogeneous computers. Furthermore, we compare our implementation with an efficient CPU implementation of GMG and with the most popular fast Poisson solver, Fast Fourier Transform, in the cuFFT library developed by NVIDIA.},
	urldate = {2021-03-01},
	journal = {arXiv:1208.4247 [physics]},
	author = {Feng, Chunsheng and Shu, Shi and Xu, Jinchao and Zhang, Chen-Song},
	month = jan,
	year = {2013},
	note = {00021 
arXiv: 1208.4247},
}

@inproceedings{treviso_explanation_2020,
	address = {Online},
	title = {The {Explanation} {Game}: {Towards} {Prediction} {Explainability} through {Sparse} {Communication}},
	shorttitle = {The {Explanation} {Game}},
	url = {https://www.aclweb.org/anthology/2020.blackboxnlp-1.10},
	doi = {10.18653/v1/2020.blackboxnlp-1.10},
	abstract = {Explainability is a topic of growing importance in NLP. In this work, we provide a unified perspective of explainability as a communication problem between an explainer and a layperson about a classifier's decision. We use this framework to compare several explainers, including gradient methods, erasure, and attention mechanisms, in terms of their communication success. In addition, we reinterpret these methods in the light of classical feature selection, and use this as inspiration for new embedded explainers, through the use of selective, sparse attention. Experiments in text classification and natural language inference, using different configurations of explainers and laypeople (including both machines and humans), reveal an advantage of attention-based explainers over gradient and erasure methods, and show that selective attention is a simpler alternative to stochastic rationalizers. Human experiments show strong results on text classification with post-hoc explainers trained to optimize communication success.},
	urldate = {2021-01-15},
	booktitle = {Proceedings of the {Third} {BlackboxNLP} {Workshop} on {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Treviso, Marcos and Martins, André F. T.},
	month = nov,
	year = {2020},
	note = {00001},
	pages = {107--118},
}

@article{de_cao_how_2020,
	title = {How do {Decisions} {Emerge} across {Layers} in {Neural} {Models}? {Interpretation} with {Differentiable} {Masking}},
	shorttitle = {How do {Decisions} {Emerge} across {Layers} in {Neural} {Models}?},
	url = {http://arxiv.org/abs/2004.14992},
	abstract = {Attribution methods assess the contribution of inputs to the model prediction. One way to do so is erasure: a subset of inputs is considered irrelevant if it can be removed without affecting the prediction. Though conceptually simple, erasure's objective is intractable and approximate search remains expensive with modern deep NLP models. Erasure is also susceptible to the hindsight bias: the fact that an input can be dropped does not mean that the model `knows' it can be dropped. The resulting pruning is over-aggressive and does not reflect how the model arrives at the prediction. To deal with these challenges, we introduce Differentiable Masking. DiffMask learns to mask-out subsets of the input while maintaining differentiability. The decision to include or disregard an input token is made with a simple model based on intermediate hidden layers of the analyzed model. First, this makes the approach efficient because we predict rather than search. Second, as with probing classifiers, this reveals what the network `knows' at the corresponding layers. This lets us not only plot attribution heatmaps but also analyze how decisions are formed across network layers. We use DiffMask to study BERT models on sentiment classification and question answering.},
	urldate = {2021-01-15},
	journal = {arXiv:2004.14992 [cs, stat]},
	author = {De Cao, Nicola and Schlichtkrull, Michael and Aziz, Wilker and Titov, Ivan},
	month = oct,
	year = {2020},
	note = {00005 
arXiv: 2004.14992},
}

@article{serrano_is_2019,
	title = {Is {Attention} {Interpretable}?},
	url = {http://arxiv.org/abs/1906.03731},
	abstract = {Attention mechanisms have recently boosted performance on a range of NLP tasks. Because attention layers explicitly weight input components' representations, it is also often assumed that attention can be used to identify information that models found important (e.g., specific contextualized word tokens). We test whether that assumption holds by manipulating attention weights in already-trained text classification models and analyzing the resulting differences in their predictions. While we observe some ways in which higher attention weights correlate with greater impact on model predictions, we also find many ways in which this does not hold, i.e., where gradient-based rankings of attention weights better predict their effects than their magnitudes. We conclude that while attention noisily predicts input components' overall importance to a model, it is by no means a fail-safe indicator.},
	urldate = {2021-01-12},
	journal = {arXiv:1906.03731 [cs]},
	author = {Serrano, Sofia and Smith, Noah A.},
	month = jun,
	year = {2019},
	note = {00124 
arXiv: 1906.03731},
}

@article{wiegreffe_attention_2019,
	title = {Attention is not not {Explanation}},
	url = {http://arxiv.org/abs/1908.04626},
	abstract = {Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model's prediction, and consequently reach insights regarding the model's decision-making process. A recent paper claims that `Attention is not Explanation' (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one's definition of explanation, and that testing it needs to take into account all elements of the model, using a rigorous experimental design. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don't perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.},
	urldate = {2021-01-12},
	journal = {arXiv:1908.04626 [cs]},
	author = {Wiegreffe, Sarah and Pinter, Yuval},
	month = aug,
	year = {2019},
	note = {00125 
arXiv: 1908.04626
version: 1},
}

@article{jain_attention_2019,
	title = {Attention is not {Explanation}},
	url = {http://arxiv.org/abs/1902.10186},
	abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work, we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful `explanations' for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do. Code for all experiments is available at https://github.com/successar/AttentionExplanation.},
	urldate = {2021-01-12},
	journal = {arXiv:1902.10186 [cs]},
	author = {Jain, Sarthak and Wallace, Byron C.},
	month = may,
	year = {2019},
	note = {00259 
arXiv: 1902.10186},
}

@article{camburu_e-snli_2018,
	title = {e-{SNLI}: {Natural} {Language} {Inference} with {Natural} {Language} {Explanations}},
	shorttitle = {e-{SNLI}},
	url = {http://arxiv.org/abs/1812.01193},
	abstract = {In order for machine learning to garner widespread public adoption, models must be able to provide interpretable and robust explanations for their decisions, as well as learn from human-provided explanations at train time. In this work, we extend the Stanford Natural Language Inference dataset with an additional layer of human-annotated natural language explanations of the entailment relations. We further implement models that incorporate these explanations into their training process and output them at test time. We show how our corpus of explanations, which we call e-SNLI, can be used for various goals, such as obtaining full sentence justiﬁcations of a model’s decisions, improving universal sentence representations and transferring to out-of-domain NLI datasets. Our dataset1 thus opens up a range of research directions for using natural language explanations, both for improving models and for asserting their trust.},
	language = {en},
	urldate = {2021-01-12},
	journal = {arXiv:1812.01193 [cs]},
	author = {Camburu, Oana-Maria and Rocktäschel, Tim and Lukasiewicz, Thomas and Blunsom, Phil},
	month = dec,
	year = {2018},
	note = {00076 
arXiv: 1812.01193},
}

@article{selvaraju_grad-cam_2020,
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-based {Localization}},
	volume = {128},
	issn = {0920-5691, 1573-1405},
	shorttitle = {Grad-{CAM}},
	url = {http://arxiv.org/abs/1610.02391},
	doi = {10.1007/s11263-019-01228-7},
	abstract = {We propose a technique for producing "visual explanations" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at https://github.com/ramprs/grad-cam/, along with a demo at http://gradcam.cloudcv.org, and a video at youtu.be/COjUB9Izk6E.},
	number = {2},
	urldate = {2021-01-08},
	journal = {International Journal of Computer Vision},
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	month = feb,
	year = {2020},
	note = {03518 
arXiv: 1610.02391},
	pages = {336--359},
}

@inproceedings{wallace_interpreting_2020,
	address = {Online},
	title = {Interpreting {Predictions} of {NLP} {Models}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-tutorials.3},
	doi = {10.18653/v1/2020.emnlp-tutorials.3},
	abstract = {Although neural NLP models are highly expressive and empirically successful, they also systematically fail in counterintuitive ways and are opaque in their decision-making process. This tutorial will provide a background on interpretation techniques, i.e., methods for explaining the predictions of NLP models. We will ﬁrst situate example-speciﬁc interpretations in the context of other ways to understand models (e.g., probing, dataset analyses). Next, we will present a thorough study of example-speciﬁc interpretations, including saliency maps, input perturbations (e.g., LIME, input reduction), adversarial attacks, and inﬂuence functions. Alongside these descriptions, we will walk through source code that creates and visualizes interpretations for a diverse set of NLP tasks. Finally, we will discuss open problems in the ﬁeld, e.g., evaluating, extending, and improving interpretation methods. The tutorial slides and the accompanying code is available online at https: //www.ericswallace.com/interpretability.},
	language = {en},
	urldate = {2021-01-12},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {Tutorial} {Abstracts}},
	publisher = {Association for Computational Linguistics},
	author = {Wallace, Eric and Gardner, Matt and Singh, Sameer},
	year = {2020},
	note = {00000},
	pages = {20--23},
}

@inproceedings{ebrahimi_hotflip_2018,
	address = {Melbourne, Australia},
	title = {{HotFlip}: {White}-{Box} {Adversarial} {Examples} for {Text} {Classification}},
	shorttitle = {{HotFlip}},
	url = {http://aclweb.org/anthology/P18-2006},
	doi = {10.18653/v1/P18-2006},
	abstract = {We propose an efﬁcient method to generate white-box adversarial examples to trick a character-level neural classiﬁer. We ﬁnd that only a few manipulations are needed to greatly decrease the accuracy. Our method relies on an atomic ﬂip operation, which swaps one token for another, based on the gradients of the onehot input vectors. Due to efﬁciency of our method, we can perform adversarial training which makes the model more robust to attacks at test time. With the use of a few semantics-preserving constraints, we demonstrate that HotFlip can be adapted to attack a word-level classiﬁer as well.},
	language = {en},
	urldate = {2021-01-12},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ebrahimi, Javid and Rao, Anyi and Lowd, Daniel and Dou, Dejing},
	year = {2018},
	note = {00277},
	pages = {31--36},
}

@inproceedings{caruana_friends_2019,
	address = {Anchorage AK USA},
	title = {Friends {Don}'t {Let} {Friends} {Deploy} {Black}-{Box} {Models}: {The} {Importance} of {Intelligibility} in {Machine} {Learning}},
	isbn = {978-1-4503-6201-6},
	shorttitle = {Friends {Don}'t {Let} {Friends} {Deploy} {Black}-{Box} {Models}},
	url = {https://dl.acm.org/doi/10.1145/3292500.3340414},
	doi = {10.1145/3292500.3340414},
	language = {en},
	urldate = {2021-01-11},
	booktitle = {Proceedings of the 25th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Caruana, Richard},
	month = jul,
	year = {2019},
	note = {00001},
	pages = {3174--3174},
}

@misc{noauthor_friends_nodate,
	title = {Friends {Don}'t {Let} {Friends} {Deploy} {Black}-{Box} {Models}: {The} {Importance} of {Intelligibility} in {Machine} {Learning}},
	url = {https://www.researchgate.net/publication/334714770_Friends_Don't_Let_Friends_Deploy_Black-Box_Models_The_Importance_of_Intelligibility_in_Machine_Learning},
	urldate = {2021-01-11},
	note = {00001},
}

@article{ribeiro_why_2016,
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {http://arxiv.org/abs/1602.04938},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
	urldate = {2021-01-11},
	journal = {arXiv:1602.04938 [cs, stat]},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	note = {04485 
arXiv: 1602.04938},
}

@article{mundhenk_efficient_2019,
	title = {Efficient {Saliency} {Maps} for {Explainable} {AI}},
	url = {https://arxiv.org/abs/1911.11293v2},
	abstract = {We describe an explainable AI saliency map method for use with deep
convolutional neural networks (CNN) that is much more efficient than popular
fine-resolution gradient methods. It is also quantitatively similar or better
in accuracy. Our technique works by measuring information at the end of each
network scale which is then combined into a single saliency map. We describe
how saliency measures can be made more efficient by exploiting Saliency Map
Order Equivalence. We visualize individual scale/layer contributions by using a
Layer Ordered Visualization of Information. This provides an interesting
comparison of scale information contributions within the network not provided
by other saliency map methods. Using our method instead of Guided Backprop,
coarse-resolution class activation methods such as Grad-CAM and Grad-CAM++ seem
to yield demonstrably superior results without sacrificing speed. This will
make fine-resolution saliency methods feasible on resource limited platforms
such as robots, cell phones, low-cost industrial devices, astronomy and
satellite imagery.},
	language = {en},
	urldate = {2021-01-11},
	author = {Mundhenk, T. Nathan and Chen, Barry Y. and Friedland, Gerald},
	month = nov,
	year = {2019},
	note = {00004},
}

@article{erhan_visualizing_nodate,
	title = {Visualizing {Higher}-{Layer} {Features} of a {Deep} {Network}},
	abstract = {Deep architectures have demonstrated state-of-the-art results in a variety of settings, especially with vision datasets. Beyond the model deﬁnitions and the quantitative analyses, there is a need for qualitative comparisons of the solutions learned by various deep architectures. The goal of this paper is to ﬁnd good qualitative interpretations of high level features represented by such models. To this end, we contrast and compare several techniques applied on Stacked Denoising Autoencoders and Deep Belief Networks, trained on several vision datasets. We show that, perhaps counter-intuitively, such interpretation is possible at the unit level, that it is simple to accomplish and that the results are consistent across various techniques. We hope that such techniques will allow researchers in deep architectures to understand more of how and why deep architectures work.},
	language = {en},
	author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Vincent, Pascal and Box, P O},
	note = {00882},
	pages = {13},
}

@article{li_visualizing_2016,
	title = {Visualizing and {Understanding} {Neural} {Models} in {NLP}},
	url = {http://arxiv.org/abs/1506.01066},
	abstract = {While neural networks have been successfully applied to many NLP tasks the resulting vector-based models are very difficult to interpret. For example it's not clear how they achieve \{{\textbackslash}em compositionality\}, building sentence meaning from the meanings of words and phrases. In this paper we describe four strategies for visualizing compositionality in neural models for NLP, inspired by similar work in computer vision. We first plot unit values to visualize compositionality of negation, intensification, and concessive clauses, allow us to see well-known markedness asymmetries in negation. We then introduce three simple and straightforward methods for visualizing a unit's \{{\textbackslash}em salience\}, the amount it contributes to the final composed meaning: (1) gradient back-propagation, (2) the variance of a token from the average word node, (3) LSTM-style gates that measure information flow. We test our methods on sentiment using simple recurrent nets and LSTMs. Our general-purpose methods may have wide applications for understanding compositionality and other semantic properties of deep networks , and also shed light on why LSTMs outperform simple recurrent nets,},
	urldate = {2021-01-08},
	journal = {arXiv:1506.01066 [cs]},
	author = {Li, Jiwei and Chen, Xinlei and Hovy, Eduard and Jurafsky, Dan},
	month = jan,
	year = {2016},
	note = {00423 
arXiv: 1506.01066
version: 2},
}

@article{kapishnikov_xrai_2019,
	title = {{XRAI}: {Better} {Attributions} {Through} {Regions}},
	shorttitle = {{XRAI}},
	url = {http://arxiv.org/abs/1906.02825},
	abstract = {Saliency methods can aid understanding of deep neural networks. Recent years have witnessed many improvements to saliency methods, as well as new ways for evaluating them. In this paper, we 1) present a novel region-based attribution method, XRAI, that builds upon integrated gradients (Sundararajan et al. 2017), 2) introduce evaluation methods for empirically assessing the quality of image-based saliency maps (Performance Information Curves (PICs)), and 3) contribute an axiom-based sanity check for attribution methods. Through empirical experiments and example results, we show that XRAI produces better results than other saliency methods for common models and the ImageNet dataset.},
	urldate = {2021-01-08},
	journal = {arXiv:1906.02825 [cs, stat]},
	author = {Kapishnikov, Andrei and Bolukbasi, Tolga and Viégas, Fernanda and Terry, Michael},
	month = aug,
	year = {2019},
	note = {00018 
arXiv: 1906.02825},
}

@article{springenberg_striving_2015,
	title = {Striving for {Simplicity}: {The} {All} {Convolutional} {Net}},
	shorttitle = {Striving for {Simplicity}},
	url = {http://arxiv.org/abs/1412.6806},
	abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
	urldate = {2021-01-08},
	journal = {arXiv:1412.6806 [cs]},
	author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
	month = apr,
	year = {2015},
	note = {02655 
arXiv: 1412.6806},
}

@article{smilkov_smoothgrad_2017,
	title = {{SmoothGrad}: removing noise by adding noise},
	shorttitle = {{SmoothGrad}},
	url = {http://arxiv.org/abs/1706.03825},
	abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
	urldate = {2021-01-08},
	journal = {arXiv:1706.03825 [cs, stat]},
	author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viégas, Fernanda and Wattenberg, Martin},
	month = jun,
	year = {2017},
	note = {00498 
arXiv: 1706.03825},
}

@article{simonyan_deep_2014,
	title = {Deep {Inside} {Convolutional} {Networks}: {Visualising} {Image} {Classification} {Models} and {Saliency} {Maps}},
	shorttitle = {Deep {Inside} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1312.6034},
	abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
	urldate = {2021-01-08},
	journal = {arXiv:1312.6034 [cs]},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	month = apr,
	year = {2014},
	note = {02312 
arXiv: 1312.6034},
}

@article{sundararajan_axiomatic_2017,
	title = {Axiomatic {Attribution} for {Deep} {Networks}},
	url = {http://arxiv.org/abs/1703.01365},
	abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
	urldate = {2021-01-08},
	journal = {arXiv:1703.01365 [cs]},
	author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
	month = jun,
	year = {2017},
	note = {01074 
arXiv: 1703.01365},
}

@article{yang_xlnet_2019,
	title = {{XLNet}: {Generalized} {Autoregressive} {Pretraining} for {Language} {Understanding}},
	shorttitle = {{XLNet}},
	url = {https://arxiv.org/abs/1906.08237v2},
	abstract = {With the capability of modeling bidirectional contexts, denoising
autoencoding based pretraining like BERT achieves better performance than
pretraining approaches based on autoregressive language modeling. However,
relying on corrupting the input with masks, BERT neglects dependency between
the masked positions and suffers from a pretrain-finetune discrepancy. In light
of these pros and cons, we propose XLNet, a generalized autoregressive
pretraining method that (1) enables learning bidirectional contexts by
maximizing the expected likelihood over all permutations of the factorization
order and (2) overcomes the limitations of BERT thanks to its autoregressive
formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the
state-of-the-art autoregressive model, into pretraining. Empirically, under
comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a
large margin, including question answering, natural language inference,
sentiment analysis, and document ranking.},
	language = {en},
	urldate = {2020-05-07},
	author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Ruslan and Le, Quoc V.},
	month = jun,
	year = {2019},
	note = {00000},
	keywords = {new},
}

@article{chen_xgboost_2016,
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	shorttitle = {{XGBoost}},
	url = {http://arxiv.org/abs/1603.02754},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly eﬀective and widely used machine learning method. In this paper, we describe a scalable endto-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	language = {en},
	urldate = {2020-06-18},
	journal = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	author = {Chen, Tianqi and Guestrin, Carlos},
	month = aug,
	year = {2016},
	note = {00000 
arXiv: 1603.02754},
	pages = {785--794},
}

@inproceedings{yang_wikiqa_2015,
	address = {Lisbon, Portugal},
	title = {{WikiQA}: {A} {Challenge} {Dataset} for {Open}-{Domain} {Question} {Answering}},
	shorttitle = {{WikiQA}},
	url = {http://aclweb.org/anthology/D15-1237},
	doi = {10.18653/v1/D15-1237},
	abstract = {We describe the WIKIQA dataset, a new publicly available set of question and sentence pairs, collected and annotated for research on open-domain question answering. Most previous work on answer sentence selection focuses on a dataset created using the TREC-QA data, which includes editor-generated questions and candidate answer sentences selected by matching content words in the question. WIKIQA is constructed using a more natural process and is more than an order of magnitude larger than the previous dataset. In addition, the WIKIQA dataset also includes questions for which there are no correct sentences, enabling researchers to work on answer triggering, a critical component in any QA system. We compare several systems on the task of answer sentence selection on both datasets and also describe the performance of a system on the problem of answer triggering using the WIKIQA dataset.},
	language = {en},
	urldate = {2020-04-27},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Yang, Yi and Yih, Wen-tau and Meek, Christopher},
	year = {2015},
	note = {00000},
	keywords = {dataset, new},
	pages = {2013--2018},
}

@article{alberti_synthetic_2019,
	title = {Synthetic {QA} {Corpora} {Generation} with {Roundtrip} {Consistency}},
	url = {http://arxiv.org/abs/1906.05416},
	abstract = {We introduce a novel method of generating synthetic question answering corpora by combining models of question generation and answer extraction, and by ﬁltering the results to ensure roundtrip consistency. By pretraining on the resulting corpora we obtain signiﬁcant improvements on SQuAD2 (Rajpurkar et al., 2018) and NQ (Kwiatkowski et al., 2019), establishing a new state-of-the-art on the latter. Our synthetic data generation models, for both question generation and answer extraction, can be fully reproduced by ﬁnetuning a publicly available BERT model (Devlin et al., 2018) on the extractive subsets of SQuAD2 and NQ. We also describe a more powerful variant that does full sequence-to-sequence pretraining for question generation, obtaining exact match and F1 at less than 0.1\% and 0.4\% from human performance on SQuAD2.},
	language = {en},
	urldate = {2020-04-29},
	journal = {arXiv:1906.05416 [cs]},
	author = {Alberti, Chris and Andor, Daniel and Pitler, Emily and Devlin, Jacob and Collins, Michael},
	month = jun,
	year = {2019},
	note = {00000 
arXiv: 1906.05416},
	keywords = {new},
}

@article{rajpurkar_squad_2016,
	title = {{SQuAD}: 100,000+ {Questions} for {Machine} {Comprehension} of {Text}},
	shorttitle = {{SQuAD}},
	url = {http://arxiv.org/abs/1606.05250},
	abstract = {We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com},
	urldate = {2020-04-27},
	journal = {arXiv:1606.05250 [cs]},
	author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	month = oct,
	year = {2016},
	note = {00000 
arXiv: 1606.05250},
	keywords = {dataset, new},
}

@article{joshi_spanbert_2020,
	title = {{SpanBERT}: {Improving} {Pre}-training by {Representing} and {Predicting} {Spans}},
	shorttitle = {{SpanBERT}},
	url = {http://arxiv.org/abs/1907.10529},
	abstract = {We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines, with substantial gains on span selection tasks such as question answering and coreference resolution. In particular, with the same training data and model size as BERT-large, our single model obtains 94.6\% and 88.7\% F1 on SQuAD 1.1 and 2.0, respectively. We also achieve a new state of the art on the OntoNotes coreference resolution task (79.6{\textbackslash}\% F1), strong performance on the TACRED relation extraction benchmark, and even show gains on GLUE.},
	language = {en},
	urldate = {2020-04-29},
	journal = {arXiv:1907.10529 [cs]},
	author = {Joshi, Mandar and Chen, Danqi and Liu, Yinhan and Weld, Daniel S. and Zettlemoyer, Luke and Levy, Omer},
	month = jan,
	year = {2020},
	note = {00000 
arXiv: 1907.10529},
	keywords = {new},
}

@article{ahmad_reqa_2019,
	title = {{ReQA}: {An} {Evaluation} for {End}-to-{End} {Answer} {Retrieval} {Models}},
	shorttitle = {{ReQA}},
	url = {http://arxiv.org/abs/1907.04780},
	doi = {10.18653/v1/D19-5819},
	abstract = {Popular QA benchmarks like SQuAD have driven progress on the task of identifying answer spans within a speciﬁc passage, with models now surpassing human performance. However, retrieving relevant answers from a huge corpus of documents is still a challenging problem, and places different requirements on the model architecture. There is growing interest in developing scalable answer retrieval models trained end-to-end, bypassing the typical document retrieval step. In this paper, we introduce Retrieval QuestionAnswering (ReQA), a benchmark for evaluating large-scale sentence-level answer retrieval models. We establish baselines using both neural encoding models as well as classical information retrieval techniques. We release our evaluation code to encourage further work on this challenging task.},
	language = {en},
	urldate = {2020-04-29},
	journal = {Proceedings of the 2nd Workshop on Machine Reading for Question Answering},
	author = {Ahmad, Amin and Constant, Noah and Yang, Yinfei and Cer, Daniel},
	year = {2019},
	note = {00000 
arXiv: 1907.04780},
	keywords = {new},
	pages = {137--146},
}

@article{chen_reading_2017,
	title = {Reading {Wikipedia} to {Answer} {Open}-{Domain} {Questions}},
	url = {http://arxiv.org/abs/1704.00051},
	abstract = {This paper proposes to tackle open- domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.},
	urldate = {2020-03-27},
	journal = {arXiv:1704.00051 [cs]},
	author = {Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},
	month = apr,
	year = {2017},
	note = {00000 
arXiv: 1704.00051},
}

@article{zhang_relation_2015,
	title = {Relation {Classification} via {Recurrent} {Neural} {Network}},
	url = {http://arxiv.org/abs/1508.01006},
	abstract = {Deep learning has gained much success in sentence-level relation classification. For example, convolutional neural networks (CNN) have delivered competitive performance without much effort on feature engineering as the conventional pattern-based methods. Thus a lot of works have been produced based on CNN structures. However, a key issue that has not been well addressed by the CNN-based method is the lack of capability to learn temporal features, especially long-distance dependency between nominal pairs. In this paper, we propose a simple framework based on recurrent neural networks (RNN) and compare it with CNN-based model. To show the limitation of popular used SemEval-2010 Task 8 dataset, we introduce another dataset refined from MIMLRE(Angeli et al., 2014). Experiments on two different datasets strongly indicates that the RNN-based model can deliver better performance on relation classification, and it is particularly capable of learning long-distance relation patterns. This makes it suitable for real-world applications where complicated expressions are often involved.},
	urldate = {2020-03-17},
	journal = {arXiv:1508.01006 [cs]},
	author = {Zhang, Dongxu and Wang, Dong},
	month = dec,
	year = {2015},
	note = {00000 
arXiv: 1508.01006},
}

@inproceedings{shi_proje_2017,
	title = {{ProjE}: {Embedding} {Projection} for {Knowledge} {Graph} {Completion}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	shorttitle = {{ProjE}},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14279},
	abstract = {With the large volume of new information created every day, determining the validity of information in a knowledge graph and filling in its missing parts are crucial tasks for many researchers and practitioners. To address this challenge, a number of knowledge graph completion methods have been developed using low-dimensional graph embeddings. Although researchers continue to improve these models using an increasingly complex feature space, we show that simple changes in the architecture of the underlying model can outperform state-of-the-art models without the need for complex feature engineering. In this work, we present a shared variable neural network model called ProjE that fills-in missing information in a knowledge graph by learning joint embeddings of the knowledge graph’s entities and edges, and through subtle, but important, changes to the standard loss function. In doing so, ProjE has a parameter size that is smaller than 11 out of 15 existing methods while performing 37\% better than the current-best method on standard datasets. We also show, via a new fact checking task, that ProjE is capable of accurately determining the veracity of many declarative statements.},
	language = {en},
	urldate = {2020-03-17},
	booktitle = {Thirty-{First} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Shi, Baoxu and Weninger, Tim},
	month = feb,
	year = {2017},
	note = {00000},
}

@article{yu_qanet_2018,
	title = {{QANet}: {Combining} {Local} {Convolution} with {Global} {Self}-{Attention} for {Reading} {Comprehension}},
	shorttitle = {{QANet}},
	url = {http://arxiv.org/abs/1804.09541},
	abstract = {Current end-to-end machine reading and question answering (Q{\textbackslash}\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q{\textbackslash}\&A architecture called QANet, which does not require recurrent networks: Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.},
	urldate = {2020-03-15},
	journal = {arXiv:1804.09541 [cs]},
	author = {Yu, Adams Wei and Dohan, David and Luong, Minh-Thang and Zhao, Rui and Chen, Kai and Norouzi, Mohammad and Le, Quoc V.},
	month = apr,
	year = {2018},
	note = {00000 
arXiv: 1804.09541},
}

@article{qiu_pre-trained_2020,
	title = {Pre-trained {Models} for {Natural} {Language} {Processing}: {A} {Survey}},
	shorttitle = {Pre-trained {Models} for {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2003.08271},
	abstract = {Recently, the emergence of pre-trained models (PTMs) has brought natural language processing (NLP) to a new era. In this survey, we provide a comprehensive review of PTMs for NLP. We first briefly introduce language representation learning and its research progress. Then we systematically categorize existing PTMs based on a taxonomy with four perspectives. Next, we describe how to adapt the knowledge of PTMs to the downstream tasks. Finally, we outline some potential directions of PTMs for future research. This survey is purposed to be a hands-on guide for understanding, using, and developing PTMs for various NLP tasks.},
	urldate = {2020-04-01},
	journal = {arXiv:2003.08271 [cs]},
	author = {Qiu, Xipeng and Sun, Tianxiang and Xu, Yige and Shao, Yunfan and Dai, Ning and Huang, Xuanjing},
	month = mar,
	year = {2020},
	note = {00000 
arXiv: 2003.08271},
}

@article{kwiatkowski_natural_2019,
	title = {Natural {Questions}: {A} {Benchmark} for {Question} {Answering} {Research}},
	volume = {7},
	issn = {2307-387X},
	shorttitle = {Natural {Questions}},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00276},
	doi = {10.1162/tacl_a_00276},
	abstract = {We present the Natural Questions corpus, a question answering dataset. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
	language = {en},
	urldate = {2020-04-21},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
	month = mar,
	year = {2019},
	note = {00000},
	keywords = {dataset, new},
	pages = {453--466},
}

@article{yahya_natural_nodate,
	title = {Natural {Language} {Questions} for the {Web} of {Data}},
	abstract = {The Linked Data initiative comprises structured databases in the Semantic-Web data model RDF. Exploring this heterogeneous data by structured query languages is tedious and error-prone even for skilled users. To ease the task, this paper presents a methodology for translating natural language questions into structured SPARQL queries over linked-data sources.},
	language = {en},
	author = {Yahya, Mohamed and Berberich, Klaus and Elbassuoni, Shady and Ramanath, Maya and Tresp, Volker and Weikum, Gerhard},
	note = {00000},
	pages = {12},
}

@article{deng_multi-task_2018,
	title = {Multi-{Task} {Learning} with {Multi}-{View} {Attention} for {Answer} {Selection} and {Knowledge} {Base} {Question} {Answering}},
	url = {http://arxiv.org/abs/1812.02354},
	abstract = {Answer selection and knowledge base question answering (KBQA) are two important tasks of question answering (QA) systems. Existing methods solve these two tasks separately, which requires large number of repetitive work and neglects the rich correlation information between tasks. In this paper, we tackle answer selection and KBQA tasks simultaneously via multi-task learning (MTL), motivated by the following motivations. First, both answer selection and KBQA can be regarded as a ranking problem, with one at text-level while the other at knowledge-level. Second, these two tasks can beneﬁt each other: answer selection can incorporate the external knowledge from knowledge base (KB), while KBQA can be improved by learning contextual information from answer selection. To fulﬁll the goal of jointly learning these two tasks, we propose a novel multi-task learning scheme that utilizes multi-view attention learned from various perspectives to enable these tasks to interact with each other as well as learn more comprehensive sentence representations. The experiments conducted on several real-world datasets demonstrate the effectiveness of the proposed method, and the performance of answer selection and KBQA is improved. Also, the multi-view attention scheme is proved to be effective in assembling attentive information from different representational perspectives.},
	language = {en},
	urldate = {2020-04-21},
	journal = {arXiv:1812.02354 [cs]},
	author = {Deng, Yang and Xie, Yuexiang and Li, Yaliang and Yang, Min and Du, Nan and Fan, Wei and Lei, Kai and Shen, Ying},
	month = dec,
	year = {2018},
	note = {00000 
arXiv: 1812.02354},
}

@inproceedings{schlichtkrull_modeling_2018,
	title = {Modeling relational data with graph convolutional networks},
	booktitle = {European {Semantic} {Web} {Conference}},
	publisher = {Springer},
	author = {Schlichtkrull, Michael and Kipf, Thomas N. and Bloem, Peter and Van Den Berg, Rianne and Titov, Ivan and Welling, Max},
	year = {2018},
	note = {00000},
	pages = {593--607},
}

@article{micikevicius_mixed_2018,
	title = {Mixed {Precision} {Training}},
	url = {http://arxiv.org/abs/1710.03740},
	abstract = {Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.},
	urldate = {2020-06-17},
	journal = {arXiv:1710.03740 [cs, stat]},
	author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
	month = feb,
	year = {2018},
	note = {00000 
arXiv: 1710.03740},
	keywords = {new},
}

@article{yuan_machine_2017,
	title = {Machine comprehension by text-to-text neural question generation},
	journal = {arXiv preprint arXiv:1705.02012},
	author = {Yuan, Xingdi and Wang, Tong and Gulcehre, Caglar and Sordoni, Alessandro and Bachman, Philip and Subramanian, Sandeep and Zhang, Saizheng and Trischler, Adam},
	year = {2017},
	note = {00000},
}

@article{seyler_knowledge_2017,
	title = {Knowledge {Questions} from {Knowledge} {Graphs}},
	url = {http://arxiv.org/abs/1610.09935},
	doi = {10.1145/3121050.3121073},
	abstract = {We address the novel problem of automatically generating quiz-style knowledge questions from a knowledge graph such as DBpedia. Questions of this kind have ample applications, for instance, to educate users about or to evaluate their knowledge in a specific domain. To solve the problem, we propose an end-to-end approach. The approach first selects a named entity from the knowledge graph as an answer. It then generates a structured triple-pattern query, which yields the answer as its sole result. If a multiple-choice question is desired, the approach selects alternative answer options. Finally, our approach uses a template-based method to verbalize the structured query and yield a natural language question. A key challenge is estimating how difficult the generated question is to human users. To do this, we make use of historical data from the Jeopardy! quiz show and a semantically annotated Web-scale document collection, engineer suitable features, and train a logistic regression classifier to predict question difficulty. Experiments demonstrate the viability of our overall approach.},
	urldate = {2020-03-21},
	journal = {Proceedings of the ACM SIGIR International Conference on Theory of Information Retrieval - ICTIR '17},
	author = {Seyler, Dominic and Yahya, Mohamed and Berberich, Klaus},
	year = {2017},
	note = {00000 
arXiv: 1610.09935},
	pages = {11--18},
}

@inproceedings{huang_knowledge_2019,
	address = {Melbourne VIC, Australia},
	series = {{WSDM} '19},
	title = {Knowledge {Graph} {Embedding} {Based} {Question} {Answering}},
	isbn = {978-1-4503-5940-5},
	url = {https://doi.org/10.1145/3289600.3290956},
	doi = {10.1145/3289600.3290956},
	abstract = {Question answering over knowledge graph (QA-KG) aims to use facts in the knowledge graph (KG) to answer natural language questions. It helps end users more efficiently and more easily access the substantial and valuable knowledge in the KG, without knowing its data structures. QA-KG is a nontrivial problem since capturing the semantic meaning of natural language is difficult for a machine. Meanwhile, many knowledge graph embedding methods have been proposed. The key idea is to represent each predicate/entity as a low-dimensional vector, such that the relation information in the KG could be preserved. The learned vectors could benefit various applications such as KG completion and recommender systems. In this paper, we explore to use them to handle the QA-KG problem. However, this remains a challenging task since a predicate could be expressed in different ways in natural language questions. Also, the ambiguity of entity names and partial names makes the number of possible answers large. To bridge the gap, we propose an effective Knowledge Embedding based Question Answering (KEQA) framework. We focus on answering the most common types of questions, i.e., simple questions, in which each question could be answered by the machine straightforwardly if its single head entity and single predicate are correctly identified. To answer a simple question, instead of inferring its head entity and predicate directly, KEQA targets at jointly recovering the question's head entity, predicate, and tail entity representations in the KG embedding spaces. Based on a carefully-designed joint distance metric, the three learned vectors' closest fact in the KG is returned as the answer. Experiments on a widely-adopted benchmark demonstrate that the proposed KEQA outperforms the state-of-the-art QA-KG methods.},
	urldate = {2020-03-10},
	booktitle = {Proceedings of the {Twelfth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Huang, Xiao and Zhang, Jingyuan and Li, Dingcheng and Li, Ping},
	month = jan,
	year = {2019},
	note = {00000},
	pages = {105--113},
}

@article{rajpurkar_know_2018,
	title = {Know {What} {You} {Don}'t {Know}: {Unanswerable} {Questions} for {SQuAD}},
	shorttitle = {Know {What} {You} {Don}'t {Know}},
	url = {http://arxiv.org/abs/1806.03822},
	abstract = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86\% F1 on SQuAD 1.1 achieves only 66\% F1 on SQuAD 2.0.},
	urldate = {2020-04-27},
	journal = {arXiv:1806.03822 [cs]},
	author = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
	month = jun,
	year = {2018},
	note = {00000 
arXiv: 1806.03822},
	keywords = {dataset, new},
}

@article{cui_kbqa_2017,
	title = {{KBQA}: learning question answering over {QA} corpora and knowledge bases},
	volume = {10},
	issn = {21508097},
	shorttitle = {{KBQA}},
	url = {http://dl.acm.org/citation.cfm?doid=3055540.3055549},
	doi = {10.14778/3055540.3055549},
	abstract = {Question answering (QA) has become a popular way for humans to access billion-scale knowledge bases. Unlike web search, QA over a knowledge base gives out accurate and concise results, provided that natural language questions can be understood and mapped precisely to structured queries over the knowledge base. The challenge, however, is that a human can ask one question in many different ways. Previous approaches have natural limits due to their representations: rule based approaches only understand a small set of “canned” questions, while keyword based or synonym based approaches cannot fully understand the questions. In this paper, we design a new kind of question representation: templates, over a billion scale knowledge base and a million scale QA corpora. For example, for questions about a city’s population, we learn templates such as What’s the population of \$city?, How many people are there in \$city?. We learned 27 million templates for 2782 intents. Based on these templates, our QA system KBQA effectively supports binary factoid questions, as well as complex questions which are composed of a series of binary factoid questions. Furthermore, we expand predicates in RDF knowledge base, which boosts the coverage of knowledge base by 57 times. Our QA system beats all other state-of-art works on both effectiveness and efﬁciency over QALD benchmarks.},
	language = {en},
	number = {5},
	urldate = {2020-03-10},
	journal = {Proceedings of the VLDB Endowment},
	author = {Cui, Wanyun and Xiao, Yanghua and Wang, Haixun and Song, Yangqiu and Hwang, Seung-won and Wang, Wei},
	month = jan,
	year = {2017},
	note = {00000},
	pages = {565--576},
}

@inproceedings{dai_intent_2017,
	address = {Taipei},
	title = {Intent {Identification} for {Knowledge} {Base} {Question} {Answering}},
	isbn = {978-1-5386-4203-0},
	url = {https://ieeexplore.ieee.org/document/8356917/},
	doi = {10.1109/TAAI.2017.18},
	urldate = {2020-03-10},
	booktitle = {2017 {Conference} on {Technologies} and {Applications} of {Artificial} {Intelligence} ({TAAI})},
	publisher = {IEEE},
	author = {Dai, Feifei and Feng, Chong and Wang, Zhiqiang and Pei, Yuxia and Huang, Heyan},
	month = dec,
	year = {2017},
	note = {00000},
	pages = {96--99},
}

@article{liang12_how_2017,
	title = {How to keep a knowledge base synchronized with its encyclopedia source},
	author = {Liang12, Jiaqing and Zhang, Sheng and Xiao134, Yanghua},
	year = {2017},
	note = {00000},
}

@article{herbert_intelligent_2018,
	title = {Intelligent conversation system using multiple classification ripple down rules and conversational context},
	volume = {112},
	issn = {0957-4174},
	url = {http://www.sciencedirect.com/science/article/pii/S095741741830407X},
	doi = {10.1016/j.eswa.2018.06.049},
	abstract = {We introduce an extension to Multiple Classification Ripple Down Rules (MCRDR), called Contextual MCRDR (C-MCRDR). We apply C-MCRDR knowledge-base systems (KBS) to the Textual Question Answering (TQA) and Natural Language Interface to Databases (NLIDB) paradigms in restricted domains as a type of spoken dialog system (SDS) or conversational agent (CA). C-MCRDR implicitly maintains topical conversational context, and intra-dialog context is retained allowing explicit referencing in KB rule conditions and classifications. To facilitate NLIDB, post-inference C-MCRDR classifications can include generic query referencing – query specificity is achieved by the binding of pre-identified context. In contrast to other scripted, or syntactically complex systems, the KB of the live system can easily be maintained courtesy of the RDR knowledge engineering approach. For evaluation, we applied this system to a pedagogical domain that uses a production database for the generation of offline course-related documents. Our system complemented the domain by providing a spoken or textual question-answering alternative for undergraduates based on the same production database. The developed system incorporates a speech-enabled chatbot interface via Automatic Speech Recognition (ASR) and experimental results from a live, integrated feedback rating system showed significant user acceptance, indicating the approach is promising, feasible and further work is warranted. Evaluation of the prototype’s viability found the system responded appropriately for 80.3\% of participant requests in the tested domain, and it responded inappropriately for 19.7\% of requests due to incorrect dialog classifications (4.4\%) or out of scope requests (15.3\%). Although the semantic range of the evaluated domain was relatively shallow, we conjecture that the developed system is readily adoptable as a CA NLIDB tool in other more semantically-rich domains and it shows promise in single or multi-domain environments.},
	language = {en},
	urldate = {2020-03-10},
	journal = {Expert Systems with Applications},
	author = {Herbert, David and Kang, Byeong Ho},
	month = dec,
	year = {2018},
	note = {00000},
	pages = {342--352},
}

@article{pan_frustratingly_2019,
	title = {Frustratingly {Easy} {Natural} {Question} {Answering}},
	url = {http://arxiv.org/abs/1909.05286},
	abstract = {Existing literature on Question Answering (QA) mostly focuses on algorithmic novelty, data augmentation, or increasingly large pre-trained language models like XLNet and RoBERTa. Additionally, a lot of systems on the QA leaderboards do not have associated research documentation in order to successfully replicate their experiments. In this paper, we outline these algorithmic components such as Attentionover-Attention, coupled with data augmentation and ensembling strategies that have shown to yield state-of-the-art results on benchmark datasets like SQuAD, even achieving super-human performance. Contrary to these prior results, when we evaluate on the recently proposed Natural Questions benchmark dataset, we ﬁnd that an incredibly simple approach of transfer learning from BERT outperforms the previous state-of-the-art system trained on 4 million more examples than ours by 1.9 F1 points. Adding ensembling strategies further improves that number by 2.3 F1 points.},
	language = {en},
	urldate = {2020-04-29},
	journal = {arXiv:1909.05286 [cs]},
	author = {Pan, Lin and Chakravarti, Rishav and Ferritto, Anthony and Glass, Michael and Gliozzo, Alfio and Roukos, Salim and Florian, Radu and Sil, Avirup},
	month = sep,
	year = {2019},
	note = {00000 
arXiv: 1909.05286},
	keywords = {new},
}

@article{saxe_exact_2014,
	title = {Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
	url = {http://arxiv.org/abs/1312.6120},
	abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
	urldate = {2020-03-27},
	journal = {arXiv:1312.6120 [cond-mat, q-bio, stat]},
	author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
	month = feb,
	year = {2014},
	note = {00000 
arXiv: 1312.6120},
}

@article{yu_deep_2014,
	title = {Deep {Learning} for {Answer} {Sentence} {Selection}},
	url = {http://arxiv.org/abs/1412.1632},
	abstract = {Answer sentence selection is the task of identifying sentences that contain the answer to a given question. This is an important problem in its own right as well as in the larger context of open domain question answering. We propose a novel approach to solving this task via means of distributed representations, and learn to match questions with answers by considering their semantic encoding. This contrasts prior work on this task, which typically relies on classifiers with large numbers of hand-crafted syntactic and semantic features and various external resources. Our approach does not require any feature engineering nor does it involve specialist linguistic data, making this model easily applicable to a wide range of domains and languages. Experimental results on a standard benchmark dataset from TREC demonstrate that---despite its simplicity---our model matches state of the art performance on the answer sentence selection task.},
	urldate = {2020-04-21},
	journal = {arXiv:1412.1632 [cs]},
	author = {Yu, Lei and Hermann, Karl Moritz and Blunsom, Phil and Pulman, Stephen},
	month = dec,
	year = {2014},
	note = {00000 
arXiv: 1412.1632},
	keywords = {new},
}

@inproceedings{bao_constraint-based_2016,
	title = {Constraint-based question answering with knowledge graph},
	booktitle = {Proceedings of {COLING} 2016, the 26th {International} {Conference} on {Computational} {Linguistics}: {Technical} {Papers}},
	author = {Bao, Junwei and Duan, Nan and Yan, Zhao and Zhou, Ming and Zhao, Tiejun},
	year = {2016},
	note = {00000},
	pages = {2503--2514},
}

@inproceedings{kim_convolutional_2014,
	address = {Doha, Qatar},
	title = {Convolutional {Neural} {Networks} for {Sentence} {Classification}},
	url = {http://aclweb.org/anthology/D14-1181},
	doi = {10.3115/v1/D14-1181},
	abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classiﬁcation tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-speciﬁc vectors through ﬁne-tuning offers further gains in performance. We additionally propose a simple modiﬁcation to the architecture to allow for the use of both task-speciﬁc and static vectors. The CNN modelsdiscussedhereinimproveuponthe state of the art on 4 out of 7 tasks, which include sentiment analysis and question classiﬁcation.},
	language = {en},
	urldate = {2020-03-10},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Kim, Yoon},
	year = {2014},
	note = {00000},
	pages = {1746--1751},
}

@article{bauer_commonsense_nodate,
	title = {Commonsense for {Generative} {Multi}-{Hop} {Question} {Answering} {Tasks}},
	abstract = {Reading comprehension QA tasks have seen a recent surge in popularity, yet most works have focused on fact-ﬁnding extractive QA. We instead focus on a more challenging multihop generative task (NarrativeQA), which requires the model to reason, gather, and synthesize disjoint pieces of information within the context to generate an answer. This type of multi-step reasoning also often requires understanding implicit relations, which humans resolve via external, background commonsense knowledge. We ﬁrst present a strong generative baseline that uses a multi-attention mechanism to perform multiple hops of reasoning and a pointer-generator decoder to synthesize the answer. This model performs substantially better than previous generative models, and is competitive with current state-of-theart span prediction models. We next introduce a novel system for selecting grounded multi-hop relational commonsense information from ConceptNet via a pointwise mutual information and term-frequency based scoring function. Finally, we effectively use this extracted commonsense information to ﬁll in gaps of reasoning between context hops, using a selectively-gated attention mechanism. This boosts the model’s performance signiﬁcantly (also veriﬁed via human evaluation), establishing a new state-of-the-art for the task. We also show promising initial results of the generalizability of our background knowledge enhancements by demonstrating some improvement on QAngaroo-WikiHop, another multihop reasoning dataset.},
	language = {en},
	author = {Bauer, Lisa and Wang, Yicheng and Bansal, Mohit},
	note = {00067},
	pages = {22},
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2020-03-29},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {14406 
arXiv: 1810.04805},
}

@article{das_chains_2017,
	title = {Chains of {Reasoning} over {Entities}, {Relations}, and {Text} using {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1607.01426},
	abstract = {Our goal is to combine the rich multistep inference of symbolic logical reasoning with the generalization capabilities of neural networks. We are particularly interested in complex reasoning about entities and relations in text and large-scale knowledge bases (KBs). Neelakantan et al. (2015) use RNNs to compose the distributed semantics of multi-hop paths in KBs; however for multiple reasons, the approach lacks accuracy and practicality. This paper proposes three significant modeling advances: (1) we learn to jointly reason about relations, entities, and entity-types; (2) we use neural attention modeling to incorporate multiple paths; (3) we learn to share strength in a single RNN that represents logical composition across all relations. On a largescale Freebase+ClueWeb prediction task, we achieve 25\% error reduction, and a 53\% error reduction on sparse relations due to shared strength. On chains of reasoning in WordNet we reduce error in mean quantile by 84\% versus previous state-of-the-art. The code and data are available at https://rajarshd.github.io/ChainsofReasoning},
	urldate = {2020-03-21},
	journal = {arXiv:1607.01426 [cs]},
	author = {Das, Rajarshi and Neelakantan, Arvind and Belanger, David and McCallum, Andrew},
	month = may,
	year = {2017},
	note = {00156 
arXiv: 1607.01426},
}

@inproceedings{zhou_attention-based_2016,
	address = {Berlin, Germany},
	title = {Attention-{Based} {Bidirectional} {Long} {Short}-{Term} {Memory} {Networks} for {Relation} {Classification}},
	url = {https://www.aclweb.org/anthology/P16-2034},
	doi = {10.18653/v1/P16-2034},
	urldate = {2020-03-17},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zhou, Peng and Shi, Wei and Tian, Jun and Qi, Zhenyu and Li, Bingchen and Hao, Hongwei and Xu, Bo},
	month = aug,
	year = {2016},
	note = {00869},
	pages = {207--212},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is all you need},
	booktitle = {Advances in neural information processing systems},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\textbackslash}Lukasz and Polosukhin, Illia},
	year = {2017},
	note = {16272},
	pages = {5998--6008},
}

@article{lan_albert_2020,
	title = {{ALBERT}: {A} {Lite} {BERT} for {Self}-supervised {Learning} of {Language} {Representations}},
	shorttitle = {{ALBERT}},
	url = {http://arxiv.org/abs/1909.11942},
	abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and {\textbackslash}squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT.},
	urldate = {2020-03-29},
	journal = {arXiv:1909.11942 [cs]},
	author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	month = feb,
	year = {2020},
	note = {00846 
arXiv: 1909.11942},
}

@article{chen_review_2020,
	title = {A review: {Knowledge} reasoning over knowledge graph},
	volume = {141},
	issn = {0957-4174},
	shorttitle = {A review},
	url = {http://www.sciencedirect.com/science/article/pii/S0957417419306669},
	doi = {10.1016/j.eswa.2019.112948},
	abstract = {Mining valuable hidden knowledge from large-scale data relies on the support of reasoning technology. Knowledge graphs, as a new type of knowledge representation, have gained much attention in natural language processing. Knowledge graphs can effectively organize and represent knowledge so that it can be efficiently utilized in advanced applications. Recently, reasoning over knowledge graphs has become a hot research topic, since it can obtain new knowledge and conclusions from existing data. Herein we review the basic concept and definitions of knowledge reasoning and the methods for reasoning over knowledge graphs. Specifically, we dissect the reasoning methods into three categories: rule-based reasoning, distributed representation-based reasoning and neural network-based reasoning. We also review the related applications of knowledge graph reasoning, such as knowledge graph completion, question answering, and recommender systems. Finally, we discuss the remaining challenges and research opportunities for knowledge graph reasoning.},
	language = {en},
	urldate = {2020-03-24},
	journal = {Expert Systems with Applications},
	author = {Chen, Xiaojun and Jia, Shengbin and Xiang, Yang},
	month = mar,
	year = {2020},
	note = {00027},
	pages = {112948},
}

@article{guo_survey_2020,
	title = {A {Survey} on {Knowledge} {Graph}-{Based} {Recommender} {Systems}},
	url = {http://arxiv.org/abs/2003.00911},
	abstract = {To solve the information explosion problem and enhance user experience in various online applications, recommender systems have been developed to model users preferences. Although numerous efforts have been made toward more personalized recommendations, recommender systems still suffer from several challenges, such as data sparsity and cold start. In recent years, generating recommendations with the knowledge graph as side information has attracted considerable interest. Such an approach can not only alleviate the abovementioned issues for a more accurate recommendation, but also provide explanations for recommended items. In this paper, we conduct a systematical survey of knowledge graph-based recommender systems. We collect recently published papers in this field and summarize them from two perspectives. On the one hand, we investigate the proposed algorithms by focusing on how the papers utilize the knowledge graph for accurate and explainable recommendation. On the other hand, we introduce datasets used in these works. Finally, we propose several potential research directions in this field.},
	urldate = {2020-03-12},
	journal = {arXiv:2003.00911 [cs, stat]},
	author = {Guo, Qingyu and Zhuang, Fuzhen and Qin, Chuan and Zhu, Hengshu and Xie, Xing and Xiong, Hui and He, Qing},
	month = feb,
	year = {2020},
	note = {00004 
arXiv: 2003.00911},
}

@article{wei_novel_2020,
	title = {A {Novel} {Cascade} {Binary} {Tagging} {Framework} for {Relational} {Triple} {Extraction}},
	url = {http://arxiv.org/abs/1909.03227},
	abstract = {Extracting relational triples from unstructured text is crucial for large-scale knowledge graph construction. However, few existing works excel in solving the overlapping triple problem where multiple relational triples in the same sentence share the same entities. In this work, we introduce a fresh perspective to revisit the relational triple extraction task and propose a novel cascade binary tagging framework (CasRel) derived from a principled problem formulation. Instead of treating relations as discrete labels as in previous works, our new framework models relations as functions that map subjects to objects in a sentence, which naturally handles the overlapping problem. Experiments show that the CasRel framework already outperforms state-of-the-art methods even when its encoder module uses a randomly initialized BERT encoder, showing the power of the new tagging framework. It enjoys further performance boost when employing a pre-trained BERT encoder, outperforming the strongest baseline by 17.5 and 30.2 absolute gain in F1-score on two public datasets NYT and WebNLG, respectively. In-depth analysis on different scenarios of overlapping triples shows that the method delivers consistent performance gain across all these scenarios. The source code and data are released online.},
	urldate = {2020-06-19},
	journal = {arXiv:1909.03227 [cs]},
	author = {Wei, Zhepei and Su, Jianlin and Wang, Yue and Tian, Yuan and Chang, Yi},
	month = apr,
	year = {2020},
	note = {00008 
arXiv: 1909.03227},
}

@article{alberti_bert_2019,
	title = {A {BERT} {Baseline} for the {Natural} {Questions}},
	url = {http://arxiv.org/abs/1901.08634},
	abstract = {This technical note describes a new baseline for the Natural Questions. Our model is based on BERT and reduces the gap between the model F1 scores reported in the original dataset paper and the human upper bound by 30\% and 50\% relative for the long and short answer tasks respectively. This baseline has been submitted to the official NQ leaderboard at ai.google.com/research/NaturalQuestions. Code, preprocessed data and pretrained model are available at https://github.com/google-research/language/tree/master/language/question\_answering/bert\_joint.},
	urldate = {2020-03-28},
	journal = {arXiv:1901.08634 [cs]},
	author = {Alberti, Chris and Lee, Kenton and Collins, Michael},
	month = dec,
	year = {2019},
	note = {00052 
arXiv: 1901.08634},
	keywords = {done, new},
}

@misc{noauthor_pdf_nodate,
	title = {({PDF}) {Bagging} {BERT} {Models} for {Robust} {Aggression} {Identification}},
	url = {https://www.researchgate.net/publication/340418172_Bagging_BERT_Models_for_Robust_Aggression_Identification},
	urldate = {2020-06-19},
	note = {00007},
	keywords = {new},
}

@article{cui_unsupervised_2020,
	title = {Unsupervised {Natural} {Language} {Inference} via {Decoupled} {Multimodal} {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2010.08200},
	abstract = {We propose to solve the natural language inference problem without any supervision from the inference labels via task-agnostic multimodal pretraining. Although recent studies of multimodal self-supervised learning also represent the linguistic and visual context, their encoders for different modalities are coupled. Thus they cannot incorporate visual information when encoding plain text alone. In this paper, we propose Multimodal Aligned Contrastive Decoupled learning (MACD) network. MACD forces the decoupled text encoder to represent the visual information via contrastive learning. Therefore, it embeds visual knowledge even for plain text inference. We conducted comprehensive experiments over plain text inference datasets (i.e. SNLI and STS-B). The unsupervised MACD even outperforms the fully-supervised BiLSTM and BiLSTM+ELMO on STS-B.},
	urldate = {2021-01-07},
	journal = {arXiv:2010.08200 [cs]},
	author = {Cui, Wanyun and Zheng, Guangyu and Wang, Wei},
	month = oct,
	year = {2020},
	note = {00000 
arXiv: 2010.08200},
}

@article{zhang_semantics-aware_2020,
	title = {Semantics-aware {BERT} for {Language} {Understanding}},
	url = {http://arxiv.org/abs/1909.02209},
	abstract = {The latest work on language representations carefully integrates contextualized features into language model training, which enables a series of success especially in various machine reading comprehension and natural language inference tasks. However, the existing language representation models including ELMo, GPT and BERT only exploit plain context-sensitive features such as character or word embeddings. They rarely consider incorporating structured semantic information which can provide rich semantics for language representation. To promote natural language understanding, we propose to incorporate explicit contextual semantics from pre-trained semantic role labeling, and introduce an improved language representation model, Semanticsaware BERT (SemBERT), which is capable of explicitly absorbing contextual semantics over a BERT backbone. SemBERT keeps the convenient usability of its BERT precursor in a light ﬁne-tuning way without substantial task-speciﬁc modiﬁcations. Compared with BERT, semantics-aware BERT is as simple in concept but more powerful. It obtains new state-ofthe-art or substantially improves results on ten reading comprehension and language inference tasks.},
	language = {en},
	urldate = {2020-09-15},
	journal = {arXiv:1909.02209 [cs]},
	author = {Zhang, Zhuosheng and Wu, Yuwei and Zhao, Hai and Li, Zuchao and Zhang, Shuailiang and Zhou, Xi and Zhou, Xiang},
	month = feb,
	year = {2020},
	note = {00000 
arXiv: 1909.02209},
}

@article{zhang_revisiting_2020,
	title = {Revisiting {Few}-sample {BERT} {Fine}-tuning},
	url = {http://arxiv.org/abs/2006.05987},
	abstract = {We study the problem of few-sample fine-tuning of BERT contextual representations, and identify three sub-optimal choices in current, broadly adopted practices. First, we observe that the omission of the gradient bias correction in the BERTAdam optimizer results in fine-tuning instability. We also find that parts of the BERT network provide a detrimental starting point for fine-tuning, and simply re-initializing these layers speeds up learning and improves performance. Finally, we study the effect of training time, and observe that commonly used recipes often do not allocate sufficient time for training. In light of these findings, we re-visit recently proposed methods to improve few-sample fine-tuning with BERT and re-evaluate their effectiveness. Generally, we observe a decrease in their relative impact when modifying the fine-tuning process based on our findings.},
	urldate = {2020-11-30},
	journal = {arXiv:2006.05987 [cs]},
	author = {Zhang, Tianyi and Wu, Felix and Katiyar, Arzoo and Weinberger, Kilian Q. and Artzi, Yoav},
	month = jul,
	year = {2020},
	note = {00000 
arXiv: 2006.05987},
}

@article{mccoy_right_2019,
	title = {Right for the {Wrong} {Reasons}: {Diagnosing} {Syntactic} {Heuristics} in {Natural} {Language} {Inference}},
	shorttitle = {Right for the {Wrong} {Reasons}},
	url = {http://arxiv.org/abs/1902.01007},
	abstract = {A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area},
	urldate = {2020-09-15},
	journal = {arXiv:1902.01007 [cs]},
	author = {McCoy, R. Thomas and Pavlick, Ellie and Linzen, Tal},
	month = jun,
	year = {2019},
	note = {00000 
arXiv: 1902.01007},
}

@article{yang_rethinking_2020,
	title = {Rethinking the {Value} of {Labels} for {Improving} {Class}-{Imbalanced} {Learning}},
	url = {http://arxiv.org/abs/2006.07529},
	abstract = {Real-world data often exhibits long-tailed distributions with heavy class imbalance, posing great challenges for deep recognition models. We identify a persisting dilemma on the value of labels in the context of imbalanced learning: on the one hand, supervision from labels typically leads to better results than its unsupervised counterparts; on the other hand, heavily imbalanced data naturally incurs "label bias" in the classifier, where the decision boundary can be drastically altered by the majority classes. In this work, we systematically investigate these two facets of labels. We demonstrate, theoretically and empirically, that class-imbalanced learning can significantly benefit in both semi-supervised and self-supervised manners. Specifically, we confirm that (1) positively, imbalanced labels are valuable: given more unlabeled data, the original labels can be leveraged with the extra data to reduce label bias in a semi-supervised manner, which greatly improves the final classifier; (2) negatively however, we argue that imbalanced labels are not useful always: classifiers that are first pre-trained in a self-supervised manner consistently outperform their corresponding baselines. Extensive experiments on large-scale imbalanced datasets verify our theoretically grounded strategies, showing superior performance over previous state-of-the-arts. Our intriguing findings highlight the need to rethink the usage of imbalanced labels in realistic long-tailed tasks. Code is available at https://github.com/YyzHarry/imbalanced-semi-self.},
	urldate = {2020-12-08},
	journal = {arXiv:2006.07529 [cs, stat]},
	author = {Yang, Yuzhe and Xu, Zhi},
	month = sep,
	year = {2020},
	note = {00000 
arXiv: 2006.07529},
}

@article{mosbach_stability_2020,
	title = {On the {Stability} of {Fine}-tuning {BERT}: {Misconceptions}, {Explanations}, and {Strong} {Baselines}},
	shorttitle = {On the {Stability} of {Fine}-tuning {BERT}},
	url = {http://arxiv.org/abs/2006.04884},
	abstract = {Fine-tuning pre-trained transformer-based language models such as BERT has become a common practice dominating leaderboards across various NLP benchmarks. Despite the strong empirical performance of fine-tuned models, fine-tuning is an unstable process: training the same model with multiple random seeds can result in a large variance of the task performance. Previous literature (Devlin et al., 2019; Lee et al., 2020; Dodge et al., 2020) identified two potential reasons for the observed instability: catastrophic forgetting and small size of the fine-tuning datasets. In this paper, we show that both hypotheses fail to explain the fine-tuning instability. We analyze BERT, RoBERTa, and ALBERT, fine-tuned on three commonly used datasets from the GLUE benchmark, and show that the observed instability is caused by optimization difficulties that lead to vanishing gradients. Additionally, we show that the remaining variance of the downstream task performance can be attributed to differences in generalization where fine-tuned models with the same training loss exhibit noticeably different test performance. Based on our analysis, we present a simple but strong baseline that makes fine-tuning BERT-based models significantly more stable than the previously proposed approaches. Code to reproduce our results is available online: https://github.com/uds-lsv/bert-stable-fine-tuning.},
	urldate = {2020-12-13},
	journal = {arXiv:2006.04884 [cs, stat]},
	author = {Mosbach, Marius and Andriushchenko, Maksym and Klakow, Dietrich},
	month = oct,
	year = {2020},
	note = {00000 
arXiv: 2006.04884},
}

@article{sun_how_2020,
	title = {How to {Fine}-{Tune} {BERT} for {Text} {Classification}?},
	url = {http://arxiv.org/abs/1905.05583},
	abstract = {Language model pre-training has proven to be useful in learning universal language representations. As a state-of-the-art language model pre-training model, BERT (Bidirectional Encoder Representations from Transformers) has achieved amazing results in many language understanding tasks. In this paper, we conduct exhaustive experiments to investigate different fine-tuning methods of BERT on text classification task and provide a general solution for BERT fine-tuning. Finally, the proposed solution obtains new state-of-the-art results on eight widely-studied text classification datasets.},
	urldate = {2020-12-13},
	journal = {arXiv:1905.05583 [cs]},
	author = {Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
	month = feb,
	year = {2020},
	note = {00174 
arXiv: 1905.05583},
}

@article{donoho_high-dimensional_2000,
	title = {High-dimensional data analysis: {The} curses and blessings of dimensionality},
	volume = {1},
	shorttitle = {High-dimensional data analysis},
	number = {2000},
	journal = {AMS math challenges lecture},
	author = {Donoho, David L.},
	year = {2000},
	note = {01258 
Publisher: Citeseer},
	pages = {32},
}

@inproceedings{wang_glue_2018,
	address = {Brussels, Belgium},
	title = {{GLUE}: {A} {Multi}-{Task} {Benchmark} and {Analysis} {Platform} for {Natural} {Language} {Understanding}},
	shorttitle = {{GLUE}},
	url = {http://aclweb.org/anthology/W18-5446},
	doi = {10.18653/v1/W18-5446},
	abstract = {For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and ﬁnd that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems.},
	language = {en},
	urldate = {2020-09-15},
	booktitle = {Proceedings of the 2018 {EMNLP} {Workshop} {BlackboxNLP}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
	year = {2018},
	note = {01078},
	pages = {353--355},
}

@article{schmidt_descending_2020,
	title = {Descending through a {Crowded} {Valley} -- {Benchmarking} {Deep} {Learning} {Optimizers}},
	url = {http://arxiv.org/abs/2007.01547},
	abstract = {Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost 35,000 individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.},
	urldate = {2020-11-06},
	journal = {arXiv:2007.01547 [cs, stat]},
	author = {Schmidt, Robin M. and Schneider, Frank and Hennig, Philipp},
	month = oct,
	year = {2020},
	note = {00004 
arXiv: 2007.01547},
}

@inproceedings{bowman_large_2015,
	address = {Lisbon, Portugal},
	title = {A large annotated corpus for learning natural language inference},
	url = {http://aclweb.org/anthology/D15-1075},
	doi = {10.18653/v1/D15-1075},
	abstract = {Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classiﬁers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the ﬁrst time.},
	language = {en},
	urldate = {2020-09-15},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
	year = {2015},
	note = {01696},
	pages = {632--642},
}

@article{wang_morley-wang-xu_2018,
	title = {Morley-{Wang}-{Xu} element methods with penalty for a fourth order elliptic singular perturbation problem},
	volume = {44},
	issn = {1572-9044},
	url = {https://doi.org/10.1007/s10444-017-9572-6},
	doi = {10.1007/s10444-017-9572-6},
	abstract = {Two Morley-Wang-Xu element methods with penalty for the fourth order elliptic singular perturbation problem are proposed in this paper, including the interior penalty Morley-Wang-Xu element method and the super penalty Morley-Wang-Xu element method. The key idea in designing these two methods is combining the Morley-Wang-Xu element and penalty formulation for the Laplace operator. Robust a priori error estimates are derived under minimal regularity assumptions on the exact solution by means of some established a posteriori error estimates. Finally, we present some numerical results to demonstrate the theoretical estimates.},
	language = {en},
	number = {4},
	urldate = {2020-10-11},
	journal = {Advances in Computational Mathematics},
	author = {Wang, Wenqing and Huang, Xuehai and Tang, Kai and Zhou, Ruiyue},
	month = aug,
	year = {2018},
	note = {00000},
	keywords = {foesp},
	pages = {1041--1061},
}

@article{gallistl_morley_2014,
	title = {Morley {Finite} {Element} {Method} for the {Eigenvalues} of the {Biharmonic} {Operator}},
	url = {http://arxiv.org/abs/1406.2876},
	abstract = {This paper studies the nonconforming Morley finite element approximation of the eigenvalues of the biharmonic operator. A new \$C{\textasciicircum}1\$ conforming companion operator leads to an \$L{\textasciicircum}2\$ error estimate for the Morley finite element method which directly compares the \$L{\textasciicircum}2\$ error with the error in the energy norm and, hence, can dispense with any additional regularity assumptions. Furthermore, the paper presents new eigenvalue error estimates for nonconforming finite elements that bound the error of (possibly multiple or clustered) eigenvalues by the approximation error of the computed invariant subspace. An application is the proof of optimal convergence rates for the adaptive Morley finite element method for eigenvalue clusters.},
	urldate = {2020-09-21},
	journal = {arXiv:1406.2876 [math]},
	author = {Gallistl, Dietmar},
	month = oct,
	year = {2014},
	note = {00000 
arXiv: 1406.2876},
}

@article{wang_modified_2006,
	title = {Modified {Morley} element method for a fourth order elliptic singular perturbation problem},
	journal = {Journal of Computational Mathematics},
	author = {Wang, Ming and Xu, Jin-chao and Hu, Yu-cheng},
	year = {2006},
	note = {00000 
Publisher: JSTOR},
	pages = {113--120},
}

@article{dunavant_high_1985,
	title = {High degree efficient symmetrical {Gaussian} quadrature rules for the triangle},
	volume = {21},
	issn = {0029-5981, 1097-0207},
	url = {http://doi.wiley.com/10.1002/nme.1620210612},
	doi = {10.1002/nme.1620210612},
	abstract = {Gaussian quadrature is required for the computation of matrices based on the isoparametric formulztion of the finite element method. A brief review of existing quadrature rules for the triangle is given, and the method for the determination of high degree efficient symmetrical rules for the triangle is discussed. New quadrature rules of degree 12-20 are presented, and a short FORTRAN program is included.},
	language = {en},
	number = {6},
	urldate = {2020-07-29},
	journal = {International Journal for Numerical Methods in Engineering},
	author = {Dunavant, D. A.},
	month = jun,
	year = {1985},
	note = {00000},
	pages = {1129--1148},
}

@book{elman_finite_2014,
	address = {Oxford, United Kingdom},
	edition = {Second edition},
	series = {Numerical mathematics and scientific computation},
	title = {Finite elements and fast iterative solvers: with applications in incompressible fluid dynamics},
	isbn = {978-0-19-967879-2 978-0-19-967880-8},
	shorttitle = {Finite elements and fast iterative solvers},
	language = {en},
	publisher = {Oxford University Press},
	author = {Elman, Howard C. and Silvester, David J. and Wathen, Andrew J.},
	year = {2014},
	note = {00000},
}

@incollection{reichenbach_finite_1996,
	address = {Berlin, Heidelberg},
	title = {Finite {Element} {Methods} for the {Stokes} {Equation}},
	isbn = {978-3-642-85238-1},
	url = {https://doi.org/10.1007/978-3-642-85238-1_18},
	abstract = {The description of the finite element method (FEM) for the Stokes problem is considered. Boundary conditions that are important in fluid mechanics are discussed. The condition of incompressibility leads to a saddle-point problem. The approximation by a mixed finite element method requires the choice of suitable finite elements. Otherwise the computation suffers from instability and useless results are produced.},
	language = {en},
	urldate = {2020-08-13},
	booktitle = {Computational {Physics}: {Selected} {Methods} {Simple} {Exercises} {Serious} {Applications}},
	publisher = {Springer},
	author = {Reichenbach, Jochen and Aksel, Nuri},
	editor = {Hoffmann, Karl Heinz and Schreiber, Michael},
	year = {1996},
	doi = {10.1007/978-3-642-85238-1_18},
	note = {00000 },
	keywords = {Stokes},
	pages = {329--340},
}

@article{chen_finite_nodate,
	title = {{FINITE} {ELEMENT} {METHODS} {FOR} {STOKES} {EQUATIONS}},
	language = {en},
	author = {Chen, Long},
	note = {00000},
	keywords = {Stokes},
	pages = {10},
}

@article{zheng_fast_2016,
	title = {Fast {Multilevel} {Solvers} for a {Class} of {Discrete} {Fourth} {Order} {Parabolic} {Problems}},
	volume = {69},
	issn = {0885-7474, 1573-7691},
	url = {http://link.springer.com/10.1007/s10915-016-0189-6},
	doi = {10.1007/s10915-016-0189-6},
	language = {en},
	number = {1},
	urldate = {2020-10-27},
	journal = {Journal of Scientific Computing},
	author = {Zheng, Bin and Chen, Luoping and Hu, Xiaozhe and Chen, Long and Nochetto, Ricardo H. and Xu, Jinchao},
	month = oct,
	year = {2016},
	note = {00000},
	pages = {201--226},
}

@article{stenberg_error_1990,
	title = {Error analysis of some finite element methods for the {Stokes} problem},
	volume = {54},
	url = {http://adsabs.harvard.edu/abs/1990MaCom..54..495S},
	doi = {10.1090/S0025-5718-1990-1010601-X},
	abstract = {We prove the optimal order of convergence for some two-dimensional finite element methods for the Stokes equations. First we consider methods of the Taylor-Hood type: the triangular  \{P\_3\} - \{P\_2\} element and the  \{Q\_k\} - \{Q\_\{k - 1\}\}, ,  k ≥ 2 , family of quadrilateral elements. Then we introduce two new low-order methods with piecewise constant approximations for the pressure. The analysis is performed using our macroelement technique, which is reviewed in a slightly altered form.},
	urldate = {2020-10-07},
	journal = {Mathematics of Computation},
	author = {Stenberg, Rolf},
	month = apr,
	year = {1990},
	note = {00000},
	keywords = {Stokes},
	pages = {495--508},
}

@article{meng_convergence_2019,
	title = {Convergence analysis of the {Adini} element on a {Shishkin} mesh for a singularly perturbed fourth-order problem in two dimensions},
	volume = {45},
	issn = {1572-9044},
	url = {https://doi.org/10.1007/s10444-018-9646-0},
	doi = {10.1007/s10444-018-9646-0},
	abstract = {We consider the singularly perturbed fourth-order boundary value problem ε2Δ2u −Δu = f on the unit square Ω⊂ℝ2\$\{{\textbackslash}Omega \}{\textbackslash}subset {\textbackslash}mathbb \{R\}{\textasciicircum}\{2\}\$, with boundary conditions u = ∂u/∂n = 0 on ∂Ω. Here, ε ∈ (0,1) is a small parameter. The problem is solved numerically by means of Adini finite elements—a simple nonconforming finite element method for this problem. Under reasonable assumptions on the structure of the boundary layers that appear in the solution, a family of suitable Shishkin meshes with N2 elements is constructed and convergence of the method is proved in a ‘broken’ version of the Sobolev norm v↦ε2{\textbar}v{\textbar}22+{\textbar}v{\textbar}121/2\$v{\textbackslash}mapsto {\textbackslash}left ({\textbackslash}varepsilon {\textasciicircum}\{2\}{\textbar}v{\textbar}\_\{2\}{\textasciicircum}\{2\} + {\textbar}v{\textbar}\_\{1\}{\textasciicircum}\{2\} {\textbackslash}right ){\textasciicircum}\{1/2\}\$. For a particular choice of the mesh, the error in the computed solution is at most C [ε1/2(N− 1 lnN)2 + min \{ε1/2,ε− 3/2N− 2\} + N− 3], where the constant C is independent of ε and N. Numerical results support our theoretical convergence rates, even for an example where not all the hypotheses of our theory are satisfied.},
	language = {en},
	number = {2},
	urldate = {2020-10-17},
	journal = {Advances in Computational Mathematics},
	author = {Meng, Xiangyun and Stynes, Martin},
	month = apr,
	year = {2019},
	note = {00000},
	keywords = {foesp},
	pages = {1105--1128},
}

@article{huang_morley-wang-xu_2020,
	title = {A {Morley}-{Wang}-{Xu} element method for a fourth order elliptic singular perturbation problem},
	url = {http://arxiv.org/abs/2011.14064},
	abstract = {A Morley-Wang-Xu (MWX) element method with a simply modified right hand side is proposed for a fourth order elliptic singular perturbation problem, in which the discrete bilinear form is standard as usual nonconforming finite element methods. The sharp error analysis is given for this MWX element method. And the Nitsche's technique is applied to the MXW element method to achieve the optimal convergence rate in the case of the boundary layers. An important feature of the MWX element method is solver-friendly. Based on a discrete Stokes complex in two dimensions, the MWX element method is decoupled into one Lagrange element method of Poisson equation, two Morley element methods of Poisson equation and one nonconforming \$P\_1\$-\$P\_0\$ element method of Brinkman problem, which implies efficient and robust solvers for the MWX element method. Some numerical examples are provided to verify the theoretical results.},
	urldate = {2020-12-09},
	journal = {arXiv:2011.14064 [cs, math]},
	author = {Huang, Xuehai and Shi, Yuling and Wang, Wenqing},
	month = nov,
	year = {2020},
	note = {00000 
arXiv: 2011.14064},
}

@article{chen_minimal_2016,
	title = {Minimal degree \${H}({\textbackslash}mathrm \{curl\})\$ and \${H}({\textbackslash}mathrm \{div\})\$ conforming finite elements on polytopal meshes},
	volume = {86},
	issn = {0025-5718, 1088-6842},
	url = {https://www.ams.org/mcom/2017-86-307/S0025-5718-2016-03152-X/},
	doi = {10.1090/mcom/3152},
	abstract = {We construct H(curl) and H(div) conforming ﬁnite elements on convex polygons and polyhedra with minimal possible degrees of freedom, i.e., the number of degrees of freedom is equal to the number of edges or faces of the polygon/polyhedron. The construction is based on generalized barycentric coordinates and the Whitney forms. In 3D, it currently requires the faces of the polyhedron be either triangles or parallelograms. Formula for computing basis functions are given. The ﬁnite elements satisfy discrete de Rham sequences in analogy to the well-known ones on simplices. Moreover, they reproduce existing H(curl)-H(div) elements on simplices, parallelograms, parallelepipeds, pyramids and triangular prisms. Approximation property of the constructed elements is also analyzed, by showing that the lowest-order simplicial N´el´elecRaviart-Thomas elements are subsets of the constructed elements on arbitrary polygons and certain polyhedra.},
	language = {en},
	number = {307},
	urldate = {2020-11-10},
	journal = {Mathematics of Computation},
	author = {Chen, Wenbin and Wang, Yanqiu},
	month = oct,
	year = {2016},
	note = {00000},
	pages = {2053--2087},
}

@article{he_mgnet_2019,
	title = {{MgNet}: {A} unified framework of multigrid and convolutional neural network},
	volume = {62},
	issn = {1674-7283, 1869-1862},
	shorttitle = {{MgNet}},
	url = {http://link.springer.com/10.1007/s11425-019-9547-2},
	doi = {10.1007/s11425-019-9547-2},
	abstract = {We develop a uniﬁed model, known as MgNet, that simultaneously recovers some convolutional neural networks (CNN) for image classiﬁcation and multigrid (MG) methods for solving discretized partial diﬀerential equations (PDEs). This model is based on close connections that we have observed and uncovered between the CNN and MG methodologies. For example, pooling operation and feature extraction in CNN correspond directly to restriction operation and iterative smoothers in MG, respectively. As the solution space is often the dual of the data space in PDEs, the analogous concept of feature space and data space (which are dual to each other) is introduced in CNN. With such connections and new concept in the uniﬁed model, the function of various convolution operations and pooling used in CNN can be better understood. As a result, modiﬁed CNN models (with fewer weights and hyperparameters) are developed that exhibit competitive and sometimes better performance in comparison with existing CNN models when applied to both CIFAR-10 and CIFAR-100 data sets.},
	language = {en},
	number = {7},
	urldate = {2020-11-10},
	journal = {Science China Mathematics},
	author = {He, Juncai and Xu, Jinchao},
	month = jul,
	year = {2019},
	note = {00000},
	pages = {1331--1354},
}

@article{han_knowledge_2020,
	title = {Knowledge of words: {An} interpretable approach for personality recognition from social media},
	volume = {194},
	issn = {09507051},
	shorttitle = {Knowledge of words},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705120300459},
	doi = {10.1016/j.knosys.2020.105550},
	abstract = {Personality is one of the fundamental and stable individual characteristics that can be detected from human behavioral data. With the rise of social media, increasing attention has been paid to the ability to recognize personality traits by analyzing the contents of user-generated text. Existing studies have used general psychological lexicons or machine learning, and even deep learning models, to predict personality, but their performance has been relatively poor or they have lacked the ability to interpret personality. In this paper, we present a novel interpretable personality recognition model based on a personality lexicon. First, we use word embedding techniques and prior-knowledge lexicons to automatically construct a Chinese semantic lexicon suitable for personality analysis. Based on this personality lexicon, we analyze the correlations between personality traits and semantic categories of words, and extract the semantic features of users’ microblogs to construct personality recognition models using classification algorithms. Extensive experiments are conducted to demonstrate that the proposed model can achieve significantly better performances compared to previous approaches.},
	language = {en},
	urldate = {2020-11-01},
	journal = {Knowledge-Based Systems},
	author = {Han, Songqiao and Huang, Hailiang and Tang, Yuqing},
	month = apr,
	year = {2020},
	note = {00001},
	pages = {105550},
}

@article{gustafsson_nitsches_2020,
	title = {Nitsche's method for {Kirchhoff} plates},
	url = {http://arxiv.org/abs/2007.00403},
	abstract = {We introduce a Nitsche’s method for the numerical approximation of the Kirchhoﬀ–Love plate equation under general Robin-type boundary conditions. We analyze the method by presenting a priori and a posteriori error estimates in mesh-dependent norms. Several numerical examples are given to validate the approach and demonstrate its properties.},
	language = {en},
	urldate = {2020-10-17},
	journal = {arXiv:2007.00403 [cs, math]},
	author = {Gustafsson, Tom and Stenberg, Rolf and Videman, Juha},
	month = jul,
	year = {2020},
	note = {00000 
arXiv: 2007.00403},
	keywords = {foesp},
}

@article{han_equation_2012,
	title = {An equation decomposition method for the numerical solution of a fourth-order elliptic singular perturbation problem: {TFPM} for {Fourth}-{Order} {Problem}},
	volume = {28},
	issn = {0749159X},
	shorttitle = {An equation decomposition method for the numerical solution of a fourth-order elliptic singular perturbation problem},
	url = {http://doi.wiley.com/10.1002/num.20666},
	doi = {10.1002/num.20666},
	language = {en},
	number = {3},
	urldate = {2020-10-17},
	journal = {Numerical Methods for Partial Differential Equations},
	author = {Han, Houde and Huang, Zhongyi},
	month = may,
	year = {2012},
	pages = {942--953},
}

@misc{noauthor_pdf_nodate-1,
	title = {崔万云博士论文.pdf},
	note = {00000},
}

@article{guzman_family_2012,
	title = {A family of non-conforming elements and the analysis of {Nitsche}’s method for a singularly perturbed fourth order problem},
	volume = {49},
	number = {2},
	journal = {Calcolo},
	author = {Guzmán, Johnny and Leykekhman, Dmitriy and Neilan, Michael},
	year = {2012},
	note = {00000 
Publisher: Springer},
	keywords = {Morley},
	pages = {95--125},
}

@inproceedings{bunescu_shortest_2005,
	address = {Vancouver, British Columbia, Canada},
	title = {A shortest path dependency kernel for relation extraction},
	url = {http://portal.acm.org/citation.cfm?doid=1220575.1220666},
	doi = {10.3115/1220575.1220666},
	abstract = {We present a novel approach to relation extraction, based on the observation that the information required to assert a relationship between two named entities in the same sentence is typically captured by the shortest path between the two entities in the dependency graph. Experiments on extracting top-level relations from the ACE (Automated Content Extraction) newspaper corpus show that the new shortest path dependency kernel outperforms a recent approach based on dependency tree kernels.},
	language = {en},
	urldate = {2020-08-11},
	booktitle = {Proceedings of the conference on {Human} {Language} {Technology} and {Empirical} {Methods} in {Natural} {Language} {Processing}  - {HLT} '05},
	publisher = {Association for Computational Linguistics},
	author = {Bunescu, Razvan C. and Mooney, Raymond J.},
	year = {2005},
	note = {00953},
	pages = {724--731},
}

@article{lafferty_conditional_nodate,
	title = {Conditional {Random} {Fields}: {Probabilistic} {Models} for {Segmenting} and {Labeling} {Sequence} {Data}},
	abstract = {We present conditional random ﬁelds, a framework for building probabilistic models to segment and label sequence data. Conditional random ﬁelds offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random ﬁelds also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random ﬁelds and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data.},
	language = {en},
	author = {Lafferty, John and McCallum, Andrew and Pereira, Fernando},
	pages = {8},
}

@article{han_overview_2020,
	title = {Overview of the {CCKS} 2019 {Knowledge} {Graph} {Evaluation} {Track}: {Entity}, {Relation}, {Event} and {QA}},
	shorttitle = {Overview of the {CCKS} 2019 {Knowledge} {Graph} {Evaluation} {Track}},
	url = {http://arxiv.org/abs/2003.03875},
	abstract = {Knowledge graph models world knowledge as concepts, entities, and the relationships between them, which has been widely used in many real-world tasks. CCKS 2019 held an evaluation track with 6 tasks and attracted more than 1,600 teams. In this paper, we give an overview of the knowledge graph evaluation tract at CCKS 2019. By reviewing the task definition, successful methods, useful resources, good strategies and research challenges associated with each task in CCKS 2019, this paper can provide a helpful reference for developing knowledge graph applications and conducting future knowledge graph researches.},
	urldate = {2020-03-11},
	journal = {arXiv:2003.03875 [cs]},
	author = {Han, Xianpei and Wang, Zhichun and Zhang, Jiangtao and Wen, Qinghua and Li, Wenqi and Tang, Buzhou and Wang, Qi and Feng, Zhifan and Zhang, Yang and Lu, Yajuan and Wang, Haitao and Chen, Wenliang and Shao, Hao and Chen, Yubo and Liu, Kang and Zhao, Jun and Wang, Taifeng and Zhang, Kezun and Wang, Meng and Jiang, Yinlin and Qi, Guilin and Zou, Lei and Hu, Sen and Zhang, Minhao and Lin, Yinnian},
	month = mar,
	year = {2020},
	note = {00000 
arXiv: 2003.03875},
}

@article{hogan_knowledge_2020,
	title = {Knowledge {Graphs}},
	url = {http://arxiv.org/abs/2003.02320},
	abstract = {In this paper we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After a general introduction, we motivate and contrast various graph-based data models and query languages that are used for knowledge graphs. We discuss the roles of schema, identity, and context in knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We summarise methods for the creation, enrichment, quality assessment, refinement, and publication of knowledge graphs. We provide an overview of prominent open knowledge graphs and enterprise knowledge graphs, their applications, and how they use the aforementioned techniques. We conclude with high-level future research directions for knowledge graphs.},
	language = {en},
	urldate = {2020-05-08},
	journal = {arXiv:2003.02320 [cs]},
	author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and d'Amato, Claudia and de Melo, Gerard and Gutierrez, Claudio and Gayo, José Emilio Labra and Kirrane, Sabrina and Neumaier, Sebastian and Polleres, Axel and Navigli, Roberto and Ngomo, Axel-Cyrille Ngonga and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
	month = apr,
	year = {2020},
	note = {00011 
arXiv: 2003.02320},
}

@article{bai_segabert_2020,
	title = {{SegaBERT}: {Pre}-training of {Segment}-aware {BERT} for {Language} {Understanding}},
	shorttitle = {{SegaBERT}},
	url = {http://arxiv.org/abs/2004.14996},
	abstract = {Pre-trained language models have achieved state-of-the-art results in various natural language processing tasks. Most of them are based on the Transformer architecture, which distinguishes tokens with the token position index of the input sequence. However, sentence index and paragraph index are also important to indicate the token position in a document. We hypothesize that better contextual representations can be generated from the text encoder with richer positional information. To verify this, we propose a segment-aware BERT, by replacing the token position embedding of Transformer with a combination of paragraph index, sentence index, and token index embeddings. We pre-trained the SegaBERT on the masked language modeling task in BERT but without any affiliated tasks. Experimental results show that our pre-trained model can outperform the original BERT model on various NLP tasks.},
	urldate = {2020-05-01},
	journal = {arXiv:2004.14996 [cs]},
	author = {Bai, He and Shi, Peng and Lin, Jimmy and Tan, Luchen and Xiong, Kun and Gao, Wen and Li, Ming},
	month = apr,
	year = {2020},
	note = {00000 
arXiv: 2004.14996},
	keywords = {new},
}

@article{zhang_bert_nodate,
	title = {{BERT} for {Question} {Answering} on {SQuAD} 2.0},
	abstract = {Machine reading comprehension and question answering is an essential task in natural language processing. Recently, Pre-trained Contextual Embeddings (PCE) models like Embeddings from Language Models (ELMo) [1] and Bidirectional Encoder Representations from Transformers (BERT) [2] have attracted lots of attention due to their great performance in a wide range of NLP tasks. In this project, we picked up BERT model and tried to ﬁne-tune it with additional task-speciﬁc layers to improve its performance on Stanford Question Answering Dataset (SQuAD 2.0). We designed several output architectures and compared their performance to BERT baseline model in great details. So far, our best-proposed single model built an LSTM Encoder, an LSTM decoder and a highway network on top of the BERT base uncased model and achieved an F1 score of 77.96 on the dev set. By applying ensemble technique with selected models, our ﬁnal version model currently ranks 12th on the Stanford CS224N SQuAD 2.0 test leaderboard with an F1 score 77.827 (name: Pisces\_BERT).},
	language = {en},
	author = {Zhang, Yuwen and Xu, Zhaozhuo},
	note = {00000},
	keywords = {new},
	pages = {9},
}

@incollection{lin_knowledge_2016,
	address = {Cham},
	title = {Knowledge {Base} {Question} {Answering} {Based} on {Deep} {Learning} {Models}},
	volume = {10102},
	isbn = {978-3-319-50495-7 978-3-319-50496-4},
	url = {http://link.springer.com/10.1007/978-3-319-50496-4_25},
	abstract = {This paper focuses on the task of knowledge-based question answering (KBQA). KBQA aims to match the questions with the structured semantics in knowledge base. In this paper, we propose a two-stage method. Firstly, we propose a topic entity extraction model (TEEM) to extract topic entities in questions, which does not rely on hand-crafted features or linguistic tools. We extract topic entities in questions with the TEEM and then search the knowledge triples which are related to the topic entities from the knowledge base as the candidate knowledge triples. Then, we apply Deep Structured Semantic Models based on convolutional neural network and bidirectional long short-term memory to match questions and predicates in the candidate knowledge triples. To obtain better training dataset, we use an iterative approach to retrieve the knowledge triples from the knowledge base. The evaluation result shows that our system achieves an AverageF1 measure of 79.57\% on test dataset.},
	language = {en},
	urldate = {2020-04-27},
	booktitle = {Natural {Language} {Understanding} and {Intelligent} {Applications}},
	publisher = {Springer International Publishing},
	author = {Xie, Zhiwen and Zeng, Zhao and Zhou, Guangyou and He, Tingting},
	editor = {Lin, Chin-Yew and Xue, Nianwen and Zhao, Dongyan and Huang, Xuanjing and Feng, Yansong},
	year = {2016},
	doi = {10.1007/978-3-319-50496-4_25},
	note = {00000 
Series Title: Lecture Notes in Computer Science},
	pages = {300--311},
}

@article{celik_deep-channel_2020,
	title = {Deep-{Channel} uses deep neural networks to detect single-molecule events from patch-clamp data},
	volume = {3},
	copyright = {2020 The Author(s)},
	issn = {2399-3642},
	url = {https://www.nature.com/articles/s42003-019-0729-3},
	doi = {10.1038/s42003-019-0729-3},
	abstract = {Numan Celik et al. present a deep learning model that automatically detects single-molecule events against a noisy background in patch-clamp electrophysiological data, based on convolutional neural networks and long short-term memory architecture. This algorithm represents a step torward a fully automated electrophysiological platform.},
	language = {en},
	number = {1},
	urldate = {2020-04-19},
	journal = {Communications Biology},
	author = {Celik, Numan and O’Brien, Fiona and Brennan, Sean and Rainbow, Richard D. and Dart, Caroline and Zheng, Yalin and Coenen, Frans and Barrett-Jolley, Richard},
	month = jan,
	year = {2020},
	note = {00000 
Number: 1
Publisher: Nature Publishing Group},
	pages = {1--10},
}

@incollection{shetty_novel_2018,
	address = {Singapore},
	title = {A {Novel} {Approach} to {Mapping} for {KBQA} {System} {Using} {Ontology}},
	isbn = {978-981-10-4740-4 978-981-10-4741-1},
	url = {http://link.springer.com/10.1007/978-981-10-4741-1_9},
	abstract = {Knowledge Based Question Answering System (KBQA) aims to provide suitable answers for the queries posted by users using ontology. Ontology is supposed to reﬁne the search with semantics and confer the best result to the user. Mapping of Question and Answers are mandatory in every Question Answering System. This research work proposed the framework for KBQA Mapping system, which helps in retrieving precise information from a large collection of documents. The KBQA system consists of three phases such as Question Preprocessing, Answer Evaluation and Concept Mapping of question with their answer. The question posted by the user will be evaluated to get the answer and the answer will be taken from the pool of KBQA. This research paper explores the novel approach to the mapping of Question and most relevant Answers in KBQA System.},
	language = {en},
	urldate = {2020-03-10},
	booktitle = {Emerging {Research} in {Computing}, {Information}, {Communication} and {Applications}},
	publisher = {Springer Singapore},
	author = {Mervin, R. and Jaya, A.},
	editor = {Shetty, N. R. and Patnaik, L. M. and Prasad, N. H. and Nalini, N.},
	year = {2018},
	doi = {10.1007/978-981-10-4741-1_9},
	note = {00000 },
	pages = {89--97},
}

@incollection{zhu_alime_2019,
	address = {Singapore},
	title = {{AliMe} {KBQA}: {Question} {Answering} over {Structured} {Knowledge} for {E}-{Commerce} {Customer} {Service}},
	volume = {1134},
	isbn = {9789811519550 9789811519567},
	shorttitle = {{AliMe} {KBQA}},
	url = {http://link.springer.com/10.1007/978-981-15-1956-7_12},
	abstract = {With the rise of knowledge graph (KG), question answering over knowledge base (KBQA) has attracted increasing attention in recent years. Despite much research has been conducted on this topic, it is still challenging to apply KBQA technology in industry because business knowledge and real-world questions can be rather complicated. In this paper, we present AliMe-KBQA, a bold attempt to apply KBQA in the E-commerce customer service ﬁeld. To handle real knowledge and questions, we extend the classic “subject-predicate-object (SPO)” structure with property hierarchy, key-value structure and compound value type (CVT), and enhance traditional KBQA with constraints recognition and reasoning ability. We launch AliMe-KBQA in the Marketing Promotion scenario for merchants during the “Double 11” period in 2018 and other such promotional events afterwards. Online results suggest that AliMe-KBQA is not only able to gain better resolution and improve customer satisfaction, but also becomes the preferred knowledge management method by business knowledge staﬀs since it oﬀers a more convenient and eﬃcient management experience.},
	language = {en},
	urldate = {2020-03-10},
	booktitle = {Knowledge {Graph} and {Semantic} {Computing}: {Knowledge} {Computing} and {Language} {Understanding}},
	publisher = {Springer Singapore},
	author = {Li, Feng-Lin and Chen, Weijia and Huang, Qi and Guo, Yikun},
	editor = {Zhu, Xiaoyan and Qin, Bing and Zhu, Xiaodan and Liu, Ming and Qian, Longhua},
	year = {2019},
	doi = {10.1007/978-981-15-1956-7_12},
	note = {00000 
Series Title: Communications in Computer and Information Science},
	pages = {136--148},
}

@article{lin_semantic_nodate,
	title = {Semantic {Entity} {Search} in the {Knowledge} {Graph}},
	language = {en},
	author = {Lin, Xinshi},
	note = {00000},
	pages = {87},
}

@article{zhang_multi-view_2019,
	title = {Multi-view multitask learning for knowledge base relation detection},
	volume = {183},
	issn = {0950-7051},
	url = {http://www.sciencedirect.com/science/article/pii/S0950705119303429},
	doi = {10.1016/j.knosys.2019.104870},
	abstract = {Relation detection is a key component of knowledge base question answering (KBQA). Existing methods mainly focus on learning semantic relevance between the question and the candidate relation, which is challenging due to the lexical variation (i.e. lexical gaps) especially for relations with few annotated samples. In this paper, we propose to estimate the semantic relevance from both the traditional question–relation view and a novel question–question view, which leverages the similarities among questions corresponding to the same relation. The question–question view can facilitate the learning of few-shot relations, and it supports accessing the annotated samples during inference, thus allowing new annotations to take effect on–the–fly. Moreover, a multi-task learning framework is devised to jointly optimize the models of different views. Experimental results on WebQSP and a Chinese KB relation detection dataset demonstrate the effectiveness and generalization ability of the proposal.},
	language = {en},
	urldate = {2020-03-10},
	journal = {Knowledge-Based Systems},
	author = {Zhang, Hongzhi and Xu, Guangluan and Liang, Xiao and Zhang, Weili and Sun, Xian and Huang, Tinglei},
	month = nov,
	year = {2019},
	note = {00000},
	pages = {104870},
}
